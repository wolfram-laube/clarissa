# Include additional CI configurations
include:
  - local: '.gitlab/infra-setup.yml'
  - local: '.gitlab/gcp-check.yml'
  - local: '.gitlab/gcp-setup.yml'
  - local: '.gitlab/gmail-drafts.yml'
  - local: '.gitlab/benchmark.yml'
  - local: '.gitlab/gcp-vm-control.yml'
  - local: '.gitlab/gdrive-upload.yml'
  - local: '.gitlab/k3s-setup.yml'
  - local: '.gitlab/parallel-jobs.yml'
  - local: '.gitlab/applications.yml'
  - local: 'conference/spe-europe-2026/ci/document-merge.gitlab-ci.yml'
    rules:
      - changes:
          - conference/spe-europe-2026/**/*

workflow:
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: always
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: always
    # Trigger on semantic version tags (v1.0.0, v2.1.3, etc.)
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
      when: always
    # Allow manual trigger via "Run pipeline" button
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: always
    # Allow API triggers (for Claude automation)
    - if: '$CI_PIPELINE_SOURCE == "api"'
      when: always
    # Allow scheduled pipelines (billing automation)
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
      when: always
    - when: never

stages:
  - build
  - test
  - classify
  - automation
  - deploy

# ----------------------------
# Anchors (infrastructure hygiene)
# ----------------------------
.python_base: &python_base
  image: python:3.11
  variables:
    PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  cache:
    key: pip-cache
    paths:
      - .cache/pip

.install_dev_deps: &install_dev_deps
  before_script:
    - python -V
    - python -m pip install --upgrade pip
    # Clean up stale editable installs that break pip
    - pip uninstall clarissa -y 2>/dev/null || true
    - find $(python -c "import site; print(site.getsitepackages()[0])") -name "__editable__*" -delete 2>/dev/null || true
    - find $(python -c "import site; print(site.getsitepackages()[0])") -name "*.pth" -exec grep -l clarissa {} \; -delete 2>/dev/null || true
    - pip cache purge 2>/dev/null || true
    - python -m pip install -e ".[dev]"

.pytest_run: &pytest_run
  script:
    - python -m pytest -q --junitxml=junit.xml
  artifacts:
    when: always
    reports:
      junit: junit.xml
    paths:
      - junit.xml
      - .pytest_cache/

.rerun_rules: &rerun_rules
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_failure
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: on_failure
    - when: never

.issue_bot_rules: &issue_bot_rules
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: always
    - when: never

.recovery_bot_rules: &recovery_bot_rules
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: always
    - when: never

.mr_bot_rules: &mr_bot_rules
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - when: never

# ----------------------------
# Build Stage - Docker Images
# ----------------------------
build_opm_image:
  stage: build
  image: docker:24
  services:
    - docker:24-dind
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
    DOCKER_HOST: tcp://docker:2376
    DOCKER_TLS_VERIFY: 1
    DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
    IMAGE_BASE: $CI_REGISTRY_IMAGE/opm-flow
  before_script:
    - docker info
    - echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin $CI_REGISTRY
  script:
    - |
      echo "Building OPM Flow image..."
      
      # Always tag with commit SHA for traceability
      docker build -t $IMAGE_BASE:$CI_COMMIT_SHORT_SHA -f src/clarissa/simulators/opm/Dockerfile src/clarissa/simulators/opm/
      
      # Tag strategy based on context (ADR-012)
      if [ -n "$CI_COMMIT_TAG" ]; then
        # Release: semantic version tag (e.g., v1.2.3)
        echo "Release build: tagging as $CI_COMMIT_TAG"
        docker tag $IMAGE_BASE:$CI_COMMIT_SHORT_SHA $IMAGE_BASE:$CI_COMMIT_TAG
        docker tag $IMAGE_BASE:$CI_COMMIT_SHORT_SHA $IMAGE_BASE:latest
        
        # Push all tags
        docker push $IMAGE_BASE:$CI_COMMIT_SHORT_SHA
        docker push $IMAGE_BASE:$CI_COMMIT_TAG
        docker push $IMAGE_BASE:latest
        
        echo "IMAGE_TAG=$IMAGE_BASE:$CI_COMMIT_TAG" >> build.env
      else
        # Development: SHA + latest
        echo "Development build: tagging as $CI_COMMIT_SHORT_SHA and latest"
        docker tag $IMAGE_BASE:$CI_COMMIT_SHORT_SHA $IMAGE_BASE:latest
        
        docker push $IMAGE_BASE:$CI_COMMIT_SHORT_SHA
        docker push $IMAGE_BASE:latest
        
        echo "IMAGE_TAG=$IMAGE_BASE:$CI_COMMIT_SHORT_SHA" >> build.env
      fi
      
      echo "IMAGE_LATEST=$IMAGE_BASE:latest" >> build.env
  artifacts:
    reports:
      dotenv: build.env
  rules:
    # Build on MRs if OPM files changed
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - src/clarissa/simulators/opm/**/*
        - Dockerfile*
      when: on_success
    # Build on main if OPM files changed
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - src/clarissa/simulators/opm/**/*
        - Dockerfile*
      when: on_success
    # Always build on version tags (releases)
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
      when: always
    # Manual trigger always available
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never

# ----------------------------
# Test Stage
# ----------------------------
tests:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  script:
    - python -m pytest -q --ignore=tests/integration --junitxml=junit.xml
  artifacts:
    when: always
    reports:
      junit: junit.xml
    paths:
      - junit.xml
      - .pytest_cache/
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
    - when: never

integration_tests_opm:
  stage: test
  image: docker:24
  services:
    - docker:24-dind
  needs:
    - job: build_opm_image
      artifacts: true
      optional: true
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
    DOCKER_HOST: tcp://docker:2376
    DOCKER_TLS_VERIFY: 1
    DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
    PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  before_script:
    - apk add --no-cache python3 py3-pip
    - python3 -m pip install --break-system-packages -e ".[dev]"
    - echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin $CI_REGISTRY
    - |
      # Pull the image we just built, or try latest
      if [ -n "$IMAGE_TAG" ]; then
        docker pull $IMAGE_TAG && docker tag $IMAGE_TAG opm-flow:latest
      else
        docker pull $CI_REGISTRY_IMAGE/opm-flow:latest && docker tag $CI_REGISTRY_IMAGE/opm-flow:latest opm-flow:latest || echo "No image available, tests will skip"
      fi
  script:
    - |
      # Register custom pytest marker to avoid warnings
      echo "[pytest]" > pytest.ini
      echo "markers = docker: marks tests as requiring docker" >> pytest.ini
      python3 -m pytest -v tests/integration/test_opm_flow.py --junitxml=junit_integration.xml || true
  artifacts:
    when: always
    reports:
      junit: junit_integration.xml
    paths:
      - junit_integration.xml
  rules:
    # Only run when OPM-related files change
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - src/clarissa/simulators/opm/**/*
        - tests/integration/test_opm_flow.py
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - src/clarissa/simulators/opm/**/*
        - tests/integration/test_opm_flow.py
      when: on_success
    # Always run on version tags
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
      when: on_success
    - when: never
  allow_failure: true

snapshot_tests:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  script:
    - |
      set -e
      mkdir -p tests/golden tests/golden/snapshots tests/golden/diffs

      echo '# Snapshot diffs' > tests/golden/summary.md
      echo '' >> tests/golden/summary.md
      echo '_No snapshot diffs produced._' >> tests/golden/summary.md
      echo '' >> tests/golden/summary.md

      python -m pytest -q \
        tests/golden/test_cli_help_snapshot.py \
        tests/golden/test_cli_demo_snapshot.py || EXIT=$?

      if [ -d tests/golden/diffs ] && ls -1 tests/golden/diffs/*.diff >/dev/null 2>&1; then
        echo '# Snapshot diffs' > tests/golden/summary.md
        echo '' >> tests/golden/summary.md
        for f in tests/golden/diffs/*.diff; do
          echo "## $(basename "$f")" >> tests/golden/summary.md
          echo '```diff' >> tests/golden/summary.md
          cat "$f" >> tests/golden/summary.md
          echo '```' >> tests/golden/summary.md
          echo '' >> tests/golden/summary.md
        done
      fi

      if [ -n "${EXIT:-}" ]; then exit "$EXIT"; fi
  artifacts:
    when: on_failure
    paths:
      - tests/golden/snapshots/
      - tests/golden/summary.md
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
    - when: never

contract_tests:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  script:
    - |
      set -e
      mkdir -p tests/contracts
      python -m pytest -q tests/contracts 2>&1 | tee pytest_contracts.log || EXIT=$?
      python scripts/generate_contract_summary.py || true
      if [ -n "${EXIT:-}" ]; then exit "$EXIT"; fi
  artifacts:
    when: on_failure
    paths:
      - tests/contracts/summary.md
      - pytest_contracts.log
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
    - when: never

governance_impact:
  stage: test
  <<: *python_base
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - when: never
  script:
    - mkdir -p tests/governance
    - python scripts/detect_governance_impact.py
  artifacts:
    when: always
    paths:
      - tests/governance/impact.md

architecture_graphs:
  stage: test
  image:
    name: ghcr.io/mermaid-js/mermaid-cli/mermaid-cli:latest
    entrypoint: [""]
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: on_success
    - when: never
  script:
    - mkdir -p docs/architecture/diagrams docs/architecture/rendered
    - mmdc --version || true
    - bash scripts/render_mermaid.sh || echo "Mermaid render failed; keeping sources."
  artifacts:
    when: always
    paths:
      - docs/architecture/diagrams/
      - docs/architecture/rendered/
  allow_failure: true


mkdocs_nav_check:
  stage: test
  image: python:3.11-slim
  script:
    - pip install pyyaml -q
    - python scripts/check_mkdocs_nav.py
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - when: never

tests_rerun:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  <<: *rerun_rules
  needs:
    - job: tests
      artifacts: true
      optional: true
  script:
    - |
      set +e
      python -m pytest -q \
        --last-failed \
        --last-failed-no-failures=none \
        --maxfail=${CI_BOT_RERUN_MAXFAIL:-1} \
        -x \
        --junitxml=junit_rerun.xml
      echo $? > rerun_exit_code.txt
      exit 0
  artifacts:
    when: always
    paths:
      - junit_rerun.xml
      - rerun_exit_code.txt

# ----------------------------
# Classify Stage
# ----------------------------
ci_classify:
  stage: classify
  <<: *python_base
  needs:
    - job: tests
      artifacts: true
      optional: true
    - job: tests_rerun
      artifacts: true
      optional: true
  script:
    - python scripts/ci_classify.py
  artifacts:
    reports:
      dotenv: ci_classify.env
    paths:
      - ci_classify.env
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - when: never

# ----------------------------
# Automation Stage
# ----------------------------
mr_report:
  stage: automation
  <<: *python_base
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - when: never
  needs:
    - job: snapshot_tests
      artifacts: true
      optional: true
    - job: contract_tests
      artifacts: true
      optional: true
    - job: governance_impact
      artifacts: true
      optional: true
    - job: architecture_graphs
      artifacts: true
      optional: true
    - job: integration_tests_opm
      artifacts: true
      optional: true
  script:
    - mkdir -p reports
    - python scripts/generate_mr_report.py
    - python scripts/render_report_html.py || true
  artifacts:
    when: always
    paths:
      - reports/mr_report.md
      - reports/mr_report.html

ci_flaky_ledger:
  stage: automation
  <<: *python_base
  <<: *issue_bot_rules
  needs:
    - job: ci_classify
      artifacts: true
      optional: true
  script:
    - python scripts/gitlab_flaky_ledger_bot.py

ci_issue_on_failure:
  stage: automation
  <<: *python_base
  <<: *issue_bot_rules
  needs:
    - job: ci_classify
      artifacts: true
      optional: true
  script:
    - python scripts/gitlab_issue_bot.py

ci_issue_on_recovery:
  stage: automation
  <<: *python_base
  <<: *recovery_bot_rules
  needs:
    - job: ci_classify
      artifacts: true
      optional: true
  script:
    - python scripts/gitlab_recovery_bot.py

mr_comment_on_failure:
  stage: automation
  <<: *python_base
  <<: *mr_bot_rules
  needs:
    - job: ci_classify
      artifacts: true
      optional: true
  script:
    - python scripts/gitlab_mr_bot.py

# ----------------------------
# Deploy Stage
# ----------------------------
pages:
  stage: deploy
  image: python:3.11
  needs: []
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: always
    - when: never
  before_script:
    - pip install mkdocs mkdocs-material pymdown-extensions jinja2 pyyaml mkdocs-jupyter
  script:
    # Copy architecture diagrams from artifacts BEFORE mkdocs build
    - mkdir -p docs/architecture/rendered
    - cp docs/architecture/rendered/*.svg docs/architecture/rendered/ 2>/dev/null || echo "No rendered architecture diagrams from CI"
    - ls -la docs/architecture/rendered/ || true
    - python scripts/build_i18n_docs.py
    - mkdocs build --site-dir public
    - mkdir -p public/guides/contributing
    - cp docs/guides/contributing/*.html public/guides/contributing/ 2>/dev/null || true
    - echo "üìÑ Setting up publication PDFs..."
    - mkdir -p public/publications/ijacsa-2026 public/publications/spe-europe-2026
    # IJACSA paper (from build_paper)
    - cp conference/ijacsa-2026/CLARISSA_Paper_IJACSA.pdf public/publications/ijacsa-2026/ 2>/dev/null || echo "IJACSA PDF not in artifacts"
    # SPE canonical outputs (from build_canonical_pdf)
    - cp conference/spe-europe-2026/outputs/*.pdf public/publications/spe-europe-2026/ 2>/dev/null || echo "No canonical PDFs"
    - cp conference/spe-europe-2026/outputs/*.html public/publications/spe-europe-2026/ 2>/dev/null || echo "No canonical HTMLs"
    # Fallback to committed files if CI didn't run
    - cp conference/spe-europe-2026/abstract-with-diagrams.pdf public/publications/spe-europe-2026/ 2>/dev/null || true
    - cp conference/spe-europe-2026/abstract-corrected-v2.html public/publications/spe-europe-2026/ 2>/dev/null || true
    # Architecture diagrams gallery page
    - mkdir -p public/architecture/rendered
    - cp docs/architecture/rendered/index.html public/architecture/rendered/ 2>/dev/null || true
    # Create publications index
    - |
      cat > public/publications/index.html << 'PUBHTML'
      <!DOCTYPE html>
      <html><head><title>CLARISSA Publications</title>
      <style>
      body{font-family:sans-serif;max-width:900px;margin:50px auto;padding:20px}
      h1{color:#1a365d}
      .conf{background:#f7fafc;padding:20px;margin:20px 0;border-radius:8px;border-left:4px solid #3182ce}
      .conf h2{margin-top:0;color:#2c5282}
      a{color:#3182ce}
      .docs{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin-top:15px}
      .doc{background:white;padding:15px;border-radius:4px;border:1px solid #e2e8f0}
      .doc a{font-weight:bold}
      </style></head><body>
      <h1>üìö CLARISSA Publications</h1>
      <p>Review copies for internal distribution.</p>
      
      <div class="conf"><h2>IJACSA 2026</h2>
      <p>International Journal of Advanced Computer Science and Applications</p>
      <div class="docs">
        <div class="doc"><a href="ijacsa-2026/CLARISSA_Paper_IJACSA.pdf">üìÑ Full Paper (PDF)</a><br><small>8 pages, all figures</small></div>
      </div></div>
      
      <div class="conf"><h2>SPE Europe Energy Conference 2026</h2>
      <p>Category: Digital Transformation and AI</p>
      <div class="docs">
        <div class="doc"><a href="spe-europe-2026/CLARISSA_SPE_Abstract.pdf">üìÑ Abstract (PDF)</a><br><small>Conference submission</small></div>
        <div class="doc"><a href="spe-europe-2026/abstract-with-diagrams.pdf">üìÑ Full Paper (PDF)</a><br><small>With Mermaid diagrams</small></div>
        <div class="doc"><a href="spe-europe-2026/abstract-corrected-v2.html">üåê Full Paper (HTML)</a><br><small>Interactive version</small></div>
      </div></div>
      
      <p style="margin-top:40px;color:#718096;font-size:0.9em">
      Groundtruth: <code>conference/spe-europe-2026/canonical/</code>
      </p>
      </body></html>
      PUBHTML
    - echo "üìÅ Publications deployed:"
    - find public/publications -type f
  artifacts:
    paths:
      - public
    expire_in: 1 week

llm_sync_package:
  stage: deploy
  image: python:3.11
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "push"'
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never
  script:
    - |
      BRANCH="${CI_MERGE_REQUEST_SOURCE_BRANCH_NAME:-${CI_COMMIT_BRANCH:-${CI_COMMIT_TAG:-main}}}"
      echo "Generating sync package for branch: $BRANCH"
      cd scripts
      python llm_sync_generator.py --branch "$BRANCH" --lite -o ../clarissa_sync_lite.md
      python llm_sync_generator.py --branch "$BRANCH" --medium -o ../clarissa_sync_medium.md
      python llm_sync_generator.py --branch "$BRANCH" --lite --since ${CI_COMMIT_BEFORE_SHA:-HEAD~5} -o ../clarissa_sync_diff.md || true
  artifacts:
    paths:
      - clarissa_sync_lite.md
      - clarissa_sync_medium.md
      - clarissa_sync_diff.md
    expire_in: 4 weeks

# ----------------------------
# Manual Trigger Jobs
# ----------------------------
# These jobs can be triggered via:
#   1. GitLab UI: CI/CD ‚Üí Pipelines ‚Üí Run Pipeline ‚Üí Select job
#   2. API: POST /projects/:id/pipeline with variables

rebuild_docs:
  stage: deploy
  image: python:3.11
  rules:
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  before_script:
    - pip install mkdocs mkdocs-material pymdown-extensions jinja2 pyyaml mkdocs-jupyter
  script:
    - echo "Rebuilding documentation..."
    - python scripts/build_i18n_docs.py
    - mkdocs build --site-dir public
    - |
      mkdir -p public/guides/contributing
      cp docs/guides/contributing/*.html public/guides/contributing/ 2>/dev/null || true
  artifacts:
    paths:
      - public
    expire_in: 1 day
  environment:
    name: docs-preview
    url: https://wolfram_laube.gitlab.io/blauweiss_llc/irena/

rebuild_opm_image:
  stage: build
  image: docker:24
  services:
    - docker:24-dind
  rules:
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
    DOCKER_HOST: tcp://docker:2376
    DOCKER_TLS_VERIFY: 1
    DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
    IMAGE_BASE: $CI_REGISTRY_IMAGE/opm-flow
  before_script:
    - docker info
    - echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin $CI_REGISTRY
  script:
    - |
      echo "Manual rebuild of OPM Flow image..."
      docker build --no-cache -t $IMAGE_BASE:$CI_COMMIT_SHORT_SHA -f src/clarissa/simulators/opm/Dockerfile src/clarissa/simulators/opm/
      docker tag $IMAGE_BASE:$CI_COMMIT_SHORT_SHA $IMAGE_BASE:latest
      docker push $IMAGE_BASE:$CI_COMMIT_SHORT_SHA
      docker push $IMAGE_BASE:latest
      echo "‚úÖ Pushed: $IMAGE_BASE:$CI_COMMIT_SHORT_SHA"
      echo "‚úÖ Pushed: $IMAGE_BASE:latest"

rerun_all_tests:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  rules:
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  script:
    - echo "Running full test suite (manual trigger)..."
    - python -m pytest -v --junitxml=junit_manual.xml
  artifacts:
    when: always
    reports:
      junit: junit_manual.xml
    paths:
      - junit_manual.xml

build_paper:
  stage: build
  image: texlive/texlive:latest
  tags:
    - docker
  rules:
    # Auto-run when paper files change
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - conference/**/*
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - conference/**/*
      when: on_success
    # Manual trigger always available
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  script:
    - cd conference/ijacsa-2026
    - echo "Copying figures to build directory..."
    - cp figures/*.png . 2>/dev/null || echo "No figures to copy"
    - ls -la *.png 2>/dev/null || echo "No PNG files found"
    - echo "Building CLARISSA paper..."
    - pdflatex -interaction=nonstopmode CLARISSA_Paper_IJACSA.tex || true
    - pdflatex -interaction=nonstopmode CLARISSA_Paper_IJACSA.tex  # Second pass for references
    - echo "‚úÖ Paper built successfully"
    - ls -la *.pdf
  artifacts:
    paths:
      - conference/ijacsa-2026/CLARISSA_Paper_IJACSA.pdf
    expire_in: 4 weeks

# =============================================================================
# BILLING AUTOMATION: Monthly Timesheet Generation
# =============================================================================
# Trigger: Scheduled pipeline (1st of month) or manual with GENERATE_TIMESHEETS=true
# Setup: GitLab CI/CD > Schedules > Cron: "0 6 1 * *" > Variable: BILLING_RUN=true
# =============================================================================
# =============================================================================
# SPE EUROPE 2026: Canonical Document Pipeline
# =============================================================================
# Trigger: Changes to canonical/ or diagrams/*.mmd
# Outputs: HTML + PDF for submission-form and full-paper
# =============================================================================

# Job 1: Render Mermaid diagrams to SVG/PNG
render_diagrams:
  stage: build
  image: minlag/mermaid-cli:latest
  tags:
    - docker
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - "conference/spe-europe-2026/diagrams/*.mmd"
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never
  script:
    - cd conference/spe-europe-2026/diagrams
    - |
      echo "üé® Rendering Mermaid diagrams..."
      for mmd in *.mmd; do
        name="${mmd%.mmd}"
        echo "   Processing $mmd..."
        mmdc -i "$mmd" -o "${name}.svg" -b transparent || echo "SVG failed for $mmd"
        mmdc -i "$mmd" -o "${name}.png" -s 2 -b white || echo "PNG failed for $mmd"
      done
    - ls -la *.svg *.png 2>/dev/null || echo "No outputs"
  artifacts:
    paths:
      - conference/spe-europe-2026/diagrams/*.svg
      - conference/spe-europe-2026/diagrams/*.png
    expire_in: 4 weeks

# Job 2: Build HTML from canonical Markdown
build_canonical_html:
  stage: build
  image: python:3.11-slim
  tags:
    - docker
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - "conference/spe-europe-2026/canonical/**/*"
        - "conference/spe-europe-2026/scripts/*.py"
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never
  script:
    - python conference/spe-europe-2026/scripts/build_canonical.py
    - ls -la conference/spe-europe-2026/outputs/
  artifacts:
    paths:
      - conference/spe-europe-2026/outputs/*.html
    expire_in: 4 weeks

# Job 3: Generate PDFs from HTML via Puppeteer
build_canonical_pdf:
  stage: test
  image: node:20-alpine
  tags:
    - docker
  needs:
    - job: build_canonical_html
      artifacts: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - "conference/spe-europe-2026/canonical/**/*"
        - "conference/spe-europe-2026/scripts/*.py"
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never
  before_script:
    - apk add --no-cache chromium
    - npm install puppeteer
    - export PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser
  script:
    - |
      cat > generate-pdfs.js << 'JSEOF'
      const puppeteer = require('puppeteer');
      const fs = require('fs');
      const path = require('path');

      (async () => {
        const browser = await puppeteer.launch({
          headless: 'new',
          executablePath: process.env.PUPPETEER_EXECUTABLE_PATH,
          args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
        });
        
        const outputDir = 'conference/spe-europe-2026/outputs';
        const htmlFiles = fs.readdirSync(outputDir).filter(f => f.endsWith('.html'));
        
        let success = 0, failed = 0;
        
        for (const htmlFile of htmlFiles) {
          const pdfFile = htmlFile.replace('.html', '.pdf');
          console.log('Converting ' + htmlFile + ' to ' + pdfFile);
          
          try {
            const page = await browser.newPage();
            
            // Set BOTH timeouts explicitly - this is critical!
            page.setDefaultTimeout(120000);
            page.setDefaultNavigationTimeout(120000);
            
            const html = fs.readFileSync(path.join(outputDir, htmlFile), 'utf8');
            
            // Use domcontentloaded - don't wait for network (CDN can be flaky)
            console.log('   Loading HTML...');
            await page.setContent(html, { 
              waitUntil: 'domcontentloaded',
              timeout: 90000 
            });
            
            // Check if there are Mermaid diagrams to render
            const hasMermaid = await page.evaluate(() => 
              document.querySelectorAll('.mermaid').length > 0
            );
            
            if (hasMermaid) {
              console.log('   Waiting for Mermaid.js to render diagrams...');
              try {
                await page.waitForFunction(() => {
                  const divs = document.querySelectorAll('.mermaid');
                  return Array.from(divs).every(d => d.querySelector('svg'));
                }, { timeout: 60000 });
                console.log('   Mermaid diagrams rendered');
              } catch (e) {
                console.log('   Mermaid timeout - diagrams may be missing');
              }
            }
            
            // Small buffer for any final rendering
            await new Promise(r => setTimeout(r, 1500));
            
            // Generate PDF
            console.log('   Generating PDF...');
            await page.pdf({
              path: path.join(outputDir, pdfFile),
              format: 'A4',
              margin: { top: '2cm', right: '2cm', bottom: '2cm', left: '2cm' },
              printBackground: true
            });
            
            await page.close();
            console.log('   ' + pdfFile + ' created');
            success++;
            
          } catch (err) {
            console.error('   Failed: ' + err.message);
            failed++;
          }
        }
        
        await browser.close();
        
        console.log('
üìä Results: ${success} success, ' + failed + ' failed');
        
        if (failed > 0 && success === 0) {
          process.exit(1); // Only fail if ALL conversions failed
        }
      })();
      JSEOF
    - node generate-pdfs.js
    - ls -la conference/spe-europe-2026/outputs/
  artifacts:
    paths:
      - conference/spe-europe-2026/outputs/*.pdf
      - conference/spe-europe-2026/outputs/*.html
    expire_in: 4 weeks
    name: "spe-canonical-${CI_COMMIT_SHORT_SHA}"

generate_timesheets:
  stage: build
  image: python:3.11-slim
  variables:
    GITLAB_PROJECT_PATH: "wolfram_laube/blauweiss_llc/irena"
  rules:
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $BILLING_RUN == "true"'
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "web" && $GENERATE_TIMESHEETS == "true"'
      when: on_success
    - when: never
  before_script:
    - pip install pyyaml requests -q
  script:
    - |
      echo "=== Monthly Timesheet Generation ==="
      if [ -n "$BILLING_PERIOD" ]; then
        PERIOD="$BILLING_PERIOD"
      else
        PERIOD=$(date -d "last month" +%Y-%m)
      fi
      echo "Period: $PERIOD"
      cd billing/scripts
      for CLIENT in nemensis; do
        echo "--- Client: $CLIENT ---"
        python generate_timesheet.py --client "$CLIENT" --period "$PERIOD" --all-consultants || echo "No entries for $CLIENT"
      done
      ls -la ../output/*.typ 2>/dev/null || echo "No timesheets generated"
  artifacts:
    paths:
      - billing/output/*.typ
      - billing/output/*.sync.json
    expire_in: 8 weeks

build_invoice:
  stage: build
  image: alpine:latest
  rules:
    # Auto-run when invoice templates change
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - billing/templates/**/*
        - billing/output/**/*.typ
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - billing/templates/**/*
        - billing/output/**/*.typ
      when: on_success
    # Manual trigger always available
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  before_script:
    - apk add --no-cache curl tar xz fontconfig ttf-liberation
    # Install Poppins font
    - mkdir -p /usr/share/fonts/poppins
    - curl -L "https://github.com/google/fonts/raw/main/ofl/poppins/Poppins-Regular.ttf" -o /usr/share/fonts/poppins/Poppins-Regular.ttf
    - curl -L "https://github.com/google/fonts/raw/main/ofl/poppins/Poppins-Bold.ttf" -o /usr/share/fonts/poppins/Poppins-Bold.ttf
    - curl -L "https://github.com/google/fonts/raw/main/ofl/poppins/Poppins-Medium.ttf" -o /usr/share/fonts/poppins/Poppins-Medium.ttf
    - fc-cache -fv
    # Install Typst
    - curl -L "https://github.com/typst/typst/releases/download/v0.12.0/typst-x86_64-unknown-linux-musl.tar.xz" -o /tmp/typst.tar.xz
    - tar xf /tmp/typst.tar.xz -C /tmp
    - mv /tmp/typst-x86_64-unknown-linux-musl/typst /usr/local/bin/
    - typst --version
  script:
    - echo "Building invoices with Typst..."
    - cd billing
    - cp templates/logo.jpg output/ 2>/dev/null || true
    - |
      cd output
      for f in *.typ; do
        if [ -f "$f" ]; then
          echo "Compiling $f..."
          typst compile --root .. "$f"
        fi
      done
    - echo "‚úÖ Invoices built"
    - ls -la *.pdf 2>/dev/null || echo "No PDFs generated"
  artifacts:
    paths:
      - billing/output/*.pdf
    expire_in: 8 weeks

# Upload billing documents to Google Drive
upload_invoice:
  stage: deploy
  image: python:3.11-slim
  needs:
    - job: build_invoice
      artifacts: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - billing/output/**/*.typ
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never
  script:
    - pip install google-auth google-api-python-client -q
    - python billing/scripts/upload_to_drive.py billing/output/*.pdf

# Create Gmail draft with billing documents
billing_email:
  stage: deploy
  image: python:3.11-slim
  needs:
    - job: build_invoice
      artifacts: true
    - job: upload_invoice
      optional: true
  rules:
    # Auto-run when billing output changes on main
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - billing/output/**/*.typ
      when: on_success
    # Manual trigger always available
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never
  variables:
    BILLING_CLIENT: "nemensis"  # Override via pipeline variable if needed
    BILLING_PERIOD: ""          # Auto-detect from files if empty
  before_script:
    - pip install requests pyyaml -q
  script:
    - |
      # Auto-detect period from latest invoice if not set
      if [ -z "$BILLING_PERIOD" ]; then
        BILLING_PERIOD=$(ls billing/output/*.sync.json 2>/dev/null | head -1 | sed 's/.*\/\([0-9]\{4\}-[0-9]\{2\}\).*//' || echo "")
        if [ -z "$BILLING_PERIOD" ]; then
          echo "‚ùå Could not detect billing period"
          exit 1
        fi
        echo "üìÖ Auto-detected period: $BILLING_PERIOD"
      fi
      export BILLING_PERIOD
    - python billing/scripts/send_billing_email.py

# =============================================================================
# LLM Relay: Claude ‚Üî IRENA Communication
# =============================================================================

relay:irena:
  stage: automation
  image: python:3.11-slim
  rules:
    - changes:
        - handoffs/handoff_to_irena.md
      when: always
    - when: never
  before_script:
    - pip install openai requests --quiet
    - apt-get update && apt-get install -y curl --quiet
  script:
    - echo "üì• Processing handoff to IRENA..."
    - python scripts/relay.py --process
    - |
      if [ -f handoffs/handoff_to_claude.md ]; then
        echo "üì§ Pushing IRENA response..."
        ENCODED_CONTENT=$(base64 -w 0 handoffs/handoff_to_claude.md)
        curl --silent --request PUT \
          --header "PRIVATE-TOKEN: ${GITLAB_TOKEN}" \
          --header "Content-Type: application/json" \
          --data "{\"branch\": \"main\", \"encoding\": \"base64\", \"content\": \"${ENCODED_CONTENT}\", \"commit_message\": \"feat(relay): IRENA auto-response [skip ci]\"}" \
          "https://gitlab.com/api/v4/projects/${CI_PROJECT_ID}/repository/files/handoffs%2Fhandoff_to_claude.md"
        echo "‚úÖ Response pushed!"
      else
        echo "‚ùå No response generated"
        exit 1
      fi
  variables:
    OPENAI_API_KEY: ${OPENAI_API_KEY}
    GITLAB_TOKEN: ${GITLAB_TOKEN}

# ============================================
# Google Drive Sync
# ============================================

sync_infrastructure:
  stage: deploy
  image: python:3.11-slim
  rules:
    - changes:
        - docs/infrastructure.md
      when: always
    - when: never
  before_script:
    - pip install google-auth google-api-python-client --quiet
    - echo "${GOOGLE_SERVICE_ACCOUNT_KEY}" > /tmp/service_account.json
  script:
    - |
      python3 << 'PYTHON'
      from google.oauth2 import service_account
      from googleapiclient.discovery import build
      from googleapiclient.http import MediaFileUpload
      
      SCOPES = ['https://www.googleapis.com/auth/drive.file']
      FOLDER_ID = '1qh0skTeyRNs4g9KwAhpd3J8Yj_XENIFs'
      
      creds = service_account.Credentials.from_service_account_file(
          '/tmp/service_account.json', scopes=SCOPES)
      service = build('drive', 'v3', credentials=creds)
      
      results = service.files().list(
          q=f"name='infrastructure.md' and '{FOLDER_ID}' in parents and trashed=false",
          fields="files(id)"
      ).execute()
      files = results.get('files', [])
      
      media = MediaFileUpload('docs/infrastructure.md', mimetype='text/markdown')
      
      if files:
          service.files().update(fileId=files[0]['id'], media_body=media).execute()
          print("‚úÖ Updated infrastructure.md in Google Drive")
      else:
          service.files().create(
              body={'name': 'infrastructure.md', 'parents': [FOLDER_ID]},
              media_body=media
          ).execute()
          print("‚úÖ Created infrastructure.md in Google Drive")
      PYTHON


# =============================================================================
# DOCUMENT MERGE: LLM-Powered Multi-Author Collaboration (ADR-014)
# =============================================================================
# Enables Doug (DOCX) + Mike/Wolfram (Markdown) to collaborate on SPE abstract
# Pipeline triggers on changes to conference/spe-europe-2026/sources/
# =============================================================================

# Doc Merge Pipeline - Shell Runner Compatible
# Works on Mac runners without Docker

# Doc Merge Pipeline - Docker Runner Compatible
# Uses proper Docker images for each job

doc_merge:normalize:
  stage: build
  image: debian:bookworm-slim
  tags:
    - docker
  before_script:
    - apt-get update -qq && apt-get install -y -qq pandoc > /dev/null 2>&1
  rules:
    - if: '$CI_PIPELINE_SOURCE == "push" || $CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - conference/spe-europe-2026/sources/**/*
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "web" && $DOC_MERGE == "true"'
      when: on_success
    - when: never
  script:
    - |
      mkdir -p normalized
      DOC_PATH="conference/spe-europe-2026/sources"
      
      echo "=== Normalizing DOCX files ==="
      for docx in ${DOC_PATH}/doug/*.docx; do
        [ -f "$docx" ] || continue
        name=$(basename "$docx" .docx)
        echo "Converting: $docx"
        pandoc "$docx" -t markdown -o "normalized/doug-${name}.md" --wrap=none
      done
      
      echo "=== Copying MD files ==="
      for md in ${DOC_PATH}/*/*.md; do
        [ -f "$md" ] || continue
        author=$(basename $(dirname "$md"))
        name=$(basename "$md" .md)
        echo "Copying: $md"
        cp "$md" "normalized/${author}-${name}.md"
      done
      
      echo "=== Normalized files ==="
      ls -la normalized/
  artifacts:
    paths:
      - normalized/
    expire_in: 1 day

doc_merge:compare:
  stage: test
  image: python:3.11-slim
  tags:
    - docker
  needs:
    - job: doc_merge:normalize
      artifacts: true
  rules:
    - if: '$CI_PIPELINE_SOURCE == "push" || $CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - conference/spe-europe-2026/sources/**/*
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "web" && $DOC_MERGE == "true"'
      when: on_success
    - when: never
  script:
    - pip install --quiet anthropic openai
    - python3 conference/spe-europe-2026/scripts/doc_compare.py normalized/ comparison-report.json
    - echo "=== Comparison Report ==="
    - cat comparison-report.json
  artifacts:
    paths:
      - comparison-report.json
    expire_in: 1 week

doc_merge:merge:
  stage: automation
  image: python:3.11-slim
  tags:
    - docker
  needs:
    - job: doc_merge:normalize
      artifacts: true
    - job: doc_merge:compare
      artifacts: true
  rules:
    - if: '$CI_PIPELINE_SOURCE == "push" || $CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - conference/spe-europe-2026/sources/**/*
      when: manual
    - if: '$CI_PIPELINE_SOURCE == "web" && $DOC_MERGE == "true"'
      when: manual
    - when: never
  script:
    - pip install --quiet anthropic openai
    - python3 conference/spe-europe-2026/scripts/doc_do_merge.py normalized/ merged-output.md merge-log.json
    - echo "=== Merge Log ==="
    - cat merge-log.json
  artifacts:
    paths:
      - merged-output.md
      - merge-log.json
    expire_in: 1 week

doc_merge:generate:
  stage: deploy
  image: node:20-bookworm
  tags:
    - docker
  needs:
    - job: doc_merge:merge
      artifacts: true
  rules:
    - if: '$CI_PIPELINE_SOURCE == "push" || $CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - conference/spe-europe-2026/sources/**/*
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "web" && $DOC_MERGE == "true"'
      when: on_success
    - when: never
  before_script:
    - apt-get update -qq && apt-get install -y -qq pandoc texlive-xetex texlive-fonts-recommended texlive-latex-extra python3 chromium librsvg2-bin > /dev/null 2>&1
    - npm install -g @mermaid-js/mermaid-cli > /dev/null 2>&1
    - export PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium
  script:
    - |
      mkdir -p doc-outputs diagrams
      
      if [ ! -f merged-output.md ]; then
        echo "No merged document found"
        exit 1
      fi
      
      echo "=== Rendering Mermaid diagrams to SVG ==="
      python3 conference/spe-europe-2026/scripts/render_mermaid_images.py \
        merged-output.md \
        doc-outputs/for-pdf.md \
        diagrams
      
      echo "=== Generating PDF with rendered diagrams ==="
      cp -r diagrams doc-outputs/
      cd doc-outputs
      pandoc for-pdf.md -o abstract-merged.pdf \
        --pdf-engine=xelatex \
        -V geometry:margin=2.5cm \
        -V fontsize=11pt \
        -V mainfont="DejaVu Serif" \
        -V sansfont="DejaVu Sans" \
        -V monofont="DejaVu Sans Mono" || \
      pandoc for-pdf.md -o abstract-merged.pdf \
        --pdf-engine=pdflatex \
        -V geometry:margin=2.5cm \
        -V fontsize=11pt
      cd ..
      
      echo "=== Generating HTML with Mermaid.js ==="
      python3 conference/spe-europe-2026/scripts/preprocess_mermaid.py \
        merged-output.md \
        doc-outputs/for-html.md
      pandoc doc-outputs/for-html.md -o doc-outputs/abstract-merged.html \
        --standalone \
        --template=conference/spe-europe-2026/templates/mermaid-template.html \
        --metadata title="CLARISSA - SPE Europe 2026"
      
      echo "=== Generating DOCX ==="
      pandoc merged-output.md -o doc-outputs/abstract-merged.docx
      
      echo "=== Copying canonical MD ==="
      cp merged-output.md doc-outputs/abstract-canonical.md
      
      echo "=== Generated files ==="
      ls -la doc-outputs/
      ls -la diagrams/
  artifacts:
    paths:
      - doc-outputs/
      - diagrams/
    expire_in: 4 weeks

# ----------------------------
# Google Drive Proxy Jobs
# ----------------------------
gdrive-list:
  stage: .pre
  tags: [gcp-docker]
  when: manual
  allow_failure: true
  image: python:3.11-slim
  script:
    - pip install google-auth google-api-python-client -q
    - |
      python3 << 'PYEOF'
      import json, os
      from google.oauth2 import service_account
      from googleapiclient.discovery import build
      
      # GOOGLE_SERVICE_ACCOUNT_KEY is a FILE type variable - read from file path
      sa_key_path = os.environ.get("GOOGLE_SERVICE_ACCOUNT_KEY", "")
      folder_id = os.environ.get("GOOGLE_DRIVE_FOLDER_ID", "1qh0skTeyRNs4g9KwAhpd3J8Yj_XENIFs")
      
      print(f"SA Key path: {sa_key_path}")
      print(f"Folder ID: {folder_id}")
      
      # Read service account key from file
      with open(sa_key_path, 'r') as f:
          sa_key = json.load(f)
      
      print(f"Loaded SA: {sa_key.get('client_email')}")
      
      credentials = service_account.Credentials.from_service_account_info(
          sa_key, scopes=["https://www.googleapis.com/auth/drive.readonly"]
      )
      service = build("drive", "v3", credentials=credentials)
      
      results = service.files().list(
          q=f"'{folder_id}' in parents and trashed=false",
          fields="files(id, name, mimeType, modifiedTime, size)",
          pageSize=100
      ).execute()
      
      files = results.get("files", [])
      print(f"
üìÅ Found {len(files)} files in Google Drive:")
      print("-" * 60)
      for f in files:
          size = f.get("size", "-")
          mod = f.get("modifiedTime", "-")[:10]
          print(f"  {f['name']:<45} {mod}")
      
      with open("gdrive_files.json", "w") as out:
          json.dump(files, out, indent=2)
      PYEOF
  artifacts:
    paths:
      - gdrive_files.json
    expire_in: 1 day

# ============================================================
# Google Drive Proxy Deployment
# ============================================================
deploy-gdrive-proxy:
  stage: deploy
  tags:
    - gcp-shell
  needs: []
  rules:
    - if: $DEPLOY_GDRIVE_PROXY == "true"
      when: always
  script:
    - echo "üöÄ Deploying Google Drive Proxy to GCP VM..."
    - echo "üîß Fixing package issues..."
    - sudo dpkg --configure -a || true
    - sudo apt --fix-broken install -y || true
    - sudo apt-get update
    - sudo apt-get install -y python3-pip python3-venv || echo "Dependencies already satisfied" 
    - sudo mkdir -p /opt/gdrive-proxy /etc/gdrive-proxy
    - |
      sudo tee /opt/gdrive-proxy/requirements.txt << 'REQEOF'
      flask
      google-api-python-client
      google-auth
      REQEOF
    - sudo python3 -m venv /opt/gdrive-proxy/venv
    - sudo /opt/gdrive-proxy/venv/bin/pip install -r /opt/gdrive-proxy/requirements.txt
    - cat scripts/gdrive_proxy.py | sudo tee /opt/gdrive-proxy/gdrive_proxy.py
    - sudo chmod +x /opt/gdrive-proxy/gdrive_proxy.py
    - echo "$GOOGLE_SERVICE_ACCOUNT_KEY" | sudo tee /etc/gdrive-proxy/sa-key.json > /dev/null
    - sudo chmod 600 /etc/gdrive-proxy/sa-key.json
    - |
      sudo tee /etc/systemd/system/gdrive-proxy.service << 'SVCEOF'
      [Unit]
      Description=Google Drive API Proxy
      After=network.target
      
      [Service]
      Type=simple
      Environment=GOOGLE_APPLICATION_CREDENTIALS=/etc/gdrive-proxy/sa-key.json
      ExecStart=/opt/gdrive-proxy/venv/bin/python /opt/gdrive-proxy/gdrive_proxy.py
      Restart=always
      RestartSec=5
      
      [Install]
      WantedBy=multi-user.target
      SVCEOF
    - sudo systemctl daemon-reload
    - sudo systemctl enable gdrive-proxy
    - sudo systemctl restart gdrive-proxy
    - sleep 3
    - curl -s http://localhost:8080/health || true
    - echo "‚úÖ GDrive Proxy deployed! Access at http://35.198.98.28:8080"# Documentation preview for feature branches
docs:preview:
  stage: deploy
  image: python:3.11
  needs: []  # Run immediately, don't wait for other stages
  rules:
    - if: '$CI_COMMIT_BRANCH != "main"'
      when: manual
    - when: never
  before_script:
    - pip install mkdocs mkdocs-material pymdown-extensions jinja2 pyyaml mkdocs-jupyter
  script:
    - echo "üî® Building documentation preview..."
    - python scripts/build_i18n_docs.py || echo "i18n script not critical"
    - mkdocs build --site-dir public
    - echo "‚úÖ Documentation built successfully!"
    - echo "üì¶ Download artifacts to preview locally"
  artifacts:
    paths:
      - public
    expire_in: 1 week
    name: "docs-preview-${CI_COMMIT_SHORT_SHA}"
