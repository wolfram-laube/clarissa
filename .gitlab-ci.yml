workflow:
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: always
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: always
    # Trigger on semantic version tags (v1.0.0, v2.1.3, etc.)
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
      when: always
    # Allow manual trigger via "Run pipeline" button
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: always
    - when: never

stages:
  - build
  - test
  - classify
  - automation
  - deploy

# ----------------------------
# Anchors (infrastructure hygiene)
# ----------------------------
.python_base: &python_base
  image: python:3.11
  variables:
    PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  cache:
    key: pip-cache
    paths:
      - .cache/pip

.install_dev_deps: &install_dev_deps
  before_script:
    - python -V
    - python -m pip install --upgrade pip
    - python -m pip install -e ".[dev]"

.pytest_run: &pytest_run
  script:
    - python -m pytest -q --junitxml=junit.xml
  artifacts:
    when: always
    reports:
      junit: junit.xml
    paths:
      - junit.xml
      - .pytest_cache/

.rerun_rules: &rerun_rules
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_failure
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: on_failure
    - when: never

.issue_bot_rules: &issue_bot_rules
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: on_success
    - when: never

.recovery_bot_rules: &recovery_bot_rules
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: on_success
    - when: never

.mr_bot_rules: &mr_bot_rules
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - when: never

# ----------------------------
# Build Stage - Docker Images
# ----------------------------
build_opm_image:
  stage: build
  image: docker:24
  services:
    - docker:24-dind
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
    DOCKER_HOST: tcp://docker:2376
    DOCKER_TLS_VERIFY: 1
    DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
    IMAGE_BASE: $CI_REGISTRY_IMAGE/opm-flow
  before_script:
    - docker info
    - echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin $CI_REGISTRY
  script:
    - |
      echo "Building OPM Flow image..."
      
      # Always tag with commit SHA for traceability
      docker build -t $IMAGE_BASE:$CI_COMMIT_SHORT_SHA -f src/clarissa/simulators/opm/Dockerfile src/clarissa/simulators/opm/
      
      # Tag strategy based on context (ADR-012)
      if [ -n "$CI_COMMIT_TAG" ]; then
        # Release: semantic version tag (e.g., v1.2.3)
        echo "Release build: tagging as $CI_COMMIT_TAG"
        docker tag $IMAGE_BASE:$CI_COMMIT_SHORT_SHA $IMAGE_BASE:$CI_COMMIT_TAG
        docker tag $IMAGE_BASE:$CI_COMMIT_SHORT_SHA $IMAGE_BASE:latest
        
        # Push all tags
        docker push $IMAGE_BASE:$CI_COMMIT_SHORT_SHA
        docker push $IMAGE_BASE:$CI_COMMIT_TAG
        docker push $IMAGE_BASE:latest
        
        echo "IMAGE_TAG=$IMAGE_BASE:$CI_COMMIT_TAG" >> build.env
      else
        # Development: SHA + latest
        echo "Development build: tagging as $CI_COMMIT_SHORT_SHA and latest"
        docker tag $IMAGE_BASE:$CI_COMMIT_SHORT_SHA $IMAGE_BASE:latest
        
        docker push $IMAGE_BASE:$CI_COMMIT_SHORT_SHA
        docker push $IMAGE_BASE:latest
        
        echo "IMAGE_TAG=$IMAGE_BASE:$CI_COMMIT_SHORT_SHA" >> build.env
      fi
      
      echo "IMAGE_LATEST=$IMAGE_BASE:latest" >> build.env
  artifacts:
    reports:
      dotenv: build.env
  rules:
    # Always build on MRs
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: always
    # Always build on main
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: always
    # Always build on version tags
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
      when: always
    - when: never

# ----------------------------
# Test Stage
# ----------------------------
tests:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  script:
    - python -m pytest -q --ignore=tests/integration --junitxml=junit.xml
  artifacts:
    when: always
    reports:
      junit: junit.xml
    paths:
      - junit.xml
      - .pytest_cache/
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
    - when: never

integration_tests_opm:
  stage: test
  image: docker:24
  services:
    - docker:24-dind
  needs:
    - job: build_opm_image
      artifacts: true
      optional: true
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
    DOCKER_HOST: tcp://docker:2376
    DOCKER_TLS_VERIFY: 1
    DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
    PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  before_script:
    - apk add --no-cache python3 py3-pip
    - python3 -m pip install --break-system-packages -e ".[dev]"
    - echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin $CI_REGISTRY
    - |
      # Pull the image we just built, or try latest
      if [ -n "$IMAGE_TAG" ]; then
        docker pull $IMAGE_TAG && docker tag $IMAGE_TAG opm-flow:latest
      else
        docker pull $CI_REGISTRY_IMAGE/opm-flow:latest && docker tag $CI_REGISTRY_IMAGE/opm-flow:latest opm-flow:latest || echo "No image available, tests will skip"
      fi
  script:
    - |
      # Register custom pytest marker to avoid warnings
      echo "[pytest]" > pytest.ini
      echo "markers = docker: marks tests as requiring docker" >> pytest.ini
      python3 -m pytest -v tests/integration/test_opm_flow.py --junitxml=junit_integration.xml || true
  artifacts:
    when: always
    reports:
      junit: junit_integration.xml
    paths:
      - junit_integration.xml
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: on_success
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
      when: on_success
    - when: never
  allow_failure: true

snapshot_tests:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  script:
    - |
      set -e
      mkdir -p tests/golden tests/golden/snapshots tests/golden/diffs

      echo '# Snapshot diffs' > tests/golden/summary.md
      echo '' >> tests/golden/summary.md
      echo '_No snapshot diffs produced._' >> tests/golden/summary.md
      echo '' >> tests/golden/summary.md

      python -m pytest -q \
        tests/golden/test_cli_help_snapshot.py \
        tests/golden/test_cli_demo_snapshot.py || EXIT=$?

      if [ -d tests/golden/diffs ] && ls -1 tests/golden/diffs/*.diff >/dev/null 2>&1; then
        echo '# Snapshot diffs' > tests/golden/summary.md
        echo '' >> tests/golden/summary.md
        for f in tests/golden/diffs/*.diff; do
          echo "## $(basename "$f")" >> tests/golden/summary.md
          echo '```diff' >> tests/golden/summary.md
          cat "$f" >> tests/golden/summary.md
          echo '```' >> tests/golden/summary.md
          echo '' >> tests/golden/summary.md
        done
      fi

      if [ -n "${EXIT:-}" ]; then exit "$EXIT"; fi
  artifacts:
    when: on_failure
    paths:
      - tests/golden/snapshots/
      - tests/golden/summary.md
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
    - when: never

contract_tests:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  script:
    - |
      set -e
      mkdir -p tests/contracts
      python -m pytest -q tests/contracts 2>&1 | tee pytest_contracts.log || EXIT=$?
      python scripts/generate_contract_summary.py || true
      if [ -n "${EXIT:-}" ]; then exit "$EXIT"; fi
  artifacts:
    when: on_failure
    paths:
      - tests/contracts/summary.md
      - pytest_contracts.log
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - if: '$CI_COMMIT_TAG =~ /^v\d+\.\d+\.\d+$/'
    - when: never

governance_impact:
  stage: test
  <<: *python_base
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - when: never
  script:
    - mkdir -p tests/governance
    - python scripts/detect_governance_impact.py
  artifacts:
    when: always
    paths:
      - tests/governance/impact.md

architecture_graphs:
  stage: test
  image:
    name: ghcr.io/mermaid-js/mermaid-cli/mermaid-cli:latest
    entrypoint: [""]
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: on_success
    - when: never
  script:
    - mkdir -p docs/architecture/diagrams docs/architecture/rendered
    - mmdc --version || true
    - bash scripts/render_mermaid.sh || echo "Mermaid render failed; keeping sources."
  artifacts:
    when: always
    paths:
      - docs/architecture/diagrams/
      - docs/architecture/rendered/
  allow_failure: true


mkdocs_nav_check:
  stage: test
  image: python:3.11-slim
  script:
    - pip install pyyaml -q
    - python scripts/check_mkdocs_nav.py
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - when: never

tests_rerun:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  <<: *rerun_rules
  needs:
    - job: tests
      artifacts: true
      optional: true
  script:
    - |
      set +e
      python -m pytest -q \
        --last-failed \
        --last-failed-no-failures=none \
        --maxfail=${CI_BOT_RERUN_MAXFAIL:-1} \
        -x \
        --junitxml=junit_rerun.xml
      echo $? > rerun_exit_code.txt
      exit 0
  artifacts:
    when: always
    paths:
      - junit_rerun.xml
      - rerun_exit_code.txt

# ----------------------------
# Classify Stage
# ----------------------------
ci_classify:
  stage: classify
  <<: *python_base
  needs:
    - job: tests
      artifacts: true
      optional: true
    - job: tests_rerun
      artifacts: true
      optional: true
  script:
    - python scripts/ci_classify.py
  artifacts:
    reports:
      dotenv: ci_classify.env
    paths:
      - ci_classify.env
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
    - when: never

# ----------------------------
# Automation Stage
# ----------------------------
mr_report:
  stage: automation
  <<: *python_base
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - when: never
  needs:
    - job: snapshot_tests
      artifacts: true
      optional: true
    - job: contract_tests
      artifacts: true
      optional: true
    - job: governance_impact
      artifacts: true
      optional: true
    - job: architecture_graphs
      artifacts: true
      optional: true
    - job: integration_tests_opm
      artifacts: true
      optional: true
  script:
    - mkdir -p reports
    - python scripts/generate_mr_report.py
    - python scripts/render_report_html.py || true
  artifacts:
    when: always
    paths:
      - reports/mr_report.md
      - reports/mr_report.html

ci_flaky_ledger:
  stage: automation
  <<: *python_base
  <<: *issue_bot_rules
  needs:
    - job: ci_classify
      artifacts: true
      optional: true
  script:
    - python scripts/gitlab_flaky_ledger_bot.py

ci_issue_on_failure:
  stage: automation
  <<: *python_base
  <<: *issue_bot_rules
  needs:
    - job: ci_classify
      artifacts: true
      optional: true
  script:
    - python scripts/gitlab_issue_bot.py

ci_issue_on_recovery:
  stage: automation
  <<: *python_base
  <<: *recovery_bot_rules
  needs:
    - job: ci_classify
      artifacts: true
      optional: true
  script:
    - python scripts/gitlab_recovery_bot.py

mr_comment_on_failure:
  stage: automation
  <<: *python_base
  <<: *mr_bot_rules
  needs:
    - job: ci_classify
      artifacts: true
      optional: true
  script:
    - python scripts/gitlab_mr_bot.py

# ----------------------------
# Deploy Stage
# ----------------------------
pages:
  stage: deploy
  image: python:3.11
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      when: on_success
    - when: never
  before_script:
    - pip install mkdocs mkdocs-material pymdown-extensions jinja2 pyyaml
  script:
    # Build i18n docs from templates
    - python scripts/build_i18n_docs.py
    # Build MkDocs site from docs/ directory
    - mkdocs build --site-dir public
    # Copy static HTML files that MkDocs doesn't process (reveal.js slides, etc.)
    - |
      echo "Copying static HTML files..."
      mkdir -p public/guides/contributing
      cp docs/guides/contributing/*.html public/guides/contributing/ 2>/dev/null || echo "No HTML files to copy"
      ls -la public/guides/contributing/ 2>/dev/null || true
  artifacts:
    paths:
      - public
    expire_in: 1 week

llm_sync_package:
  stage: deploy
  image: python:3.11
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "push"'
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never
  script:
    - |
      BRANCH="${CI_MERGE_REQUEST_SOURCE_BRANCH_NAME:-${CI_COMMIT_BRANCH:-${CI_COMMIT_TAG:-main}}}"
      echo "Generating sync package for branch: $BRANCH"
      cd scripts
      python llm_sync_generator.py --branch "$BRANCH" --lite -o ../clarissa_sync_lite.md
      python llm_sync_generator.py --branch "$BRANCH" --medium -o ../clarissa_sync_medium.md
      python llm_sync_generator.py --branch "$BRANCH" --lite --since ${CI_COMMIT_BEFORE_SHA:-HEAD~5} -o ../clarissa_sync_diff.md || true
  artifacts:
    paths:
      - clarissa_sync_lite.md
      - clarissa_sync_medium.md
      - clarissa_sync_diff.md
    expire_in: 4 weeks

# ----------------------------
# Manual Trigger Jobs
# ----------------------------
# These jobs can be triggered via:
#   1. GitLab UI: CI/CD â†’ Pipelines â†’ Run Pipeline â†’ Select job
#   2. API: POST /projects/:id/pipeline with variables

rebuild_docs:
  stage: deploy
  image: python:3.11
  rules:
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  before_script:
    - pip install mkdocs mkdocs-material pymdown-extensions jinja2 pyyaml
  script:
    - echo "Rebuilding documentation..."
    - python scripts/build_i18n_docs.py
    - mkdocs build --site-dir public
    - |
      mkdir -p public/guides/contributing
      cp docs/guides/contributing/*.html public/guides/contributing/ 2>/dev/null || true
  artifacts:
    paths:
      - public
    expire_in: 1 day
  environment:
    name: docs-preview
    url: https://wolfram_laube.gitlab.io/blauweiss_llc/irena/

rebuild_opm_image:
  stage: build
  image: docker:24
  services:
    - docker:24-dind
  rules:
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
    DOCKER_HOST: tcp://docker:2376
    DOCKER_TLS_VERIFY: 1
    DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
    IMAGE_BASE: $CI_REGISTRY_IMAGE/opm-flow
  before_script:
    - docker info
    - echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin $CI_REGISTRY
  script:
    - |
      echo "Manual rebuild of OPM Flow image..."
      docker build --no-cache -t $IMAGE_BASE:$CI_COMMIT_SHORT_SHA -f src/clarissa/simulators/opm/Dockerfile src/clarissa/simulators/opm/
      docker tag $IMAGE_BASE:$CI_COMMIT_SHORT_SHA $IMAGE_BASE:latest
      docker push $IMAGE_BASE:$CI_COMMIT_SHORT_SHA
      docker push $IMAGE_BASE:latest
      echo "âœ… Pushed: $IMAGE_BASE:$CI_COMMIT_SHORT_SHA"
      echo "âœ… Pushed: $IMAGE_BASE:latest"

rerun_all_tests:
  stage: test
  <<: *python_base
  <<: *install_dev_deps
  rules:
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  script:
    - echo "Running full test suite (manual trigger)..."
    - python -m pytest -v --junitxml=junit_manual.xml
  artifacts:
    when: always
    reports:
      junit: junit_manual.xml
    paths:
      - junit_manual.xml

build_paper:
  stage: build
  image: texlive/texlive:latest
  rules:
    # Auto-run when paper files change
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - conference/**/*
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - conference/**/*
      when: on_success
    # Manual trigger always available
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  script:
    - cd conference/ijacsa-2026
    - echo "Building CLARISSA paper..."
    - pdflatex -interaction=nonstopmode CLARISSA_Paper_IJACSA.tex || true
    - pdflatex -interaction=nonstopmode CLARISSA_Paper_IJACSA.tex  # Second pass for references
    - echo "âœ… Paper built successfully"
    - ls -la *.pdf
  artifacts:
    paths:
      - conference/ijacsa-2026/CLARISSA_Paper_IJACSA.pdf
    expire_in: 4 weeks

build_invoice:
  stage: build
  image: alpine:latest
  rules:
    # Auto-run when invoice templates change
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - billing/templates/**/*
        - billing/output/**/*.typ
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - billing/templates/**/*
        - billing/output/**/*.typ
      when: on_success
    # Manual trigger always available
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
    - when: never
  before_script:
    - apk add --no-cache curl tar xz fontconfig ttf-liberation
    # Install Poppins font
    - mkdir -p /usr/share/fonts/poppins
    - curl -L "https://github.com/google/fonts/raw/main/ofl/poppins/Poppins-Regular.ttf" -o /usr/share/fonts/poppins/Poppins-Regular.ttf
    - curl -L "https://github.com/google/fonts/raw/main/ofl/poppins/Poppins-Bold.ttf" -o /usr/share/fonts/poppins/Poppins-Bold.ttf
    - curl -L "https://github.com/google/fonts/raw/main/ofl/poppins/Poppins-Medium.ttf" -o /usr/share/fonts/poppins/Poppins-Medium.ttf
    - fc-cache -fv
    # Install Typst
    - curl -L "https://github.com/typst/typst/releases/download/v0.12.0/typst-x86_64-unknown-linux-musl.tar.xz" -o /tmp/typst.tar.xz
    - tar xf /tmp/typst.tar.xz -C /tmp
    - mv /tmp/typst-x86_64-unknown-linux-musl/typst /usr/local/bin/
    - typst --version
  script:
    - echo "Building invoices with Typst..."
    - cd billing/output
    - cp ../templates/logo.jpg . 2>/dev/null || true
    - |
      for f in *.typ; do
        if [ -f "$f" ]; then
          echo "Compiling $f..."
          typst compile "$f"
        fi
      done
    - echo "âœ… Invoices built"
    - ls -la *.pdf 2>/dev/null || echo "No PDFs generated"
  artifacts:
    paths:
      - billing/output/*.pdf
    expire_in: 8 weeks

# Upload billing documents to Google Drive
upload_invoice:
  stage: deploy
  image: python:3.11-slim
  needs:
    - job: build_invoice
      artifacts: true
  rules:
    # Only run after successful build_invoice on main
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes:
        - billing/output/**/*.typ
      when: on_success
    # Manual trigger
    - if: '$CI_PIPELINE_SOURCE == "web"'
      when: manual
      allow_failure: true
    - when: never
  variables:
    GOOGLE_APPLICATION_CREDENTIALS: $GOOGLE_SERVICE_ACCOUNT_KEY
  script:
    - pip install google-auth google-api-python-client -q
    - |
      python3 << 'PYEOF'
      import os
      import json
      from pathlib import Path
      from google.oauth2 import service_account
      from googleapiclient.discovery import build
      from googleapiclient.http import MediaFileUpload

      SCOPES = ['https://www.googleapis.com/auth/drive.file']
      FOLDER_ID = os.environ.get('GOOGLE_DRIVE_FOLDER_ID')

      # Get credentials from file variable
      key_path = os.environ.get('GOOGLE_SERVICE_ACCOUNT_KEY')
      if os.path.isfile(key_path):
          credentials = service_account.Credentials.from_service_account_file(key_path, scopes=SCOPES)
      else:
          credentials = service_account.Credentials.from_service_account_info(json.loads(key_path), scopes=SCOPES)

      service = build('drive', 'v3', credentials=credentials)

      def get_or_create_folder(parent_id, name):
          query = f"name='{name}' and '{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"
          results = service.files().list(q=query, fields="files(id)").execute()
          if results.get('files'):
              return results['files'][0]['id']
          meta = {'name': name, 'mimeType': 'application/vnd.google-apps.folder', 'parents': [parent_id]}
          return service.files().create(body=meta, fields='id').execute()['id']

      def upload_file(folder_id, file_path):
          name = file_path.name
          query = f"name='{name}' and '{folder_id}' in parents and trashed=false"
          results = service.files().list(q=query, fields="files(id)").execute()
          media = MediaFileUpload(str(file_path), mimetype='application/pdf')
          
          if results.get('files'):
              service.files().update(fileId=results['files'][0]['id'], media_body=media).execute()
              print(f"ðŸ”„ Updated: {name}")
          else:
              meta = {'name': name, 'parents': [folder_id]}
              service.files().create(body=meta, media_body=media, fields='id').execute()
              print(f"âœ… Uploaded: {name}")

      # Find and upload all PDFs
      pdf_dir = Path('billing/output')
      pdfs = list(pdf_dir.glob('*.pdf'))

      if not pdfs:
          print("âš ï¸ No PDFs found to upload")
          exit(0)

      print(f"ðŸ“¤ Uploading {len(pdfs)} file(s) to Google Drive...")

      # Group by year/month from filename (e.g., 2026-01_timesheet_nemensis.pdf)
      for pdf in pdfs:
          # Try to extract year-month from filename
          name = pdf.stem
          if name.startswith('20') and '-' in name[:7]:
              year_month = name[:7]  # "2026-01"
              year, month = year_month.split('-')
              # Get client from filename
              parts = name.split('_')
              client = parts[2] if len(parts) > 2 else 'misc'
              folder_path = f"{year}/{month}_{client}"
          elif name.startswith('AR_'):
              # Invoice: AR_001_2026_client.pdf
              parts = name.split('_')
              year = parts[2] if len(parts) > 2 else '2026'
              client = parts[3] if len(parts) > 3 else 'misc'
              folder_path = f"{year}/{client}"
          else:
              folder_path = "misc"

          # Create folder path
          current = FOLDER_ID
          for folder in folder_path.split('/'):
              current = get_or_create_folder(current, folder)
          
          upload_file(current, pdf)

      print(f"\nðŸ”— https://drive.google.com/drive/folders/{FOLDER_ID}")
      PYEOF
    - echo "âœ… Upload complete"
