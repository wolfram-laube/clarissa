{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLARISSA Tutorial 08: RIGOR Benchmark Framework\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the RIGOR evaluation dimensions\n",
    "- Implement benchmark test cases\n",
    "- Score deck generation quality\n",
    "- Compare system performance across tiers\n",
    "\n",
    "**Prerequisites:** Notebooks 01-07\n",
    "\n",
    "**Estimated Time:** 45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RIGOR?\n",
    "\n",
    "**R**eservoir **I**nput **G**eneration **O**utput **R**eview\n",
    "\n",
    "A benchmark framework for evaluating conversational simulation systems across four dimensions:\n",
    "\n",
    "| Dimension | What it Measures | Example |\n",
    "|-----------|------------------|----------|\n",
    "| **Syntactic Validity** | Parser acceptance, keyword correctness | Does OPM Flow accept the deck? |\n",
    "| **Semantic Correctness** | Logical consistency, unit coherence | Are FIELD units used consistently? |\n",
    "| **Physical Plausibility** | Realistic parameters, sensible gradients | Is pressure gradient ~0.45 psi/ft? |\n",
    "| **Conversational Efficiency** | Turns to completion, clarification rate | How many questions asked? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Tuple, Callable\n",
    "from enum import Enum, auto\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"RIGOR Benchmark Framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Complexity Tiers\n",
    "\n",
    "RIGOR defines three complexity tiers for progressive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexityTier(Enum):\n",
    "    \"\"\"RIGOR complexity tiers.\"\"\"\n",
    "    FOUNDATIONAL = 1   # Simple models, single-phase\n",
    "    INTERMEDIATE = 2   # Multi-well, black-oil\n",
    "    ADVANCED = 3       # Compositional, mid-conversation changes\n",
    "\n",
    "@dataclass\n",
    "class TierSpec:\n",
    "    \"\"\"Specification for a complexity tier.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    max_cells: int\n",
    "    max_wells: int\n",
    "    phases: List[str]\n",
    "    features: List[str]\n",
    "    expected_turns: int  # Baseline for efficiency scoring\n",
    "\n",
    "TIER_SPECS = {\n",
    "    ComplexityTier.FOUNDATIONAL: TierSpec(\n",
    "        name=\"Foundational\",\n",
    "        description=\"Linear displacement, laboratory coreflood, single-phase flow\",\n",
    "        max_cells=1000,\n",
    "        max_wells=2,\n",
    "        phases=[\"OIL\", \"WATER\"],\n",
    "        features=[\"Cartesian grid\", \"Uniform properties\", \"Simple schedule\"],\n",
    "        expected_turns=5\n",
    "    ),\n",
    "    ComplexityTier.INTERMEDIATE: TierSpec(\n",
    "        name=\"Intermediate\",\n",
    "        description=\"Pattern flood, multi-well, black-oil\",\n",
    "        max_cells=50000,\n",
    "        max_wells=20,\n",
    "        phases=[\"OIL\", \"WATER\", \"GAS\"],\n",
    "        features=[\"5-spot pattern\", \"Variable permeability\", \"Well controls\"],\n",
    "        expected_turns=10\n",
    "    ),\n",
    "    ComplexityTier.ADVANCED: TierSpec(\n",
    "        name=\"Advanced\",\n",
    "        description=\"Compositional EOS, mid-conversation model changes\",\n",
    "        max_cells=500000,\n",
    "        max_wells=100,\n",
    "        phases=[\"COMPOSITIONAL\"],\n",
    "        features=[\"EOS modeling\", \"Thermal effects\", \"Model pivots\"],\n",
    "        expected_turns=20\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"RIGOR Complexity Tiers:\")\n",
    "print(\"=\" * 60)\n",
    "for tier, spec in TIER_SPECS.items():\n",
    "    print(f\"\\n{spec.name} (Tier {tier.value})\")\n",
    "    print(f\"  {spec.description}\")\n",
    "    print(f\"  Max cells: {spec.max_cells:,}\")\n",
    "    print(f\"  Max wells: {spec.max_wells}\")\n",
    "    print(f\"  Expected turns: {spec.expected_turns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Evaluation Dimensions\n",
    "\n",
    "Define scorers for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DimensionScore:\n",
    "    \"\"\"Score for a single evaluation dimension.\"\"\"\n",
    "    dimension: str\n",
    "    score: float  # 0.0 to 1.0\n",
    "    max_score: float = 1.0\n",
    "    details: List[str] = field(default_factory=list)\n",
    "    penalties: List[Tuple[str, float]] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def percentage(self) -> float:\n",
    "        return (self.score / self.max_score) * 100\n",
    "\n",
    "class SyntacticValidator:\n",
    "    \"\"\"Check syntactic validity of generated decks.\"\"\"\n",
    "    \n",
    "    # Required sections\n",
    "    REQUIRED_SECTIONS = ['RUNSPEC', 'GRID', 'PROPS', 'SOLUTION', 'SCHEDULE']\n",
    "    \n",
    "    # Basic keyword patterns\n",
    "    KEYWORD_PATTERNS = {\n",
    "        'DIMENS': r'DIMENS\\s+\\d+\\s+\\d+\\s+\\d+\\s*/',\n",
    "        'WELSPECS': r'WELSPECS[\\s\\S]*?/',\n",
    "        'COMPDAT': r'COMPDAT[\\s\\S]*?/',\n",
    "    }\n",
    "    \n",
    "    def validate(self, deck: str) -> DimensionScore:\n",
    "        \"\"\"Validate deck syntax.\"\"\"\n",
    "        score = 1.0\n",
    "        details = []\n",
    "        penalties = []\n",
    "        \n",
    "        # Check required sections\n",
    "        for section in self.REQUIRED_SECTIONS:\n",
    "            if section in deck:\n",
    "                details.append(f\"{section} present\")\n",
    "            else:\n",
    "                penalties.append((f\"Missing {section}\", 0.15))\n",
    "                score -= 0.15\n",
    "        \n",
    "        # Check keyword patterns\n",
    "        for keyword, pattern in self.KEYWORD_PATTERNS.items():\n",
    "            if re.search(pattern, deck):\n",
    "                details.append(f\"{keyword} valid\")\n",
    "            elif keyword in deck:\n",
    "                penalties.append((f\"{keyword} malformed\", 0.1))\n",
    "                score -= 0.1\n",
    "        \n",
    "        # Check for terminator\n",
    "        if deck.strip().endswith('END'):\n",
    "            details.append(\"END terminator present\")\n",
    "        else:\n",
    "            penalties.append((\"Missing END\", 0.05))\n",
    "            score -= 0.05\n",
    "        \n",
    "        return DimensionScore(\n",
    "            dimension=\"Syntactic Validity\",\n",
    "            score=max(0, score),\n",
    "            details=details,\n",
    "            penalties=penalties\n",
    "        )\n",
    "\n",
    "class SemanticValidator:\n",
    "    \"\"\"Check semantic correctness.\"\"\"\n",
    "    \n",
    "    def validate(self, deck: str, metadata: Dict) -> DimensionScore:\n",
    "        \"\"\"Validate semantic consistency.\"\"\"\n",
    "        score = 1.0\n",
    "        details = []\n",
    "        penalties = []\n",
    "        \n",
    "        # Check unit consistency\n",
    "        has_field = 'FIELD' in deck\n",
    "        has_metric = 'METRIC' in deck\n",
    "        \n",
    "        if has_field and has_metric:\n",
    "            penalties.append((\"Mixed unit systems\", 0.3))\n",
    "            score -= 0.3\n",
    "        elif has_field or has_metric:\n",
    "            details.append(f\"Unit system: {'FIELD' if has_field else 'METRIC'}\")\n",
    "        else:\n",
    "            penalties.append((\"No unit system specified\", 0.2))\n",
    "            score -= 0.2\n",
    "        \n",
    "        # Check grid dimensions match data\n",
    "        dimens_match = re.search(r'DIMENS\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)', deck)\n",
    "        if dimens_match:\n",
    "            nx, ny, nz = map(int, dimens_match.groups())\n",
    "            total = nx * ny * nz\n",
    "            details.append(f\"Grid: {nx}x{ny}x{nz} = {total:,} cells\")\n",
    "            \n",
    "            # Check PORO count matches\n",
    "            poro_match = re.search(r'PORO\\s+(\\d+)\\*', deck)\n",
    "            if poro_match:\n",
    "                poro_count = int(poro_match.group(1))\n",
    "                if poro_count == total:\n",
    "                    details.append(\"PORO count matches grid\")\n",
    "                else:\n",
    "                    penalties.append((f\"PORO count {poro_count} != grid {total}\", 0.2))\n",
    "                    score -= 0.2\n",
    "        \n",
    "        # Check well locations within grid\n",
    "        # (Simplified - would need full parsing in production)\n",
    "        \n",
    "        return DimensionScore(\n",
    "            dimension=\"Semantic Correctness\",\n",
    "            score=max(0, score),\n",
    "            details=details,\n",
    "            penalties=penalties\n",
    "        )\n",
    "\n",
    "class PhysicsValidator:\n",
    "    \"\"\"Check physical plausibility.\"\"\"\n",
    "    \n",
    "    # Typical ranges\n",
    "    RANGES = {\n",
    "        'porosity': (0.01, 0.40),\n",
    "        'permeability': (0.1, 10000),  # md\n",
    "        'pressure_gradient': (0.35, 0.55),  # psi/ft\n",
    "        'water_saturation': (0.0, 1.0),\n",
    "    }\n",
    "    \n",
    "    def validate(self, deck: str, metadata: Dict) -> DimensionScore:\n",
    "        \"\"\"Validate physics.\"\"\"\n",
    "        score = 1.0\n",
    "        details = []\n",
    "        penalties = []\n",
    "        \n",
    "        # Extract and check porosity\n",
    "        poro_match = re.search(r'PORO\\s+\\d+\\*([\\d.]+)', deck)\n",
    "        if poro_match:\n",
    "            poro = float(poro_match.group(1))\n",
    "            if self.RANGES['porosity'][0] <= poro <= self.RANGES['porosity'][1]:\n",
    "                details.append(f\"Porosity {poro:.2f} in range\")\n",
    "            else:\n",
    "                penalties.append((f\"Porosity {poro:.2f} out of range\", 0.2))\n",
    "                score -= 0.2\n",
    "        \n",
    "        # Extract and check permeability\n",
    "        permx_match = re.search(r'PERMX\\s+\\d+\\*([\\d.]+)', deck)\n",
    "        if permx_match:\n",
    "            perm = float(permx_match.group(1))\n",
    "            if self.RANGES['permeability'][0] <= perm <= self.RANGES['permeability'][1]:\n",
    "                details.append(f\"Permeability {perm:.0f} md in range\")\n",
    "            else:\n",
    "                penalties.append((f\"Permeability {perm:.0f} unusual\", 0.15))\n",
    "                score -= 0.15\n",
    "        \n",
    "        # Check pressure gradient (from EQUIL)\n",
    "        equil_match = re.search(r'EQUIL\\s+[\\d.]+\\s+([\\d.]+)', deck)\n",
    "        tops_match = re.search(r'TOPS\\s+\\d+\\*([\\d.]+)', deck)\n",
    "        if equil_match and tops_match:\n",
    "            pressure = float(equil_match.group(1))\n",
    "            depth = float(tops_match.group(1))\n",
    "            if depth > 0:\n",
    "                gradient = pressure / depth\n",
    "                if self.RANGES['pressure_gradient'][0] <= gradient <= self.RANGES['pressure_gradient'][1]:\n",
    "                    details.append(f\"Pressure gradient {gradient:.3f} psi/ft OK\")\n",
    "                else:\n",
    "                    penalties.append((f\"Pressure gradient {gradient:.3f} unusual\", 0.2))\n",
    "                    score -= 0.2\n",
    "        \n",
    "        return DimensionScore(\n",
    "            dimension=\"Physical Plausibility\",\n",
    "            score=max(0, score),\n",
    "            details=details,\n",
    "            penalties=penalties\n",
    "        )\n",
    "\n",
    "class EfficiencyValidator:\n",
    "    \"\"\"Measure conversational efficiency.\"\"\"\n",
    "    \n",
    "    def validate(self, conversation: List[Dict], tier: ComplexityTier) -> DimensionScore:\n",
    "        \"\"\"Score based on turns and clarifications.\"\"\"\n",
    "        spec = TIER_SPECS[tier]\n",
    "        \n",
    "        total_turns = len(conversation)\n",
    "        clarifications = sum(1 for msg in conversation \n",
    "                            if msg.get('role') == 'assistant' and '?' in msg.get('content', ''))\n",
    "        \n",
    "        details = [\n",
    "            f\"Total turns: {total_turns}\",\n",
    "            f\"Clarifications: {clarifications}\",\n",
    "            f\"Expected: {spec.expected_turns}\"\n",
    "        ]\n",
    "        penalties = []\n",
    "        \n",
    "        # Score based on turns vs expected\n",
    "        if total_turns <= spec.expected_turns:\n",
    "            score = 1.0\n",
    "            details.append(\"Under or at expected turns\")\n",
    "        else:\n",
    "            excess = total_turns - spec.expected_turns\n",
    "            penalty = min(0.5, excess * 0.05)\n",
    "            score = 1.0 - penalty\n",
    "            penalties.append((f\"{excess} excess turns\", penalty))\n",
    "        \n",
    "        # Penalty for excessive clarifications\n",
    "        if clarifications > spec.expected_turns / 2:\n",
    "            penalty = 0.1 * (clarifications - spec.expected_turns / 2)\n",
    "            penalties.append((\"Excessive clarifications\", min(0.3, penalty)))\n",
    "            score -= min(0.3, penalty)\n",
    "        \n",
    "        return DimensionScore(\n",
    "            dimension=\"Conversational Efficiency\",\n",
    "            score=max(0, score),\n",
    "            details=details,\n",
    "            penalties=penalties\n",
    "        )\n",
    "\n",
    "# Test validators\n",
    "print(\"Testing validators...\")\n",
    "\n",
    "sample_deck = '''RUNSPEC\n",
    "TITLE\n",
    "Test Model\n",
    "\n",
    "FIELD\n",
    "\n",
    "DIMENS\n",
    "  10 10 5 /\n",
    "\n",
    "GRID\n",
    "PORO\n",
    "  500*0.22 /\n",
    "PERMX\n",
    "  500*150 /\n",
    "TOPS\n",
    "  100*8500 /\n",
    "\n",
    "PROPS\n",
    "SOLUTION\n",
    "EQUIL\n",
    "  8500 3800 9500 0 0 0 1 /\n",
    "\n",
    "SCHEDULE\n",
    "WELSPECS\n",
    "  PROD1 G1 5 5 1* OIL /\n",
    "/\n",
    "COMPDAT\n",
    "  PROD1 5 5 1 5 OPEN /\n",
    "/\n",
    "END\n",
    "'''\n",
    "\n",
    "# Run validators\n",
    "syntactic = SyntacticValidator().validate(sample_deck)\n",
    "semantic = SemanticValidator().validate(sample_deck, {})\n",
    "physics = PhysicsValidator().validate(sample_deck, {})\n",
    "\n",
    "print(f\"\\nSyntactic: {syntactic.percentage:.0f}%\")\n",
    "print(f\"Semantic: {semantic.percentage:.0f}%\")\n",
    "print(f\"Physics: {physics.percentage:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Benchmark Test Cases\n",
    "\n",
    "Define specific test cases for each tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A single benchmark test case.\"\"\"\n",
    "    id: str\n",
    "    tier: ComplexityTier\n",
    "    name: str\n",
    "    description: str\n",
    "    user_prompt: str\n",
    "    expected_features: List[str]\n",
    "    validation_checks: List[Callable]\n",
    "\n",
    "# Tier 1 Test Cases\n",
    "TIER1_TESTS = [\n",
    "    TestCase(\n",
    "        id=\"T1-01\",\n",
    "        tier=ComplexityTier.FOUNDATIONAL,\n",
    "        name=\"Linear Coreflood\",\n",
    "        description=\"Simple 1D displacement model\",\n",
    "        user_prompt=\"Create a coreflood model: 20 cells in x-direction, \"\n",
    "                   \"water injection at one end, producer at the other. \"\n",
    "                   \"Porosity 0.25, permeability 100 md.\",\n",
    "        expected_features=[\"1D grid (nx>1, ny=1, nz=1)\", \"2 wells\", \"Water injection\"],\n",
    "        validation_checks=[]\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"T1-02\",\n",
    "        tier=ComplexityTier.FOUNDATIONAL,\n",
    "        name=\"Single Well Depletion\",\n",
    "        description=\"Radial flow to single producer\",\n",
    "        user_prompt=\"Model a single producer well in a 10x10x3 grid. \"\n",
    "                   \"Well at center, producing at 500 stb/d for 2 years.\",\n",
    "        expected_features=[\"3D grid\", \"1 producer\", \"Rate control\"],\n",
    "        validation_checks=[]\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Tier 2 Test Cases\n",
    "TIER2_TESTS = [\n",
    "    TestCase(\n",
    "        id=\"T2-01\",\n",
    "        tier=ComplexityTier.INTERMEDIATE,\n",
    "        name=\"5-Spot Waterflood\",\n",
    "        description=\"Classic pattern flood\",\n",
    "        user_prompt=\"Create a 5-spot waterflood pattern on 40-acre spacing. \"\n",
    "                   \"Depth 8500 ft, pressure 3800 psi. \"\n",
    "                   \"Run for 10 years with water injection.\",\n",
    "        expected_features=[\"5 wells\", \"4 injectors + 1 producer\", \"Pattern geometry\"],\n",
    "        validation_checks=[]\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"T2-02\",\n",
    "        tier=ComplexityTier.INTERMEDIATE,\n",
    "        name=\"Multi-Layer Model\",\n",
    "        description=\"Layered reservoir with varying properties\",\n",
    "        user_prompt=\"Build a model with 5 layers. Top 2 layers high perm (200md), \"\n",
    "                   \"middle layer shale barrier (1md), bottom 2 layers medium perm (50md). \"\n",
    "                   \"20x20 areal grid.\",\n",
    "        expected_features=[\"5 layers\", \"Variable permeability\", \"Barrier layer\"],\n",
    "        validation_checks=[]\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Tier 3 Test Cases\n",
    "TIER3_TESTS = [\n",
    "    TestCase(\n",
    "        id=\"T3-01\",\n",
    "        tier=ComplexityTier.ADVANCED,\n",
    "        name=\"Black-Oil to Compositional Pivot\",\n",
    "        description=\"Mid-conversation model type change\",\n",
    "        user_prompt=\"Start with a black-oil waterflood model for the Permian. \"\n",
    "                   \"[After initial model] Actually, we need to evaluate CO2 injection \"\n",
    "                   \"for tertiary recovery. Convert to compositional.\",\n",
    "        expected_features=[\"Model pivot\", \"EOS components\", \"CO2 properties\"],\n",
    "        validation_checks=[]\n",
    "    ),\n",
    "]\n",
    "\n",
    "ALL_TESTS = TIER1_TESTS + TIER2_TESTS + TIER3_TESTS\n",
    "\n",
    "print(f\"Benchmark suite: {len(ALL_TESTS)} test cases\")\n",
    "print(\"\\nTest case summary:\")\n",
    "for tier in ComplexityTier:\n",
    "    tests = [t for t in ALL_TESTS if t.tier == tier]\n",
    "    print(f\"  Tier {tier.value} ({tier.name}): {len(tests)} tests\")\n",
    "    for t in tests:\n",
    "        print(f\"    - {t.id}: {t.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Benchmark Runner\n",
    "\n",
    "Execute test cases and collect scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Result from running a single test case.\"\"\"\n",
    "    test_id: str\n",
    "    tier: ComplexityTier\n",
    "    syntactic: DimensionScore\n",
    "    semantic: DimensionScore\n",
    "    physics: DimensionScore\n",
    "    efficiency: DimensionScore\n",
    "    deck: str\n",
    "    conversation: List[Dict]\n",
    "    execution_time: float\n",
    "    \n",
    "    @property\n",
    "    def overall_score(self) -> float:\n",
    "        \"\"\"Weighted average of all dimensions.\"\"\"\n",
    "        weights = {\n",
    "            'syntactic': 0.25,\n",
    "            'semantic': 0.25,\n",
    "            'physics': 0.30,\n",
    "            'efficiency': 0.20\n",
    "        }\n",
    "        return (\n",
    "            self.syntactic.score * weights['syntactic'] +\n",
    "            self.semantic.score * weights['semantic'] +\n",
    "            self.physics.score * weights['physics'] +\n",
    "            self.efficiency.score * weights['efficiency']\n",
    "        )\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    \"\"\"Run RIGOR benchmark suite.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.syntactic_validator = SyntacticValidator()\n",
    "        self.semantic_validator = SemanticValidator()\n",
    "        self.physics_validator = PhysicsValidator()\n",
    "        self.efficiency_validator = EfficiencyValidator()\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "    \n",
    "    def run_test(self, test: TestCase, deck: str, \n",
    "                 conversation: List[Dict]) -> BenchmarkResult:\n",
    "        \"\"\"Run a single test case.\"\"\"\n",
    "        import time\n",
    "        start = time.time()\n",
    "        \n",
    "        # Run all validators\n",
    "        syntactic = self.syntactic_validator.validate(deck)\n",
    "        semantic = self.semantic_validator.validate(deck, {})\n",
    "        physics = self.physics_validator.validate(deck, {})\n",
    "        efficiency = self.efficiency_validator.validate(conversation, test.tier)\n",
    "        \n",
    "        result = BenchmarkResult(\n",
    "            test_id=test.id,\n",
    "            tier=test.tier,\n",
    "            syntactic=syntactic,\n",
    "            semantic=semantic,\n",
    "            physics=physics,\n",
    "            efficiency=efficiency,\n",
    "            deck=deck,\n",
    "            conversation=conversation,\n",
    "            execution_time=time.time() - start\n",
    "        )\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def run_suite(self, tests: List[TestCase], \n",
    "                  deck_generator: Callable) -> List[BenchmarkResult]:\n",
    "        \"\"\"Run full test suite with a deck generator function.\"\"\"\n",
    "        results = []\n",
    "        for test in tests:\n",
    "            # Generate deck (would call actual system in production)\n",
    "            deck, conversation = deck_generator(test.user_prompt)\n",
    "            result = self.run_test(test, deck, conversation)\n",
    "            results.append(result)\n",
    "            print(f\"  {test.id}: {result.overall_score:.0%}\")\n",
    "        return results\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate benchmark report.\"\"\"\n",
    "        lines = [\"RIGOR Benchmark Report\", \"=\" * 50, \"\"]\n",
    "        \n",
    "        # By tier\n",
    "        for tier in ComplexityTier:\n",
    "            tier_results = [r for r in self.results if r.tier == tier]\n",
    "            if tier_results:\n",
    "                lines.append(f\"\\n{TIER_SPECS[tier].name} (Tier {tier.value})\")\n",
    "                lines.append(\"-\" * 40)\n",
    "                \n",
    "                for r in tier_results:\n",
    "                    lines.append(f\"  {r.test_id}: {r.overall_score:.0%}\")\n",
    "                    lines.append(f\"    Syntactic: {r.syntactic.percentage:.0f}%\")\n",
    "                    lines.append(f\"    Semantic:  {r.semantic.percentage:.0f}%\")\n",
    "                    lines.append(f\"    Physics:   {r.physics.percentage:.0f}%\")\n",
    "                    lines.append(f\"    Efficiency:{r.efficiency.percentage:.0f}%\")\n",
    "                \n",
    "                avg = sum(r.overall_score for r in tier_results) / len(tier_results)\n",
    "                lines.append(f\"  Tier Average: {avg:.0%}\")\n",
    "        \n",
    "        # Overall\n",
    "        if self.results:\n",
    "            overall = sum(r.overall_score for r in self.results) / len(self.results)\n",
    "            lines.append(f\"\\n{'=' * 50}\")\n",
    "            lines.append(f\"Overall Score: {overall:.0%}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Mock deck generator for demo\n",
    "def mock_generator(prompt: str) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"Mock deck generator for testing.\"\"\"\n",
    "    # Return sample deck and conversation\n",
    "    return sample_deck, [\n",
    "        {'role': 'user', 'content': prompt},\n",
    "        {'role': 'assistant', 'content': 'I will create that model.'},\n",
    "        {'role': 'assistant', 'content': 'What porosity should I use?'},\n",
    "        {'role': 'user', 'content': '0.22'},\n",
    "        {'role': 'assistant', 'content': 'Here is your deck...'}\n",
    "    ]\n",
    "\n",
    "# Run benchmark\n",
    "runner = BenchmarkRunner()\n",
    "print(\"Running RIGOR benchmark...\\n\")\n",
    "runner.run_suite(TIER1_TESTS + TIER2_TESTS[:1], mock_generator)\n",
    "\n",
    "print(\"\\n\" + runner.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Leaderboard and Comparison\n",
    "\n",
    "Compare different system configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfig:\n",
    "    \"\"\"Configuration of a CLARISSA system variant.\"\"\"\n",
    "    name: str\n",
    "    llm_model: str\n",
    "    use_rl: bool\n",
    "    use_constraints: bool\n",
    "    use_analogs: bool\n",
    "\n",
    "class Leaderboard:\n",
    "    \"\"\"Track and compare system performance.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entries: List[Dict] = []\n",
    "    \n",
    "    def add_entry(self, config: SystemConfig, results: List[BenchmarkResult]):\n",
    "        \"\"\"Add benchmark results for a system configuration.\"\"\"\n",
    "        overall = sum(r.overall_score for r in results) / len(results) if results else 0\n",
    "        \n",
    "        # By dimension\n",
    "        dim_scores = {\n",
    "            'syntactic': sum(r.syntactic.score for r in results) / len(results),\n",
    "            'semantic': sum(r.semantic.score for r in results) / len(results),\n",
    "            'physics': sum(r.physics.score for r in results) / len(results),\n",
    "            'efficiency': sum(r.efficiency.score for r in results) / len(results),\n",
    "        }\n",
    "        \n",
    "        self.entries.append({\n",
    "            'config': config,\n",
    "            'overall': overall,\n",
    "            'dimensions': dim_scores,\n",
    "            'num_tests': len(results),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Sort by overall score\n",
    "        self.entries.sort(key=lambda x: x['overall'], reverse=True)\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display leaderboard.\"\"\"\n",
    "        print(\"\\nRIGOR Leaderboard\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Rank':<5} {'System':<25} {'Overall':<10} {'Syn':<8} {'Sem':<8} {'Phy':<8} {'Eff':<8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for i, entry in enumerate(self.entries, 1):\n",
    "            config = entry['config']\n",
    "            dims = entry['dimensions']\n",
    "            print(f\"{i:<5} {config.name:<25} {entry['overall']:<10.0%} \"\n",
    "                  f\"{dims['syntactic']:<8.0%} {dims['semantic']:<8.0%} \"\n",
    "                  f\"{dims['physics']:<8.0%} {dims['efficiency']:<8.0%}\")\n",
    "\n",
    "# Demo leaderboard\n",
    "leaderboard = Leaderboard()\n",
    "\n",
    "# Add mock entries\n",
    "configs = [\n",
    "    SystemConfig(\"CLARISSA v0.1 (baseline)\", \"GPT-3.5\", False, False, False),\n",
    "    SystemConfig(\"CLARISSA v0.2 (+constraints)\", \"GPT-3.5\", False, True, False),\n",
    "    SystemConfig(\"CLARISSA v0.3 (+RL)\", \"GPT-4\", True, True, False),\n",
    "    SystemConfig(\"CLARISSA v0.4 (full)\", \"Claude-3\", True, True, True),\n",
    "]\n",
    "\n",
    "# Mock results with improving scores\n",
    "for i, config in enumerate(configs):\n",
    "    mock_results = []\n",
    "    base_score = 0.6 + i * 0.1\n",
    "    for test in TIER1_TESTS:\n",
    "        mock_results.append(BenchmarkResult(\n",
    "            test_id=test.id,\n",
    "            tier=test.tier,\n",
    "            syntactic=DimensionScore(\"Syntactic\", base_score + 0.05),\n",
    "            semantic=DimensionScore(\"Semantic\", base_score),\n",
    "            physics=DimensionScore(\"Physics\", base_score - 0.05),\n",
    "            efficiency=DimensionScore(\"Efficiency\", base_score + 0.1),\n",
    "            deck=\"\",\n",
    "            conversation=[],\n",
    "            execution_time=1.0\n",
    "        ))\n",
    "    leaderboard.add_entry(config, mock_results)\n",
    "\n",
    "leaderboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we learned:\n",
    "\n",
    "1. **RIGOR Framework**: Four evaluation dimensions for CUI simulation systems\n",
    "2. **Complexity Tiers**: Progressive difficulty from coreflood to compositional\n",
    "3. **Validators**: Syntactic, semantic, physics, and efficiency scoring\n",
    "4. **Test Cases**: Standardized prompts for benchmarking\n",
    "5. **Leaderboard**: Compare system configurations\n",
    "\n",
    "**Key Insight**: Systematic evaluation enables objective comparison and improvement tracking.\n",
    "\n",
    "**Next Tutorial:** [09_Full_Pipeline_Demo.ipynb](09_Full_Pipeline_Demo.ipynb) - End-to-end example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}