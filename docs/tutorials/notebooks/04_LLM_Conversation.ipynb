{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLARISSA Tutorial 04: LLM Conversation Layer\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Design effective prompts for reservoir simulation domain\n",
    "- Manage multi-turn conversations with context\n",
    "- Classify user intents and extract entities\n",
    "- Optimize context window usage\n",
    "\n",
    "**Prerequisites:** Notebooks 01-03 (ECLIPSE fundamentals, OPM Flow, Knowledge Layer)\n",
    "\n",
    "**Estimated Time:** 60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The LLM Conversation Layer is CLARISSA's brain - it interprets user intent, maintains conversation context, and orchestrates the other components. This tutorial covers:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| System Prompt | Define CLARISSA's role and capabilities |\n",
    "| Intent Classifier | Understand what the user wants |\n",
    "| Entity Extractor | Pull out reservoir parameters |\n",
    "| Conversation Manager | Track multi-turn state |\n",
    "| Context Optimizer | Fit relevant info in token limits |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - works in both GitPod and Colab\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from enum import Enum, auto\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Check for API keys (optional - we'll use mock responses if not available)\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "if OPENAI_API_KEY or ANTHROPIC_API_KEY:\n",
    "    print('API key found - will use real LLM calls')\n",
    "else:\n",
    "    print('No API key - using mock responses (tutorial still works!)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: System Prompt Design\n",
    "\n",
    "The system prompt defines CLARISSA's personality, capabilities, and constraints. A well-designed prompt is crucial for consistent, accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLARISSA_SYSTEM_PROMPT = '''\n",
    "You are CLARISSA (Conversational Language Agent for Reservoir Integrated Simulation System Analysis), \n",
    "an AI assistant specialized in helping reservoir engineers build and run simulations.\n",
    "\n",
    "## Your Capabilities\n",
    "1. Generate ECLIPSE-format simulation decks from natural language descriptions\n",
    "2. Explain reservoir simulation concepts and keywords\n",
    "3. Suggest reasonable defaults based on analog reservoirs\n",
    "4. Validate physics constraints (pressure, saturation, rates)\n",
    "5. Help interpret simulation results\n",
    "\n",
    "## Your Constraints\n",
    "- Always ask for clarification when specifications are ambiguous\n",
    "- Flag assumptions explicitly so engineers can review them\n",
    "- Never generate decks with physically impossible parameters\n",
    "- Recommend OPM Flow compatible keywords when possible\n",
    "\n",
    "## Response Format\n",
    "- For deck generation: provide complete, runnable sections\n",
    "- For explanations: be concise but technically accurate\n",
    "- For clarifications: ask one question at a time\n",
    "- Always specify units (FIELD or METRIC)\n",
    "\n",
    "## Current Session Context\n",
    "{context}\n",
    "'''\n",
    "\n",
    "def build_system_prompt(context: Dict[str, Any]) -> str:\n",
    "    \"\"\"Build system prompt with current session context.\"\"\"\n",
    "    context_str = json.dumps(context, indent=2) if context else \"No active model\"\n",
    "    return CLARISSA_SYSTEM_PROMPT.format(context=context_str)\n",
    "\n",
    "# Example context\n",
    "session_context = {\n",
    "    \"model_name\": \"Permian_Waterflood_v1\",\n",
    "    \"grid_size\": [20, 20, 5],\n",
    "    \"phases\": [\"OIL\", \"WATER\"],\n",
    "    \"units\": \"FIELD\",\n",
    "    \"wells_defined\": 5\n",
    "}\n",
    "\n",
    "print(build_system_prompt(session_context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Intent Classification\n",
    "\n",
    "CLARISSA needs to understand what the user wants to do. We define a set of intents and use pattern matching (or an LLM) to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserIntent(Enum):\n",
    "    \"\"\"Possible user intents in CLARISSA.\"\"\"\n",
    "    CREATE_MODEL = auto()      # Start a new simulation model\n",
    "    MODIFY_MODEL = auto()      # Change existing model parameters\n",
    "    ADD_WELL = auto()          # Add a well to the model\n",
    "    SET_SCHEDULE = auto()      # Define time steps and controls\n",
    "    RUN_SIMULATION = auto()    # Execute the simulation\n",
    "    EXPLAIN_CONCEPT = auto()   # Ask about reservoir concepts\n",
    "    EXPLAIN_KEYWORD = auto()   # Ask about ECLIPSE keywords\n",
    "    SHOW_RESULTS = auto()      # View simulation results\n",
    "    EXPORT_DECK = auto()       # Export the deck file\n",
    "    CLARIFY = auto()           # User providing clarification\n",
    "    UNKNOWN = auto()           # Cannot determine intent\n",
    "\n",
    "# Pattern-based intent classification (fast, no API needed)\n",
    "INTENT_PATTERNS = {\n",
    "    UserIntent.CREATE_MODEL: [\n",
    "        r'create.*model', r'new.*simulation', r'build.*deck',\n",
    "        r'start.*model', r'set up.*reservoir'\n",
    "    ],\n",
    "    UserIntent.ADD_WELL: [\n",
    "        r'add.*well', r'new.*well', r'place.*well',\n",
    "        r'drill.*well', r'producer', r'injector'\n",
    "    ],\n",
    "    UserIntent.MODIFY_MODEL: [\n",
    "        r'change.*perm', r'modify.*poro', r'update.*grid',\n",
    "        r'set.*pressure', r'adjust'\n",
    "    ],\n",
    "    UserIntent.RUN_SIMULATION: [\n",
    "        r'run.*sim', r'execute', r'start.*run', r'simulate'\n",
    "    ],\n",
    "    UserIntent.EXPLAIN_CONCEPT: [\n",
    "        r'what is', r'explain', r'how does', r'why',\n",
    "        r'tell me about', r'describe'\n",
    "    ],\n",
    "    UserIntent.EXPLAIN_KEYWORD: [\n",
    "        r'keyword', r'WELSPECS', r'COMPDAT', r'WCONPROD',\n",
    "        r'EQUIL', r'syntax'\n",
    "    ],\n",
    "    UserIntent.EXPORT_DECK: [\n",
    "        r'export', r'save.*deck', r'download', r'generate.*file'\n",
    "    ],\n",
    "    UserIntent.SHOW_RESULTS: [\n",
    "        r'show.*result', r'plot', r'graph', r'production.*rate',\n",
    "        r'water.*cut', r'pressure.*profile'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def classify_intent_patterns(user_message: str) -> Tuple[UserIntent, float]:\n",
    "    \"\"\"Classify intent using regex patterns.\n",
    "    \n",
    "    Returns (intent, confidence).\n",
    "    \"\"\"\n",
    "    message_lower = user_message.lower()\n",
    "    \n",
    "    for intent, patterns in INTENT_PATTERNS.items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, message_lower):\n",
    "                return intent, 0.8  # Pattern match = 80% confidence\n",
    "    \n",
    "    return UserIntent.UNKNOWN, 0.3\n",
    "\n",
    "# Test intent classification\n",
    "test_messages = [\n",
    "    \"I want to create a new waterflood model\",\n",
    "    \"Add a producer well at coordinates 10,10\",\n",
    "    \"What is relative permeability?\",\n",
    "    \"Run the simulation for 5 years\",\n",
    "    \"Show me the oil production rate\",\n",
    "    \"Something completely random\"\n",
    "]\n",
    "\n",
    "print(\"Intent Classification Results:\")\n",
    "print(\"-\" * 60)\n",
    "for msg in test_messages:\n",
    "    intent, conf = classify_intent_patterns(msg)\n",
    "    print(f\"{msg[:40]:40} -> {intent.name:20} ({conf:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Entity Extraction\n",
    "\n",
    "Once we know the intent, we need to extract specific parameters (entities) from the user's message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExtractedEntities:\n",
    "    \"\"\"Entities extracted from user message.\"\"\"\n",
    "    well_names: List[str] = field(default_factory=list)\n",
    "    coordinates: List[Tuple[int, int]] = field(default_factory=list)\n",
    "    depths: List[float] = field(default_factory=list)\n",
    "    rates: List[float] = field(default_factory=list)\n",
    "    pressures: List[float] = field(default_factory=list)\n",
    "    percentages: List[float] = field(default_factory=list)\n",
    "    time_values: List[Tuple[float, str]] = field(default_factory=list)  # (value, unit)\n",
    "    keywords: List[str] = field(default_factory=list)\n",
    "    well_types: List[str] = field(default_factory=list)  # producer, injector\n",
    "    fluids: List[str] = field(default_factory=list)  # oil, water, gas\n",
    "\n",
    "def extract_entities(message: str) -> ExtractedEntities:\n",
    "    \"\"\"Extract reservoir-specific entities from user message.\"\"\"\n",
    "    entities = ExtractedEntities()\n",
    "    message_lower = message.lower()\n",
    "    \n",
    "    # Well names (P1, INJ-1, PROD_01, etc.)\n",
    "    well_pattern = r'\\b([A-Z]+[-_]?\\d+)\\b'\n",
    "    entities.well_names = re.findall(well_pattern, message)\n",
    "    \n",
    "    # Coordinates (10,10 or i=10, j=10)\n",
    "    coord_pattern = r'(?:at\\s+)?(?:coordinates?\\s+)?(\\d+)\\s*[,\\s]\\s*(\\d+)'\n",
    "    coords = re.findall(coord_pattern, message_lower)\n",
    "    entities.coordinates = [(int(i), int(j)) for i, j in coords]\n",
    "    \n",
    "    # Depths (8000 ft, 2500m)\n",
    "    depth_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:ft|feet|m|meters?)\\s*(?:depth|deep|tvd)?'\n",
    "    entities.depths = [float(d) for d in re.findall(depth_pattern, message_lower)]\n",
    "    \n",
    "    # Rates (1000 stb/d, 5000 bpd, 10 mscf/d)\n",
    "    rate_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:stb/d|bpd|bbl/d|mscf/d|scf/d)'\n",
    "    entities.rates = [float(r) for r in re.findall(rate_pattern, message_lower)]\n",
    "    \n",
    "    # Pressures (3000 psi, 200 bar)\n",
    "    pressure_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:psi|psia|bar|kpa)'\n",
    "    entities.pressures = [float(p) for p in re.findall(pressure_pattern, message_lower)]\n",
    "    \n",
    "    # Percentages (20%, 0.2 porosity)\n",
    "    pct_pattern = r'(\\d+(?:\\.\\d+)?)\\s*%'\n",
    "    entities.percentages = [float(p) for p in re.findall(pct_pattern, message_lower)]\n",
    "    \n",
    "    # Time values (5 years, 365 days, 12 months)\n",
    "    time_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(years?|months?|days?|weeks?)'\n",
    "    entities.time_values = [(float(v), u) for v, u in re.findall(time_pattern, message_lower)]\n",
    "    \n",
    "    # ECLIPSE keywords\n",
    "    keywords = ['WELSPECS', 'COMPDAT', 'WCONPROD', 'WCONINJE', 'EQUIL', \n",
    "                'PERMX', 'PORO', 'SWOF', 'PVDO', 'TSTEP', 'DIMENS']\n",
    "    entities.keywords = [kw for kw in keywords if kw in message.upper()]\n",
    "    \n",
    "    # Well types\n",
    "    if 'producer' in message_lower or 'production' in message_lower:\n",
    "        entities.well_types.append('producer')\n",
    "    if 'injector' in message_lower or 'injection' in message_lower:\n",
    "        entities.well_types.append('injector')\n",
    "    \n",
    "    # Fluids\n",
    "    for fluid in ['oil', 'water', 'gas']:\n",
    "        if fluid in message_lower:\n",
    "            entities.fluids.append(fluid)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Test entity extraction\n",
    "test_messages = [\n",
    "    \"Add a producer well P1 at coordinates 10, 15 with 1000 stb/d rate\",\n",
    "    \"Set the reservoir depth to 8500 ft and initial pressure to 3800 psi\",\n",
    "    \"Run simulation for 5 years with water injection at 2000 bpd\",\n",
    "    \"Change PERMX to 150 md and porosity to 22%\"\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    print(f\"\\nMessage: {msg}\")\n",
    "    entities = extract_entities(msg)\n",
    "    for field_name, value in entities.__dict__.items():\n",
    "        if value:\n",
    "            print(f\"  {field_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Conversation Manager\n",
    "\n",
    "The Conversation Manager tracks the state across multiple turns, maintaining context and handling clarifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    \"\"\"A single message in the conversation.\"\"\"\n",
    "    role: str  # 'user', 'assistant', 'system'\n",
    "    content: str\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "    intent: Optional[UserIntent] = None\n",
    "    entities: Optional[ExtractedEntities] = None\n",
    "\n",
    "@dataclass\n",
    "class ConversationState:\n",
    "    \"\"\"Current state of the simulation being built.\"\"\"\n",
    "    model_name: Optional[str] = None\n",
    "    grid_defined: bool = False\n",
    "    props_defined: bool = False\n",
    "    wells: List[Dict] = field(default_factory=list)\n",
    "    schedule_defined: bool = False\n",
    "    pending_clarification: Optional[str] = None\n",
    "    assumptions: List[str] = field(default_factory=list)\n",
    "\n",
    "class ConversationManager:\n",
    "    \"\"\"Manages multi-turn conversation state.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = 20):\n",
    "        self.messages: List[Message] = []\n",
    "        self.state = ConversationState()\n",
    "        self.max_history = max_history\n",
    "    \n",
    "    def add_user_message(self, content: str) -> Message:\n",
    "        \"\"\"Process and add a user message.\"\"\"\n",
    "        intent, confidence = classify_intent_patterns(content)\n",
    "        entities = extract_entities(content)\n",
    "        \n",
    "        msg = Message(\n",
    "            role='user',\n",
    "            content=content,\n",
    "            intent=intent,\n",
    "            entities=entities\n",
    "        )\n",
    "        self.messages.append(msg)\n",
    "        self._update_state(msg)\n",
    "        return msg\n",
    "    \n",
    "    def add_assistant_message(self, content: str) -> Message:\n",
    "        \"\"\"Add an assistant response.\"\"\"\n",
    "        msg = Message(role='assistant', content=content)\n",
    "        self.messages.append(msg)\n",
    "        return msg\n",
    "    \n",
    "    def _update_state(self, msg: Message):\n",
    "        \"\"\"Update conversation state based on user message.\"\"\"\n",
    "        if msg.intent == UserIntent.CREATE_MODEL:\n",
    "            self.state.model_name = \"New_Model\"\n",
    "        \n",
    "        if msg.entities and msg.entities.well_names:\n",
    "            for name in msg.entities.well_names:\n",
    "                if not any(w['name'] == name for w in self.state.wells):\n",
    "                    well_type = 'injector' if 'injector' in msg.entities.well_types else 'producer'\n",
    "                    self.state.wells.append({\n",
    "                        'name': name,\n",
    "                        'type': well_type,\n",
    "                        'coordinates': msg.entities.coordinates[0] if msg.entities.coordinates else None\n",
    "                    })\n",
    "    \n",
    "    def get_context_for_llm(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get conversation history formatted for LLM API.\"\"\"\n",
    "        # Keep only recent messages to fit context window\n",
    "        recent = self.messages[-self.max_history:]\n",
    "        return [\n",
    "            {'role': m.role, 'content': m.content}\n",
    "            for m in recent\n",
    "        ]\n",
    "    \n",
    "    def get_state_summary(self) -> str:\n",
    "        \"\"\"Get a summary of current model state.\"\"\"\n",
    "        lines = [\"Current Model State:\"]\n",
    "        lines.append(f\"  Model: {self.state.model_name or 'Not started'}\")\n",
    "        lines.append(f\"  Grid defined: {self.state.grid_defined}\")\n",
    "        lines.append(f\"  Properties defined: {self.state.props_defined}\")\n",
    "        lines.append(f\"  Wells: {len(self.state.wells)}\")\n",
    "        for w in self.state.wells:\n",
    "            lines.append(f\"    - {w['name']} ({w['type']}) at {w.get('coordinates', 'TBD')}\")\n",
    "        if self.state.assumptions:\n",
    "            lines.append(f\"  Assumptions made: {len(self.state.assumptions)}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Demo conversation\n",
    "conv = ConversationManager()\n",
    "\n",
    "# Simulate a multi-turn conversation\n",
    "turns = [\n",
    "    \"I want to create a new waterflood model for the Permian Basin\",\n",
    "    \"Add a producer well P1 at coordinates 10, 10\",\n",
    "    \"Add water injector INJ1 at 1, 1 with 2000 bpd injection rate\",\n",
    "]\n",
    "\n",
    "for turn in turns:\n",
    "    msg = conv.add_user_message(turn)\n",
    "    print(f\"\\nUser: {turn}\")\n",
    "    print(f\"  Intent: {msg.intent.name}\")\n",
    "    conv.add_assistant_message(f\"Processed: {msg.intent.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(conv.get_state_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Context Window Optimization\n",
    "\n",
    "LLMs have limited context windows. We need to intelligently select what information to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Rough token estimate (4 chars per token for English).\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "@dataclass\n",
    "class ContextBudget:\n",
    "    \"\"\"Token budget allocation for context window.\"\"\"\n",
    "    total: int = 8000  # Conservative for most models\n",
    "    system_prompt: int = 1000\n",
    "    conversation_history: int = 2000\n",
    "    retrieved_knowledge: int = 3000\n",
    "    current_deck: int = 1500\n",
    "    response_buffer: int = 500\n",
    "\n",
    "class ContextOptimizer:\n",
    "    \"\"\"Optimizes context window usage.\"\"\"\n",
    "    \n",
    "    def __init__(self, budget: ContextBudget = None):\n",
    "        self.budget = budget or ContextBudget()\n",
    "    \n",
    "    def build_context(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        conversation: List[Dict[str, str]],\n",
    "        knowledge: List[str],\n",
    "        current_deck: Optional[str] = None\n",
    "    ) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"Build optimized context within token budget.\n",
    "        \n",
    "        Returns (context_string, token_usage).\n",
    "        \"\"\"\n",
    "        usage = {}\n",
    "        parts = []\n",
    "        \n",
    "        # 1. System prompt (always include, truncate if needed)\n",
    "        sys_tokens = estimate_tokens(system_prompt)\n",
    "        if sys_tokens > self.budget.system_prompt:\n",
    "            # Truncate to fit\n",
    "            char_limit = self.budget.system_prompt * 4\n",
    "            system_prompt = system_prompt[:char_limit] + \"...\"\n",
    "        parts.append(system_prompt)\n",
    "        usage['system'] = estimate_tokens(system_prompt)\n",
    "        \n",
    "        # 2. Conversation history (most recent first)\n",
    "        conv_parts = []\n",
    "        conv_tokens = 0\n",
    "        for msg in reversed(conversation):\n",
    "            msg_text = f\"{msg['role']}: {msg['content']}\"\n",
    "            msg_tokens = estimate_tokens(msg_text)\n",
    "            if conv_tokens + msg_tokens > self.budget.conversation_history:\n",
    "                break\n",
    "            conv_parts.insert(0, msg_text)\n",
    "            conv_tokens += msg_tokens\n",
    "        if conv_parts:\n",
    "            parts.append(\"\\n--- Conversation History ---\\n\" + \"\\n\".join(conv_parts))\n",
    "        usage['conversation'] = conv_tokens\n",
    "        \n",
    "        # 3. Retrieved knowledge (prioritize by relevance)\n",
    "        knowledge_parts = []\n",
    "        knowledge_tokens = 0\n",
    "        for doc in knowledge:\n",
    "            doc_tokens = estimate_tokens(doc)\n",
    "            if knowledge_tokens + doc_tokens > self.budget.retrieved_knowledge:\n",
    "                break\n",
    "            knowledge_parts.append(doc)\n",
    "            knowledge_tokens += doc_tokens\n",
    "        if knowledge_parts:\n",
    "            parts.append(\"\\n--- Relevant Knowledge ---\\n\" + \"\\n\".join(knowledge_parts))\n",
    "        usage['knowledge'] = knowledge_tokens\n",
    "        \n",
    "        # 4. Current deck state (if available)\n",
    "        if current_deck:\n",
    "            deck_tokens = estimate_tokens(current_deck)\n",
    "            if deck_tokens > self.budget.current_deck:\n",
    "                # Summarize deck instead of including full text\n",
    "                current_deck = self._summarize_deck(current_deck)\n",
    "            parts.append(\"\\n--- Current Deck ---\\n\" + current_deck)\n",
    "            usage['deck'] = estimate_tokens(current_deck)\n",
    "        \n",
    "        usage['total'] = sum(usage.values())\n",
    "        usage['remaining'] = self.budget.total - usage['total']\n",
    "        \n",
    "        return \"\\n\".join(parts), usage\n",
    "    \n",
    "    def _summarize_deck(self, deck: str) -> str:\n",
    "        \"\"\"Create a summary of deck contents.\"\"\"\n",
    "        lines = deck.split('\\n')\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if stripped in ['RUNSPEC', 'GRID', 'PROPS', 'SOLUTION', 'SCHEDULE']:\n",
    "                current_section = stripped\n",
    "                sections.append(current_section)\n",
    "        \n",
    "        return f\"Deck contains sections: {', '.join(sections)}\\n(Full deck truncated for context)\"\n",
    "\n",
    "# Demo context optimization\n",
    "optimizer = ContextOptimizer()\n",
    "\n",
    "context, usage = optimizer.build_context(\n",
    "    system_prompt=CLARISSA_SYSTEM_PROMPT.format(context=\"Demo model\"),\n",
    "    conversation=[\n",
    "        {'role': 'user', 'content': 'Create a waterflood model'},\n",
    "        {'role': 'assistant', 'content': 'I will create a 5-spot waterflood pattern.'},\n",
    "        {'role': 'user', 'content': 'Add a producer at 10,10'}\n",
    "    ],\n",
    "    knowledge=[\n",
    "        \"WELSPECS defines well name, group, and location...\",\n",
    "        \"Typical Permian Basin permeability: 50-200 md...\"\n",
    "    ],\n",
    "    current_deck=\"RUNSPEC\\nDIMENS\\n10 10 5 /\\n...\"\n",
    ")\n",
    "\n",
    "print(\"Token Usage:\")\n",
    "for key, value in usage.items():\n",
    "    print(f\"  {key}: {value} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: LLM Integration (Mock + Real)\n",
    "\n",
    "Now we tie it all together with actual LLM calls (or mock responses for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockLLM:\n",
    "    \"\"\"Mock LLM for testing without API keys.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.responses = {\n",
    "            UserIntent.CREATE_MODEL: \"I'll help you create a new simulation model. What type of reservoir are you modeling? (e.g., waterflood, gas reservoir, compositional)\",\n",
    "            UserIntent.ADD_WELL: \"I'll add the well to your model. Let me generate the WELSPECS and COMPDAT entries.\",\n",
    "            UserIntent.EXPLAIN_CONCEPT: \"Here's an explanation of that concept...\",\n",
    "            UserIntent.RUN_SIMULATION: \"I'll prepare your deck for simulation. Let me verify all required sections are complete.\",\n",
    "            UserIntent.UNKNOWN: \"I'm not sure what you're asking. Could you please clarify?\"\n",
    "        }\n",
    "    \n",
    "    def generate(self, messages: List[Dict], intent: UserIntent = None) -> str:\n",
    "        \"\"\"Generate a mock response.\"\"\"\n",
    "        if intent and intent in self.responses:\n",
    "            return self.responses[intent]\n",
    "        return self.responses[UserIntent.UNKNOWN]\n",
    "\n",
    "class CLARISSAAgent:\n",
    "    \"\"\"Main CLARISSA conversation agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_real_llm: bool = False):\n",
    "        self.conversation = ConversationManager()\n",
    "        self.optimizer = ContextOptimizer()\n",
    "        self.llm = MockLLM()  # Use mock by default\n",
    "        self.knowledge_cache: List[str] = []\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"Process user input and generate response.\"\"\"\n",
    "        # 1. Add and analyze user message\n",
    "        msg = self.conversation.add_user_message(user_input)\n",
    "        \n",
    "        # 2. Build optimized context\n",
    "        context, usage = self.optimizer.build_context(\n",
    "            system_prompt=build_system_prompt({}),\n",
    "            conversation=self.conversation.get_context_for_llm(),\n",
    "            knowledge=self.knowledge_cache\n",
    "        )\n",
    "        \n",
    "        # 3. Generate response\n",
    "        response = self.llm.generate(\n",
    "            self.conversation.get_context_for_llm(),\n",
    "            intent=msg.intent\n",
    "        )\n",
    "        \n",
    "        # 4. Add response to history\n",
    "        self.conversation.add_assistant_message(response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_status(self) -> str:\n",
    "        \"\"\"Get current conversation and model status.\"\"\"\n",
    "        return self.conversation.get_state_summary()\n",
    "\n",
    "# Demo the agent\n",
    "agent = CLARISSAAgent()\n",
    "\n",
    "print(\"CLARISSA Conversation Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "demo_inputs = [\n",
    "    \"I want to create a waterflood model for the Permian Basin\",\n",
    "    \"Add a producer well P1 at location 10, 10\",\n",
    "    \"Run the simulation for 5 years\"\n",
    "]\n",
    "\n",
    "for user_input in demo_inputs:\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    response = agent.chat(user_input)\n",
    "    print(f\"CLARISSA: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(agent.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we learned:\n",
    "\n",
    "1. **System Prompt Design**: Define CLARISSA's role, capabilities, and constraints\n",
    "2. **Intent Classification**: Understand what the user wants using patterns or LLM\n",
    "3. **Entity Extraction**: Pull reservoir-specific parameters from natural language\n",
    "4. **Conversation Management**: Track state across multiple turns\n",
    "5. **Context Optimization**: Fit relevant information within token limits\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Domain-specific prompts improve accuracy\n",
    "- Hybrid approach (patterns + LLM) balances speed and accuracy\n",
    "- Context window management is critical for complex models\n",
    "\n",
    "**Next Tutorial:** [05_Constraint_Engine.ipynb](05_Constraint_Engine.ipynb) - Physics validation with Z3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}