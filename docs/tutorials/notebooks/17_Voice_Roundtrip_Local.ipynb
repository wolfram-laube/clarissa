{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤ CLARISSA Voice Roundtrip - Local Testing\n",
    "\n",
    "**Cross-Platform Voice Pipeline Demo**\n",
    "\n",
    "This notebook demonstrates the complete voice roundtrip:\n",
    "```\n",
    "ğŸ¤ Microphone â†’ ğŸ“ WAV File â†’ ğŸ—£ï¸ Whisper STT â†’ ğŸ§  Intent Parser â†’ âš¡ Result\n",
    "```\n",
    "\n",
    "**Works on:** macOS, Windows, Linux  \n",
    "**Requirements:** Python 3.9+, microphone access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Colab Setup â€” API Keys & Dependencies\n",
    "import sys, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q anthropic openai\n",
    "    from google.colab import userdata\n",
    "    # Set keys from Colab Secrets (Settings â†’ Secrets â†’ Add)\n",
    "    try: os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "    except: pass\n",
    "    try: os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "    except: pass\n",
    "    try: os.environ['GITLAB_TOKEN'] = userdata.get('GITLAB_TOKEN')\n",
    "    except: pass\n",
    "    # Fallback: manual input\n",
    "    if not os.environ.get('ANTHROPIC_API_KEY'):\n",
    "        import getpass\n",
    "        for key in ['ANTHROPIC_API_KEY', 'OPENAI_API_KEY', 'GITLAB_TOKEN']:\n",
    "            if not os.environ.get(key):\n",
    "                try: os.environ[key] = getpass.getpass(f'{key}: ')\n",
    "                except: pass\n",
    "\n",
    "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n",
    "for k in ['ANTHROPIC_API_KEY','OPENAI_API_KEY','GITLAB_TOKEN']:\n",
    "    print(f'  {k}: {\"âœ“\" if os.environ.get(k) else \"âœ—\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Installation\n",
    "\n",
    "Run this cell to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-platform audio recording + API clients\n",
    "!pip install -q sounddevice scipy numpy openai anthropic httpx\n",
    "\n",
    "print(\"âœ… Packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Configuration\n",
    "\n",
    "Set your API keys (at least one required):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option A: Set directly (for quick testing)\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
    "# os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'\n",
    "\n",
    "# Option B: Load from .env file or environment\n",
    "# Keys should already be set in your shell profile\n",
    "\n",
    "# Check what's available\n",
    "openai_ok = bool(os.getenv('OPENAI_API_KEY'))\n",
    "anthropic_ok = bool(os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "print(\"ğŸ”‘ API Key Status:\")\n",
    "print(f\"   OpenAI:    {'âœ… Set' if openai_ok else 'âŒ Not set (required for Whisper STT)'}\")\n",
    "print(f\"   Anthropic: {'âœ… Set' if anthropic_ok else 'âš ï¸ Not set (optional for intent parsing)'}\")\n",
    "\n",
    "if not openai_ok:\n",
    "    print(\"\\nâš ï¸ OpenAI API key required for Whisper transcription!\")\n",
    "    print(\"   Set it with: os.environ['OPENAI_API_KEY'] = 'sk-...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Audio Recording Module\n",
    "\n",
    "Cross-platform microphone recording using `sounddevice`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Audio settings\n",
    "SAMPLE_RATE = 16000  # Whisper expects 16kHz\n",
    "CHANNELS = 1         # Mono\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"Show available audio input devices.\"\"\"\n",
    "    print(\"ğŸ¤ Available Audio Input Devices:\")\n",
    "    print(\"-\" * 50)\n",
    "    devices = sd.query_devices()\n",
    "    for i, d in enumerate(devices):\n",
    "        if d['max_input_channels'] > 0:\n",
    "            default = \" (DEFAULT)\" if i == sd.default.device[0] else \"\"\n",
    "            print(f\"   [{i}] {d['name']}{default}\")\n",
    "    print(\"-\" * 50)\n",
    "    return sd.default.device[0]\n",
    "\n",
    "def record_audio(duration_seconds: float = 5.0, device: int = None) -> tuple[np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Record audio from microphone.\n",
    "    \n",
    "    Args:\n",
    "        duration_seconds: Recording duration\n",
    "        device: Audio device index (None = default)\n",
    "    \n",
    "    Returns:\n",
    "        (audio_data, wav_path)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”´ Recording for {duration_seconds} seconds...\")\n",
    "    print(\"   Speak now!\")\n",
    "    \n",
    "    # Record\n",
    "    audio = sd.rec(\n",
    "        int(duration_seconds * SAMPLE_RATE),\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        channels=CHANNELS,\n",
    "        dtype='int16',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Show countdown\n",
    "    for i in range(int(duration_seconds), 0, -1):\n",
    "        print(f\"   â±ï¸ {i}...\", end='\\r')\n",
    "        time.sleep(1)\n",
    "    \n",
    "    sd.wait()  # Wait for recording to complete\n",
    "    print(\"\\nâœ… Recording complete!\")\n",
    "    \n",
    "    # Save to WAV file\n",
    "    wav_path = tempfile.mktemp(suffix='.wav')\n",
    "    wav.write(wav_path, SAMPLE_RATE, audio)\n",
    "    \n",
    "    # Calculate stats\n",
    "    audio_flat = audio.flatten()\n",
    "    rms = np.sqrt(np.mean(audio_flat.astype(np.float32)**2))\n",
    "    peak = np.max(np.abs(audio_flat))\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Audio Stats:\")\n",
    "    print(f\"   Duration: {len(audio)/SAMPLE_RATE:.1f}s\")\n",
    "    print(f\"   RMS Level: {rms:.0f} (good if > 500)\")\n",
    "    print(f\"   Peak Level: {peak} / 32767\")\n",
    "    print(f\"   File: {wav_path}\")\n",
    "    \n",
    "    if rms < 200:\n",
    "        print(\"\\nâš ï¸ Audio level very low - check microphone!\")\n",
    "    \n",
    "    return audio, wav_path\n",
    "\n",
    "# List devices\n",
    "default_device = list_audio_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Speech-to-Text (Whisper API)\n",
    "\n",
    "Transcribe audio using OpenAI's Whisper with reservoir domain vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import io\n",
    "\n",
    "# Domain vocabulary to improve recognition\n",
    "DOMAIN_VOCABULARY = \"\"\"\n",
    "Reservoir simulation terms: permeability, porosity, water saturation, \n",
    "oil saturation, pressure, BHP, bottomhole pressure, OOIP, STOIIP,\n",
    "waterflood, injector, producer, PROD1, INJ1, INJ2, INJ3, INJ4,\n",
    "millidarcy, mD, psi, bar, bbl/day, STB, FOPT, FOPR, FWPT, FWPR, FWCT,\n",
    "water cut, GOR, gas-oil ratio, layer, grid, cell, timestep,\n",
    "3D visualization, cross-section, animation, ECLIPSE, OPM Flow\n",
    "\"\"\"\n",
    "\n",
    "async def transcribe_audio(wav_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Transcribe audio file using OpenAI Whisper API.\n",
    "    \n",
    "    Returns:\n",
    "        {'text': str, 'latency_ms': int, 'cost_usd': float}\n",
    "    \"\"\"\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY not set!\")\n",
    "    \n",
    "    print(\"\\nğŸ—£ï¸ Transcribing with Whisper API...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with open(wav_path, 'rb') as f:\n",
    "        audio_bytes = f.read()\n",
    "    \n",
    "    # Estimate duration for cost calculation\n",
    "    audio_duration_s = len(audio_bytes) / (SAMPLE_RATE * 2)  # 16-bit = 2 bytes\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        response = await client.post(\n",
    "            \"https://api.openai.com/v1/audio/transcriptions\",\n",
    "            headers={\"Authorization\": f\"Bearer {api_key}\"},\n",
    "            files={\"file\": (\"audio.wav\", audio_bytes, \"audio/wav\")},\n",
    "            data={\n",
    "                \"model\": \"whisper-1\",\n",
    "                \"language\": \"en\",\n",
    "                \"prompt\": DOMAIN_VOCABULARY,\n",
    "                \"response_format\": \"json\"\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    latency_ms = int((time.time() - start_time) * 1000)\n",
    "    cost_usd = (audio_duration_s / 60) * 0.006  # $0.006/minute\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Whisper API error: {response.text}\")\n",
    "    \n",
    "    text = response.json().get('text', '').strip()\n",
    "    \n",
    "    print(f\"\\nğŸ“ Transcription Result:\")\n",
    "    print(f\"   Text: \\\"{text}\\\"\")\n",
    "    print(f\"   Latency: {latency_ms}ms\")\n",
    "    print(f\"   Cost: ${cost_usd:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'latency_ms': latency_ms,\n",
    "        'cost_usd': cost_usd\n",
    "    }\n",
    "\n",
    "# Sync wrapper\n",
    "def transcribe_audio_sync(wav_path: str) -> dict:\n",
    "    import asyncio\n",
    "    return asyncio.get_event_loop().run_until_complete(transcribe_audio(wav_path))\n",
    "\n",
    "print(\"âœ… Transcription module ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Intent Parser\n",
    "\n",
    "Parse transcribed text into structured intents using rules + LLM fallback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Optional\n",
    "import re\n",
    "import json\n",
    "\n",
    "class IntentType(Enum):\n",
    "    VISUALIZE_PROPERTY = \"visualize_property\"\n",
    "    QUERY_VALUE = \"query_value\"\n",
    "    MODIFY_PARAMETER = \"modify_parameter\"\n",
    "    RUN_SIMULATION = \"run_simulation\"\n",
    "    NAVIGATE = \"navigate\"\n",
    "    HELP = \"help\"\n",
    "    CONFIRM = \"confirm\"\n",
    "    CANCEL = \"cancel\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class Intent:\n",
    "    type: IntentType\n",
    "    confidence: float\n",
    "    slots: Dict[str, Any] = field(default_factory=dict)\n",
    "    raw_text: str = \"\"\n",
    "    parse_method: str = \"unknown\"\n",
    "\n",
    "def parse_with_rules(text: str) -> Optional[Intent]:\n",
    "    \"\"\"\n",
    "    Rule-based intent parsing - works WITHOUT any API.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower().strip()\n",
    "    slots = {}\n",
    "    \n",
    "    # Control commands (highest priority)\n",
    "    if text_lower in [\"stop\", \"cancel\", \"never mind\", \"abort\"]:\n",
    "        return Intent(IntentType.CANCEL, 1.0, {}, text, \"rules\")\n",
    "    \n",
    "    if text_lower in [\"yes\", \"yeah\", \"confirm\", \"ok\", \"okay\", \"do it\", \"go ahead\"]:\n",
    "        return Intent(IntentType.CONFIRM, 1.0, {}, text, \"rules\")\n",
    "    \n",
    "    if text_lower == \"help\" or \"how do i\" in text_lower or \"what can\" in text_lower:\n",
    "        return Intent(IntentType.HELP, 1.0, {}, text, \"rules\")\n",
    "    \n",
    "    # Visualization commands\n",
    "    viz_triggers = [\"show\", \"display\", \"visualize\", \"plot\", \"view\", \"see\"]\n",
    "    if any(t in text_lower for t in viz_triggers):\n",
    "        # Extract property\n",
    "        if \"perm\" in text_lower:\n",
    "            slots[\"property\"] = \"permeability\"\n",
    "        elif \"poro\" in text_lower:\n",
    "            slots[\"property\"] = \"porosity\"\n",
    "        elif \"saturation\" in text_lower or \" sw\" in text_lower:\n",
    "            slots[\"property\"] = \"water_saturation\"\n",
    "        elif \"pressure\" in text_lower or \"bhp\" in text_lower:\n",
    "            slots[\"property\"] = \"pressure\"\n",
    "        \n",
    "        # Extract layer\n",
    "        layer_match = re.search(r'layer\\s*(\\d+)', text_lower)\n",
    "        if layer_match:\n",
    "            slots[\"layer\"] = int(layer_match.group(1))\n",
    "        \n",
    "        # Extract time\n",
    "        time_match = re.search(r'(?:day|time|at)\\s*(\\d+)', text_lower)\n",
    "        if time_match:\n",
    "            slots[\"time_days\"] = int(time_match.group(1))\n",
    "        \n",
    "        # Extract view type\n",
    "        if \"3d\" in text_lower:\n",
    "            slots[\"view_type\"] = \"3d\"\n",
    "        elif \"animation\" in text_lower:\n",
    "            slots[\"view_type\"] = \"animation\"\n",
    "        \n",
    "        confidence = 0.95 if slots else 0.75\n",
    "        return Intent(IntentType.VISUALIZE_PROPERTY, confidence, slots, text, \"rules\")\n",
    "    \n",
    "    # Query commands\n",
    "    query_triggers = [\"what is\", \"how much\", \"tell me\", \"current\"]\n",
    "    if any(t in text_lower for t in query_triggers):\n",
    "        if \"oil rate\" in text_lower or \"fopr\" in text_lower:\n",
    "            slots[\"property\"] = \"oil_rate\"\n",
    "        elif \"water rate\" in text_lower or \"fwpr\" in text_lower:\n",
    "            slots[\"property\"] = \"water_rate\"\n",
    "        elif \"water cut\" in text_lower or \"fwct\" in text_lower:\n",
    "            slots[\"property\"] = \"water_cut\"\n",
    "        elif \"pressure\" in text_lower:\n",
    "            slots[\"property\"] = \"pressure\"\n",
    "        elif \"gor\" in text_lower:\n",
    "            slots[\"property\"] = \"gor\"\n",
    "        \n",
    "        if slots:\n",
    "            return Intent(IntentType.QUERY_VALUE, 0.90, slots, text, \"rules\")\n",
    "    \n",
    "    # Navigation\n",
    "    if any(t in text_lower for t in [\"go to\", \"navigate\", \"open\"]):\n",
    "        if \"result\" in text_lower:\n",
    "            slots[\"target\"] = \"results\"\n",
    "        elif \"model\" in text_lower:\n",
    "            slots[\"target\"] = \"model\"\n",
    "        if slots:\n",
    "            return Intent(IntentType.NAVIGATE, 0.85, slots, text, \"rules\")\n",
    "    \n",
    "    # Run simulation\n",
    "    if any(t in text_lower for t in [\"run sim\", \"start sim\", \"execute\"]):\n",
    "        return Intent(IntentType.RUN_SIMULATION, 0.85, {}, text, \"rules\")\n",
    "    \n",
    "    return None  # No rule matched\n",
    "\n",
    "async def parse_with_llm(text: str) -> Intent:\n",
    "    \"\"\"\n",
    "    Parse intent using LLM (Claude or GPT-4).\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"Parse this reservoir simulation voice command into JSON.\n",
    "Available intents: visualize_property, query_value, modify_parameter, run_simulation, navigate, help, confirm, cancel\n",
    "Extract slots: property, layer, time_days, view_type, well, target\n",
    "Respond with ONLY valid JSON: {\"intent\": \"...\", \"confidence\": 0.0-1.0, \"slots\": {...}}\"\"\"\n",
    "    \n",
    "    # Try Claude first\n",
    "    if os.getenv('ANTHROPIC_API_KEY'):\n",
    "        try:\n",
    "            import anthropic\n",
    "            client = anthropic.Anthropic()\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=200,\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"{system_prompt}\\n\\nCommand: \\\"{text}\\\"\"}]\n",
    "            )\n",
    "            result = json.loads(response.content[0].text)\n",
    "            return Intent(\n",
    "                IntentType(result['intent']),\n",
    "                result['confidence'],\n",
    "                result.get('slots', {}),\n",
    "                text,\n",
    "                \"claude\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"   Claude failed: {e}\")\n",
    "    \n",
    "    # Fall back to GPT-4\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        try:\n",
    "            import openai\n",
    "            client = openai.OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f'Command: \"{text}\"'}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            result_text = response.choices[0].message.content\n",
    "            if \"```\" in result_text:\n",
    "                result_text = result_text.split(\"```\")[1].replace(\"json\", \"\").strip()\n",
    "            result = json.loads(result_text)\n",
    "            return Intent(\n",
    "                IntentType(result['intent']),\n",
    "                result['confidence'],\n",
    "                result.get('slots', {}),\n",
    "                text,\n",
    "                \"openai\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"   OpenAI failed: {e}\")\n",
    "    \n",
    "    return Intent(IntentType.UNKNOWN, 0.0, {}, text, \"none\")\n",
    "\n",
    "async def parse_intent(text: str) -> Intent:\n",
    "    \"\"\"\n",
    "    Parse intent: Rules first, then LLM fallback.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ§  Parsing intent...\")\n",
    "    \n",
    "    # Try rules first (instant, no API cost)\n",
    "    result = parse_with_rules(text)\n",
    "    if result:\n",
    "        print(f\"   âœ… Parsed with rules\")\n",
    "        return result\n",
    "    \n",
    "    # Fall back to LLM\n",
    "    print(f\"   â†ª Rules didn't match, trying LLM...\")\n",
    "    return await parse_with_llm(text)\n",
    "\n",
    "def parse_intent_sync(text: str) -> Intent:\n",
    "    import asyncio\n",
    "    return asyncio.get_event_loop().run_until_complete(parse_intent(text))\n",
    "\n",
    "print(\"âœ… Intent parser ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Complete Voice Roundtrip\n",
    "\n",
    "**This is the main cell - run it to test!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def voice_roundtrip(duration: float = 5.0):\n",
    "    \"\"\"\n",
    "    Complete voice processing pipeline:\n",
    "    ğŸ¤ Record â†’ ğŸ“ WAV â†’ ğŸ—£ï¸ Transcribe â†’ ğŸ§  Parse â†’ âš¡ Result\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ¤ CLARISSA Voice Roundtrip\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_start = time.time()\n",
    "    \n",
    "    # Step 1: Record\n",
    "    print(\"\\nğŸ“Œ Step 1: Recording Audio\")\n",
    "    audio, wav_path = record_audio(duration)\n",
    "    \n",
    "    # Playback for verification\n",
    "    print(\"\\nğŸ”Š Playback (verify your recording):\")\n",
    "    display(Audio(wav_path))\n",
    "    \n",
    "    # Step 2: Transcribe\n",
    "    print(\"\\nğŸ“Œ Step 2: Speech-to-Text (Whisper)\")\n",
    "    transcription = await transcribe_audio(wav_path)\n",
    "    \n",
    "    if not transcription['text']:\n",
    "        print(\"\\nâŒ No speech detected - try again!\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Parse Intent\n",
    "    print(\"\\nğŸ“Œ Step 3: Intent Parsing\")\n",
    "    intent = await parse_intent(transcription['text'])\n",
    "    \n",
    "    # Results\n",
    "    total_time = time.time() - total_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š ROUNDTRIP RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nğŸ¤ You said: \\\"{transcription['text']}\\\"\")\n",
    "    print(f\"\\nğŸ¯ Intent: {intent.type.value}\")\n",
    "    print(f\"   Confidence: {intent.confidence:.0%}\")\n",
    "    print(f\"   Slots: {intent.slots}\")\n",
    "    print(f\"   Parse method: {intent.parse_method}\")\n",
    "    print(f\"\\nâ±ï¸ Timing:\")\n",
    "    print(f\"   Transcription: {transcription['latency_ms']}ms\")\n",
    "    print(f\"   Total roundtrip: {total_time:.1f}s\")\n",
    "    print(f\"\\nğŸ’° Cost: ${transcription['cost_usd']:.4f}\")\n",
    "    \n",
    "    # Simulated response\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ’¬ CLARISSA would respond:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if intent.type == IntentType.VISUALIZE_PROPERTY:\n",
    "        prop = intent.slots.get('property', 'permeability')\n",
    "        layer = intent.slots.get('layer', 'all layers')\n",
    "        print(f'\\n   \"Displaying {prop} for {layer}...\"')\n",
    "    elif intent.type == IntentType.QUERY_VALUE:\n",
    "        prop = intent.slots.get('property', 'value')\n",
    "        print(f'\\n   \"The current {prop} is 1,234 STB/day.\"')\n",
    "    elif intent.type == IntentType.HELP:\n",
    "        print('\\n   \"You can say things like: show permeability, what is the water cut, go to results...\"')\n",
    "    elif intent.type == IntentType.CANCEL:\n",
    "        print('\\n   \"Operation cancelled.\"')\n",
    "    else:\n",
    "        print(f'\\n   \"I understood: {intent.type.value}. Processing...\"')\n",
    "    \n",
    "    return {\n",
    "        'transcription': transcription,\n",
    "        'intent': intent,\n",
    "        'total_time_s': total_time\n",
    "    }\n",
    "\n",
    "# Sync wrapper for easy execution\n",
    "def run_voice_test(duration: float = 5.0):\n",
    "    return asyncio.get_event_loop().run_until_complete(voice_roundtrip(duration))\n",
    "\n",
    "print(\"âœ… Voice roundtrip function ready!\")\n",
    "print(\"\\nğŸ‘‡ Run the next cell to test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Run the Test!\n",
    "\n",
    "**Example commands to try:**\n",
    "- \"Show me the permeability\"\n",
    "- \"What is the water cut?\"\n",
    "- \"Display porosity in 3D\"\n",
    "- \"Show layer 5 at day 1000\"\n",
    "- \"Help\"\n",
    "- \"Cancel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤ RUN THIS TO TEST!\n",
    "# Speak clearly into your microphone after running\n",
    "\n",
    "result = run_voice_test(duration=5.0)  # 5 seconds recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Batch Testing with Test Utterances\n",
    "\n",
    "Test the intent parser without microphone (uses text input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test utterances from CLARISSA test suite\n",
    "TEST_UTTERANCES = [\n",
    "    (\"show me the permeability\", \"visualize_property\", {\"property\": \"permeability\"}),\n",
    "    (\"display porosity in 3D\", \"visualize_property\", {\"property\": \"porosity\", \"view_type\": \"3d\"}),\n",
    "    (\"show layer 3\", \"visualize_property\", {\"layer\": 3}),\n",
    "    (\"what is the oil rate\", \"query_value\", {\"property\": \"oil_rate\"}),\n",
    "    (\"tell me the water cut\", \"query_value\", {\"property\": \"water_cut\"}),\n",
    "    (\"go to the results\", \"navigate\", {\"target\": \"results\"}),\n",
    "    (\"help\", \"help\", {}),\n",
    "    (\"stop\", \"cancel\", {}),\n",
    "    (\"yes\", \"confirm\", {}),\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Intent Parser Test Suite\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "passed = 0\n",
    "for text, expected_intent, expected_slots in TEST_UTTERANCES:\n",
    "    intent = parse_with_rules(text)\n",
    "    \n",
    "    if intent is None:\n",
    "        status = \"âŒ NO MATCH\"\n",
    "    elif intent.type.value == expected_intent:\n",
    "        # Check key slots\n",
    "        slots_ok = all(intent.slots.get(k) == v for k, v in expected_slots.items())\n",
    "        if slots_ok:\n",
    "            status = \"âœ… PASS\"\n",
    "            passed += 1\n",
    "        else:\n",
    "            status = f\"âš ï¸ SLOTS: got {intent.slots}\"\n",
    "    else:\n",
    "        status = f\"âŒ INTENT: got {intent.type.value}\"\n",
    "    \n",
    "    print(f\"{status:20} | \\\"{text}\\\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Results: {passed}/{len(TEST_UTTERANCES)} passed ({100*passed/len(TEST_UTTERANCES):.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Summary\n",
    "\n",
    "This notebook demonstrates CLARISSA's voice pipeline:\n",
    "\n",
    "| Stage | Technology | Latency | Cost |\n",
    "|-------|------------|---------|------|\n",
    "| ğŸ¤ Capture | sounddevice (cross-platform) | ~5s (recording) | Free |\n",
    "| ğŸ—£ï¸ STT | OpenAI Whisper API | ~500-1500ms | $0.006/min |\n",
    "| ğŸ§  Intent | Rules + Claude/GPT-4 | <100ms (rules) | Free (rules) |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Local Whisper**: For air-gapped deployments, use `faster-whisper`\n",
    "2. **VAD**: Add Voice Activity Detection for automatic start/stop\n",
    "3. **Streaming**: Process audio in real-time for lower latency\n",
    "\n",
    "### References\n",
    "\n",
    "- [ADR-028: Voice Input Architecture](../docs/architecture/adr/ADR-028-voice-input-architecture.md)\n",
    "- [OpenAI Whisper API](https://platform.openai.com/docs/guides/speech-to-text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": [],
   "name": "17_Voice_Roundtrip_Local"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}