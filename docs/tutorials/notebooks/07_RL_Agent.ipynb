{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLARISSA Tutorial 07: Reinforcement Learning Agent\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand RL fundamentals for simulation optimization\n",
    "- Implement PPO-based action selection\n",
    "- Design reward functions for deck generation\n",
    "- Train agents with simulation feedback\n",
    "\n",
    "**Prerequisites:** Notebooks 01-06\n",
    "\n",
    "**Estimated Time:** 90 minutes\n",
    "\n",
    "**Note:** GPU recommended for training (Colab T4 works well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Reinforcement Learning?\n",
    "\n",
    "LLMs generate plausible text, but simulation success requires:\n",
    "\n",
    "| Challenge | RL Solution |\n",
    "|-----------|-------------|\n",
    "| Convergence failures | Learn from simulator feedback |\n",
    "| Suboptimal defaults | Optimize based on outcomes |\n",
    "| Action sequencing | Policy learns effective orderings |\n",
    "| Error recovery | Reward successful corrections |\n",
    "\n",
    "CLARISSA uses RL to optimize the *sequence of actions* taken during deck generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from enum import Enum, auto\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "# Check for PyTorch (optional - we have numpy fallback)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_AVAILABLE = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'PyTorch available on {device}')\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print('PyTorch not available - using NumPy implementation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The CLARISSA Action Space\n",
    "\n",
    "Define the actions the RL agent can take during deck generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    \"\"\"Actions available to the CLARISSA RL agent.\"\"\"\n",
    "    # Information gathering\n",
    "    ASK_CLARIFICATION = auto()     # Request more info from user\n",
    "    QUERY_KNOWLEDGE = auto()       # Look up in knowledge base\n",
    "    QUERY_ANALOG = auto()          # Find similar reservoirs\n",
    "    \n",
    "    # Deck construction\n",
    "    SET_GRID = auto()              # Define grid dimensions\n",
    "    SET_ROCK_PROPS = auto()        # Set porosity, permeability\n",
    "    SET_FLUID_PROPS = auto()       # Set PVT, densities\n",
    "    SET_RELPERM = auto()           # Set relative permeability\n",
    "    SET_INITIAL = auto()           # Set initial conditions\n",
    "    ADD_WELL = auto()              # Add a well\n",
    "    SET_SCHEDULE = auto()          # Define time steps\n",
    "    \n",
    "    # Validation & execution\n",
    "    VALIDATE_DECK = auto()         # Run constraint checks\n",
    "    RUN_SIMULATION = auto()        # Execute OPM Flow\n",
    "    FIX_ERROR = auto()             # Attempt automatic fix\n",
    "    \n",
    "    # Completion\n",
    "    PRESENT_RESULTS = auto()       # Show results to user\n",
    "    DONE = auto()                  # Task complete\n",
    "\n",
    "# Action metadata\n",
    "ACTION_INFO = {\n",
    "    Action.ASK_CLARIFICATION: {'cost': 1, 'reversible': True},\n",
    "    Action.QUERY_KNOWLEDGE: {'cost': 0.5, 'reversible': True},\n",
    "    Action.QUERY_ANALOG: {'cost': 1, 'reversible': True},\n",
    "    Action.SET_GRID: {'cost': 2, 'reversible': True},\n",
    "    Action.SET_ROCK_PROPS: {'cost': 1, 'reversible': True},\n",
    "    Action.SET_FLUID_PROPS: {'cost': 1, 'reversible': True},\n",
    "    Action.SET_RELPERM: {'cost': 1, 'reversible': True},\n",
    "    Action.SET_INITIAL: {'cost': 1, 'reversible': True},\n",
    "    Action.ADD_WELL: {'cost': 2, 'reversible': True},\n",
    "    Action.SET_SCHEDULE: {'cost': 1, 'reversible': True},\n",
    "    Action.VALIDATE_DECK: {'cost': 1, 'reversible': True},\n",
    "    Action.RUN_SIMULATION: {'cost': 10, 'reversible': False},\n",
    "    Action.FIX_ERROR: {'cost': 2, 'reversible': True},\n",
    "    Action.PRESENT_RESULTS: {'cost': 0, 'reversible': True},\n",
    "    Action.DONE: {'cost': 0, 'reversible': False},\n",
    "}\n",
    "\n",
    "print(f\"Action space size: {len(Action)}\")\n",
    "print(\"\\nAction costs:\")\n",
    "for action in Action:\n",
    "    info = ACTION_INFO[action]\n",
    "    print(f\"  {action.name:20} cost={info['cost']:4} reversible={info['reversible']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: State Representation\n",
    "\n",
    "The state encodes what CLARISSA knows and what's been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DeckState:\n",
    "    \"\"\"State of deck generation process.\"\"\"\n",
    "    # Completion flags (0 or 1)\n",
    "    grid_defined: float = 0.0\n",
    "    rock_defined: float = 0.0\n",
    "    fluid_defined: float = 0.0\n",
    "    relperm_defined: float = 0.0\n",
    "    initial_defined: float = 0.0\n",
    "    wells_defined: float = 0.0\n",
    "    schedule_defined: float = 0.0\n",
    "    \n",
    "    # Quality metrics (0 to 1)\n",
    "    validation_score: float = 0.0\n",
    "    physics_score: float = 0.0\n",
    "    \n",
    "    # Simulation status\n",
    "    sim_attempted: float = 0.0\n",
    "    sim_converged: float = 0.0\n",
    "    sim_time_ratio: float = 0.0  # actual/target time\n",
    "    \n",
    "    # Conversation context\n",
    "    clarifications_asked: float = 0.0\n",
    "    user_satisfaction: float = 0.5  # Estimated\n",
    "    \n",
    "    # Resource usage\n",
    "    steps_taken: float = 0.0\n",
    "    \n",
    "    def to_vector(self) -> np.ndarray:\n",
    "        \"\"\"Convert state to feature vector.\"\"\"\n",
    "        return np.array([\n",
    "            self.grid_defined,\n",
    "            self.rock_defined,\n",
    "            self.fluid_defined,\n",
    "            self.relperm_defined,\n",
    "            self.initial_defined,\n",
    "            self.wells_defined,\n",
    "            self.schedule_defined,\n",
    "            self.validation_score,\n",
    "            self.physics_score,\n",
    "            self.sim_attempted,\n",
    "            self.sim_converged,\n",
    "            self.sim_time_ratio,\n",
    "            self.clarifications_asked / 5.0,  # Normalize\n",
    "            self.user_satisfaction,\n",
    "            self.steps_taken / 20.0,  # Normalize\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    @property\n",
    "    def completeness(self) -> float:\n",
    "        \"\"\"How complete is the deck (0-1).\"\"\"\n",
    "        sections = [self.grid_defined, self.rock_defined, self.fluid_defined,\n",
    "                   self.relperm_defined, self.initial_defined, \n",
    "                   self.wells_defined, self.schedule_defined]\n",
    "        return sum(sections) / len(sections)\n",
    "\n",
    "STATE_DIM = len(DeckState().to_vector())\n",
    "ACTION_DIM = len(Action)\n",
    "\n",
    "print(f\"State dimension: {STATE_DIM}\")\n",
    "print(f\"Action dimension: {ACTION_DIM}\")\n",
    "\n",
    "# Demo state\n",
    "state = DeckState(grid_defined=1.0, rock_defined=1.0, wells_defined=0.5)\n",
    "print(f\"\\nExample state completeness: {state.completeness:.1%}\")\n",
    "print(f\"State vector: {state.to_vector()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Reward Function Design\n",
    "\n",
    "The reward function shapes what the agent learns to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCalculator:\n",
    "    \"\"\"Calculate rewards for RL agent actions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Reward weights\n",
    "        self.w_completion = 10.0   # Completing deck sections\n",
    "        self.w_validation = 5.0    # Passing validation\n",
    "        self.w_convergence = 20.0  # Simulation converging\n",
    "        self.w_efficiency = -0.1   # Penalty per step\n",
    "        self.w_clarification = -0.5  # Penalty for asking\n",
    "        self.w_sim_fail = -5.0     # Penalty for failed sim\n",
    "    \n",
    "    def calculate(self, prev_state: DeckState, action: Action, \n",
    "                  new_state: DeckState, sim_result: Optional[Dict] = None) -> float:\n",
    "        \"\"\"Calculate reward for state transition.\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # 1. Completion progress\n",
    "        completion_delta = new_state.completeness - prev_state.completeness\n",
    "        reward += self.w_completion * completion_delta\n",
    "        \n",
    "        # 2. Validation improvement\n",
    "        if action == Action.VALIDATE_DECK:\n",
    "            validation_delta = new_state.validation_score - prev_state.validation_score\n",
    "            reward += self.w_validation * validation_delta\n",
    "        \n",
    "        # 3. Simulation outcome\n",
    "        if action == Action.RUN_SIMULATION:\n",
    "            if new_state.sim_converged > 0:\n",
    "                reward += self.w_convergence * new_state.sim_time_ratio\n",
    "            else:\n",
    "                reward += self.w_sim_fail\n",
    "        \n",
    "        # 4. Efficiency penalty\n",
    "        reward += self.w_efficiency\n",
    "        \n",
    "        # 5. Clarification penalty (but sometimes necessary)\n",
    "        if action == Action.ASK_CLARIFICATION:\n",
    "            # Less penalty early, more penalty late\n",
    "            penalty_scale = prev_state.completeness\n",
    "            reward += self.w_clarification * (1 + penalty_scale)\n",
    "        \n",
    "        # 6. Bonus for successful completion\n",
    "        if action == Action.DONE and new_state.sim_converged > 0:\n",
    "            efficiency_bonus = max(0, 1 - new_state.steps_taken / 15)\n",
    "            reward += 10.0 * efficiency_bonus\n",
    "        \n",
    "        return reward\n",
    "\n",
    "# Demo reward calculation\n",
    "calc = RewardCalculator()\n",
    "\n",
    "# Scenario 1: Define grid (good progress)\n",
    "s1 = DeckState()\n",
    "s2 = DeckState(grid_defined=1.0)\n",
    "r1 = calc.calculate(s1, Action.SET_GRID, s2)\n",
    "print(f\"Set grid: reward = {r1:.2f}\")\n",
    "\n",
    "# Scenario 2: Run simulation that converges\n",
    "s3 = DeckState(grid_defined=1, rock_defined=1, fluid_defined=1, relperm_defined=1,\n",
    "               initial_defined=1, wells_defined=1, schedule_defined=1, validation_score=1)\n",
    "s4 = DeckState(grid_defined=1, rock_defined=1, fluid_defined=1, relperm_defined=1,\n",
    "               initial_defined=1, wells_defined=1, schedule_defined=1, validation_score=1,\n",
    "               sim_attempted=1, sim_converged=1, sim_time_ratio=0.9)\n",
    "r2 = calc.calculate(s3, Action.RUN_SIMULATION, s4)\n",
    "print(f\"Successful simulation: reward = {r2:.2f}\")\n",
    "\n",
    "# Scenario 3: Simulation fails\n",
    "s5 = DeckState(grid_defined=1, rock_defined=1, fluid_defined=1, relperm_defined=1,\n",
    "               initial_defined=1, wells_defined=1, schedule_defined=1, validation_score=0.5,\n",
    "               sim_attempted=1, sim_converged=0)\n",
    "r3 = calc.calculate(s3, Action.RUN_SIMULATION, s5)\n",
    "print(f\"Failed simulation: reward = {r3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Environment Simulation\n",
    "\n",
    "A simulated environment for training (before connecting to OPM Flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeckGenerationEnv:\n",
    "    \"\"\"Simulated environment for deck generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_steps: int = 20):\n",
    "        self.max_steps = max_steps\n",
    "        self.reward_calc = RewardCalculator()\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Reset to initial state.\"\"\"\n",
    "        self.state = DeckState()\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        return self.state.to_vector()\n",
    "    \n",
    "    def step(self, action: Action) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"Take action, return (new_state, reward, done, info).\"\"\"\n",
    "        prev_state = DeckState(**self.state.__dict__)\n",
    "        self.steps += 1\n",
    "        self.state.steps_taken = self.steps\n",
    "        \n",
    "        info = {'action': action.name}\n",
    "        \n",
    "        # Apply action effects (simplified simulation)\n",
    "        if action == Action.SET_GRID:\n",
    "            self.state.grid_defined = 1.0\n",
    "        elif action == Action.SET_ROCK_PROPS:\n",
    "            if self.state.grid_defined:\n",
    "                self.state.rock_defined = 1.0\n",
    "        elif action == Action.SET_FLUID_PROPS:\n",
    "            self.state.fluid_defined = 1.0\n",
    "        elif action == Action.SET_RELPERM:\n",
    "            self.state.relperm_defined = 1.0\n",
    "        elif action == Action.SET_INITIAL:\n",
    "            if self.state.grid_defined:\n",
    "                self.state.initial_defined = 1.0\n",
    "        elif action == Action.ADD_WELL:\n",
    "            if self.state.grid_defined:\n",
    "                self.state.wells_defined = min(1.0, self.state.wells_defined + 0.25)\n",
    "        elif action == Action.SET_SCHEDULE:\n",
    "            self.state.schedule_defined = 1.0\n",
    "        elif action == Action.VALIDATE_DECK:\n",
    "            self.state.validation_score = self.state.completeness * 0.9 + random.random() * 0.1\n",
    "            self.state.physics_score = self.state.validation_score * 0.95\n",
    "        elif action == Action.RUN_SIMULATION:\n",
    "            self.state.sim_attempted = 1.0\n",
    "            # Convergence depends on validation score\n",
    "            if self.state.validation_score > 0.7 and random.random() < self.state.validation_score:\n",
    "                self.state.sim_converged = 1.0\n",
    "                self.state.sim_time_ratio = 0.8 + random.random() * 0.2\n",
    "            else:\n",
    "                self.state.sim_converged = 0.0\n",
    "        elif action == Action.ASK_CLARIFICATION:\n",
    "            self.state.clarifications_asked += 1\n",
    "            self.state.user_satisfaction = max(0.3, self.state.user_satisfaction - 0.05)\n",
    "        elif action == Action.DONE:\n",
    "            self.done = True\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self.reward_calc.calculate(prev_state, action, self.state)\n",
    "        \n",
    "        # Check termination\n",
    "        if self.steps >= self.max_steps:\n",
    "            self.done = True\n",
    "            info['timeout'] = True\n",
    "        \n",
    "        return self.state.to_vector(), reward, self.done, info\n",
    "    \n",
    "    def get_valid_actions(self) -> List[Action]:\n",
    "        \"\"\"Return list of valid actions in current state.\"\"\"\n",
    "        valid = [Action.ASK_CLARIFICATION, Action.QUERY_KNOWLEDGE]\n",
    "        \n",
    "        # Grid must be first\n",
    "        if not self.state.grid_defined:\n",
    "            valid.append(Action.SET_GRID)\n",
    "        else:\n",
    "            valid.extend([Action.SET_ROCK_PROPS, Action.SET_INITIAL, Action.ADD_WELL])\n",
    "        \n",
    "        # Other properties can be set anytime\n",
    "        valid.extend([Action.SET_FLUID_PROPS, Action.SET_RELPERM, Action.SET_SCHEDULE])\n",
    "        \n",
    "        # Validation requires some content\n",
    "        if self.state.completeness > 0.3:\n",
    "            valid.append(Action.VALIDATE_DECK)\n",
    "        \n",
    "        # Simulation requires validation\n",
    "        if self.state.validation_score > 0.5:\n",
    "            valid.append(Action.RUN_SIMULATION)\n",
    "        \n",
    "        # Done if simulation succeeded\n",
    "        if self.state.sim_converged > 0:\n",
    "            valid.append(Action.DONE)\n",
    "        \n",
    "        return valid\n",
    "\n",
    "# Test environment\n",
    "env = DeckGenerationEnv()\n",
    "state = env.reset()\n",
    "\n",
    "print(\"Environment test:\")\n",
    "print(f\"Initial valid actions: {[a.name for a in env.get_valid_actions()]}\")\n",
    "\n",
    "# Take some actions\n",
    "actions = [Action.SET_GRID, Action.SET_ROCK_PROPS, Action.SET_FLUID_PROPS,\n",
    "           Action.SET_RELPERM, Action.SET_INITIAL, Action.ADD_WELL, \n",
    "           Action.ADD_WELL, Action.SET_SCHEDULE, Action.VALIDATE_DECK]\n",
    "\n",
    "total_reward = 0\n",
    "for action in actions:\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"  {action.name:20} reward={reward:+.2f}  completeness={env.state.completeness:.0%}\")\n",
    "\n",
    "print(f\"\\nTotal reward: {total_reward:.2f}\")\n",
    "print(f\"Valid actions now: {[a.name for a in env.get_valid_actions()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Policy Network (PPO)\n",
    "\n",
    "A neural network that learns to select actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class PolicyNetwork(nn.Module):\n",
    "        \"\"\"Actor-Critic network for PPO.\"\"\"\n",
    "        \n",
    "        def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "            super().__init__()\n",
    "            \n",
    "            # Shared feature extractor\n",
    "            self.shared = nn.Sequential(\n",
    "                nn.Linear(state_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            \n",
    "            # Actor head (policy)\n",
    "            self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "            \n",
    "            # Critic head (value function)\n",
    "            self.critic = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        def forward(self, state):\n",
    "            features = self.shared(state)\n",
    "            action_logits = self.actor(features)\n",
    "            value = self.critic(features)\n",
    "            return action_logits, value\n",
    "        \n",
    "        def get_action(self, state, valid_actions: List[int] = None):\n",
    "            \"\"\"Sample action from policy.\"\"\"\n",
    "            with torch.no_grad():\n",
    "                logits, value = self.forward(state)\n",
    "                \n",
    "                # Mask invalid actions\n",
    "                if valid_actions is not None:\n",
    "                    mask = torch.ones_like(logits) * float('-inf')\n",
    "                    mask[valid_actions] = 0\n",
    "                    logits = logits + mask\n",
    "                \n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                action = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "            return action, probs[action].item(), value.item()\n",
    "    \n",
    "    # Create network\n",
    "    policy = PolicyNetwork(STATE_DIM, ACTION_DIM).to(device)\n",
    "    print(f\"Policy network created: {sum(p.numel() for p in policy.parameters())} parameters\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    test_state = torch.randn(STATE_DIM).to(device)\n",
    "    logits, value = policy(test_state)\n",
    "    print(f\"Output shapes: logits={logits.shape}, value={value.shape}\")\n",
    "\n",
    "else:\n",
    "    # NumPy fallback for simple policy\n",
    "    class SimplePolicy:\n",
    "        \"\"\"Simple policy without neural network.\"\"\"\n",
    "        \n",
    "        def __init__(self, action_dim: int):\n",
    "            self.action_dim = action_dim\n",
    "            self.action_values = np.zeros(action_dim)\n",
    "        \n",
    "        def get_action(self, state, valid_actions: List[int] = None):\n",
    "            if valid_actions:\n",
    "                # Choose from valid actions based on learned values\n",
    "                values = self.action_values[valid_actions]\n",
    "                probs = np.exp(values) / np.sum(np.exp(values))\n",
    "                action_idx = np.random.choice(len(valid_actions), p=probs)\n",
    "                action = valid_actions[action_idx]\n",
    "            else:\n",
    "                action = np.random.randint(self.action_dim)\n",
    "            return action, 1.0/self.action_dim, 0.0\n",
    "    \n",
    "    policy = SimplePolicy(ACTION_DIM)\n",
    "    print(\"Using simple NumPy policy (install PyTorch for full PPO)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Training Loop\n",
    "\n",
    "Train the agent through interaction with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLTrainer:\n",
    "    \"\"\"Train RL agent for deck generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, policy, lr: float = 3e-4):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps_clip = 0.2  # PPO clip parameter\n",
    "        \n",
    "        if TORCH_AVAILABLE:\n",
    "            self.optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Tracking\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "    \n",
    "    def collect_episode(self) -> Tuple[List, float]:\n",
    "        \"\"\"Collect one episode of experience.\"\"\"\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            # Get valid actions\n",
    "            valid_actions = [a.value - 1 for a in self.env.get_valid_actions()]\n",
    "            \n",
    "            # Select action\n",
    "            if TORCH_AVAILABLE:\n",
    "                state_tensor = torch.FloatTensor(state).to(device)\n",
    "                action_idx, prob, value = self.policy.get_action(state_tensor, valid_actions)\n",
    "            else:\n",
    "                action_idx, prob, value = self.policy.get_action(state, valid_actions)\n",
    "            \n",
    "            action = list(Action)[action_idx]\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            trajectory.append({\n",
    "                'state': state,\n",
    "                'action': action_idx,\n",
    "                'reward': reward,\n",
    "                'prob': prob,\n",
    "                'value': value,\n",
    "                'done': done\n",
    "            })\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return trajectory, total_reward\n",
    "    \n",
    "    def compute_returns(self, trajectory: List) -> List[float]:\n",
    "        \"\"\"Compute discounted returns.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for step in reversed(trajectory):\n",
    "            G = step['reward'] + self.gamma * G * (1 - step['done'])\n",
    "            returns.insert(0, G)\n",
    "        return returns\n",
    "    \n",
    "    def train_episode(self) -> float:\n",
    "        \"\"\"Train on one episode.\"\"\"\n",
    "        trajectory, total_reward = self.collect_episode()\n",
    "        returns = self.compute_returns(trajectory)\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        self.episode_lengths.append(len(trajectory))\n",
    "        \n",
    "        if TORCH_AVAILABLE and len(trajectory) > 0:\n",
    "            # PPO update\n",
    "            states = torch.FloatTensor([t['state'] for t in trajectory]).to(device)\n",
    "            actions = torch.LongTensor([t['action'] for t in trajectory]).to(device)\n",
    "            old_probs = torch.FloatTensor([t['prob'] for t in trajectory]).to(device)\n",
    "            returns_t = torch.FloatTensor(returns).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, values = self.policy(states)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            new_probs = probs.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "            \n",
    "            # PPO loss\n",
    "            ratio = new_probs / (old_probs + 1e-8)\n",
    "            advantages = returns_t - values.squeeze()\n",
    "            surr1 = ratio * advantages.detach()\n",
    "            surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages.detach()\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            critic_loss = nn.MSELoss()(values.squeeze(), returns_t)\n",
    "            \n",
    "            loss = actor_loss + 0.5 * critic_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def train(self, num_episodes: int = 100, print_every: int = 10):\n",
    "        \"\"\"Train for multiple episodes.\"\"\"\n",
    "        for ep in range(num_episodes):\n",
    "            reward = self.train_episode()\n",
    "            \n",
    "            if (ep + 1) % print_every == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-print_every:])\n",
    "                avg_length = np.mean(self.episode_lengths[-print_every:])\n",
    "                print(f\"Episode {ep+1:4d} | Avg Reward: {avg_reward:+.2f} | Avg Length: {avg_length:.1f}\")\n",
    "\n",
    "# Train!\n",
    "trainer = RLTrainer(env, policy)\n",
    "print(\"Training RL agent...\\n\")\n",
    "trainer.train(num_episodes=50, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Rewards\n",
    "ax1.plot(trainer.episode_rewards)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Episode Rewards')\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "ax2.plot(trainer.episode_lengths)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Steps')\n",
    "ax2.set_title('Episode Lengths')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Evaluation\n",
    "\n",
    "Evaluate the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, policy, num_episodes: int = 10, verbose: bool = False):\n",
    "    \"\"\"Evaluate trained agent.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            valid_actions = [a.value - 1 for a in env.get_valid_actions()]\n",
    "            \n",
    "            if TORCH_AVAILABLE:\n",
    "                state_tensor = torch.FloatTensor(state).to(device)\n",
    "                action_idx, _, _ = policy.get_action(state_tensor, valid_actions)\n",
    "            else:\n",
    "                action_idx, _, _ = policy.get_action(state, valid_actions)\n",
    "            \n",
    "            action = list(Action)[action_idx]\n",
    "            actions_taken.append(action.name)\n",
    "            \n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        results.append({\n",
    "            'reward': total_reward,\n",
    "            'steps': len(actions_taken),\n",
    "            'converged': env.state.sim_converged > 0,\n",
    "            'actions': actions_taken\n",
    "        })\n",
    "        \n",
    "        if verbose:\n",
    "            status = 'converged' if env.state.sim_converged > 0 else 'FAILED'\n",
    "            print(f\"Episode {ep+1}: {status} in {len(actions_taken)} steps, reward={total_reward:.1f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating trained agent:\")\n",
    "print(\"=\" * 50)\n",
    "results = evaluate_agent(env, policy, num_episodes=10, verbose=True)\n",
    "\n",
    "# Summary\n",
    "success_rate = sum(1 for r in results if r['converged']) / len(results)\n",
    "avg_reward = np.mean([r['reward'] for r in results])\n",
    "avg_steps = np.mean([r['steps'] for r in results])\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Success rate: {success_rate:.0%}\")\n",
    "print(f\"  Avg reward: {avg_reward:.2f}\")\n",
    "print(f\"  Avg steps: {avg_steps:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we learned:\n",
    "\n",
    "1. **Action Space**: Define actions for deck generation\n",
    "2. **State Representation**: Encode progress and quality\n",
    "3. **Reward Design**: Shape learning with completion, validation, convergence\n",
    "4. **Environment**: Simulate deck generation process\n",
    "5. **PPO Policy**: Neural network for action selection\n",
    "6. **Training**: Collect experience and update policy\n",
    "\n",
    "**Key Insight**: RL optimizes the *sequence* of actions, not just individual decisions.\n",
    "\n",
    "**Next Tutorial:** [08_RIGOR_Benchmark.ipynb](08_RIGOR_Benchmark.ipynb) - Evaluation framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}