{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Knowledge Layer\n",
    "\n",
    "**CLARISSA's** knowledge layer provides the contextual intelligence that transforms a generic LLM into a reservoir simulation expert. This notebook covers:\n",
    "\n",
    "1. Vector database architecture with pgvector\n",
    "2. Embedding generation for technical documentation\n",
    "3. Semantic search for keyword assistance\n",
    "4. Reservoir analog database for intelligent defaults\n",
    "5. Hybrid search strategies (semantic + keyword)\n",
    "6. Knowledge ingestion pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Colab Setup — API Keys & Dependencies\n",
    "import sys, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q openai\n",
    "    from google.colab import userdata\n",
    "    # Set keys from Colab Secrets (Settings → Secrets → Add)\n",
    "    try: os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "    except: pass\n",
    "    try: os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "    except: pass\n",
    "    try: os.environ['GITLAB_TOKEN'] = userdata.get('GITLAB_TOKEN')\n",
    "    except: pass\n",
    "    # Fallback: manual input\n",
    "    if not os.environ.get('ANTHROPIC_API_KEY'):\n",
    "        import getpass\n",
    "        for key in ['ANTHROPIC_API_KEY', 'OPENAI_API_KEY', 'GITLAB_TOKEN']:\n",
    "            if not os.environ.get(key):\n",
    "                try: os.environ[key] = getpass.getpass(f'{key}: ')\n",
    "                except: pass\n",
    "\n",
    "print(f'Environment: {\"Colab\" if IN_COLAB else \"Local\"}')\n",
    "for k in ['ANTHROPIC_API_KEY','OPENAI_API_KEY','GITLAB_TOKEN']:\n",
    "    print(f'  {k}: {\"✓\" if os.environ.get(k) else \"✗\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why a Knowledge Layer?\n",
    "\n",
    "LLMs have general knowledge but lack:\n",
    "\n",
    "- **Current simulator documentation** (OPM Flow specifics)\n",
    "- **Keyword syntax details** (exact formats, defaults)\n",
    "- **Reservoir analogs** (typical values for Permian, Bakken, etc.)\n",
    "- **User corrections** (learned fixes from past sessions)\n",
    "\n",
    "The Knowledge Layer fills these gaps through **Retrieval-Augmented Generation (RAG)**:\n",
    "\n",
    "```\n",
    "User Query → Embed → Search Vector DB → Retrieve Context → LLM + Context → Response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Schema\n",
    "\n",
    "We use PostgreSQL with the **pgvector** extension for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Schema for CLARISSA Knowledge Layer\n",
    "\n",
    "SCHEMA_SQL = '''\n",
    "-- ============================================================\n",
    "-- CLARISSA Knowledge Layer Schema\n",
    "-- PostgreSQL + pgvector\n",
    "-- ============================================================\n",
    "\n",
    "-- Enable pgvector extension\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "\n",
    "-- ============================================================\n",
    "-- 1. Simulator Knowledge Base\n",
    "-- General documentation, tutorials, FAQs\n",
    "-- ============================================================\n",
    "CREATE TABLE simulator_knowledge (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    \n",
    "    -- Content\n",
    "    source_type VARCHAR(50) NOT NULL,  -- 'manual', 'keyword_ref', 'tutorial', 'faq'\n",
    "    source_file VARCHAR(255),\n",
    "    section_title VARCHAR(500),\n",
    "    content TEXT NOT NULL,\n",
    "    \n",
    "    -- Embedding (1536 dimensions for OpenAI ada-002 or similar)\n",
    "    embedding vector(1536),\n",
    "    \n",
    "    -- Metadata for filtering\n",
    "    keywords TEXT[],              -- Associated keywords mentioned\n",
    "    related_keywords TEXT[],      -- Keywords this content helps explain\n",
    "    simulator VARCHAR(50),        -- 'opm', 'eclipse', 'both'\n",
    "    \n",
    "    -- Timestamps\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Indexes for efficient search\n",
    "CREATE INDEX idx_simknow_embedding ON simulator_knowledge \n",
    "    USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n",
    "CREATE INDEX idx_simknow_keywords ON simulator_knowledge USING GIN (keywords);\n",
    "CREATE INDEX idx_simknow_source ON simulator_knowledge (source_type);\n",
    "\n",
    "-- Full-text search index\n",
    "CREATE INDEX idx_simknow_fts ON simulator_knowledge \n",
    "    USING GIN (to_tsvector('english', content));\n",
    "\n",
    "-- ============================================================\n",
    "-- 2. ECLIPSE Keywords Reference\n",
    "-- Structured keyword information\n",
    "-- ============================================================\n",
    "CREATE TABLE eclipse_keywords (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    \n",
    "    -- Keyword identification\n",
    "    keyword VARCHAR(20) UNIQUE NOT NULL,\n",
    "    section VARCHAR(20) NOT NULL,  -- 'RUNSPEC', 'GRID', 'PROPS', etc.\n",
    "    \n",
    "    -- Documentation\n",
    "    description TEXT NOT NULL,\n",
    "    syntax_template TEXT,          -- Example syntax\n",
    "    parameters JSONB,              -- Parameter definitions\n",
    "    examples JSONB,                -- Usage examples\n",
    "    \n",
    "    -- Relationships\n",
    "    required_keywords TEXT[],      -- Keywords that must also be present\n",
    "    incompatible_keywords TEXT[],  -- Keywords that conflict\n",
    "    related_keywords TEXT[],       -- Semantically related keywords\n",
    "    \n",
    "    -- OPM Flow compatibility\n",
    "    opm_supported BOOLEAN DEFAULT true,\n",
    "    opm_notes TEXT,                -- Compatibility notes\n",
    "    \n",
    "    -- Embedding for semantic search\n",
    "    description_embedding vector(1536),\n",
    "    \n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_keywords_section ON eclipse_keywords (section);\n",
    "CREATE INDEX idx_keywords_opm ON eclipse_keywords (opm_supported);\n",
    "CREATE INDEX idx_keywords_embedding ON eclipse_keywords \n",
    "    USING ivfflat (description_embedding vector_cosine_ops) WITH (lists = 50);\n",
    "\n",
    "-- ============================================================\n",
    "-- 3. Reservoir Analogs Database\n",
    "-- Typical property values by basin/formation\n",
    "-- ============================================================\n",
    "CREATE TABLE reservoir_analogs (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    \n",
    "    -- Location/identification\n",
    "    name VARCHAR(100) NOT NULL,\n",
    "    basin VARCHAR(100) NOT NULL,\n",
    "    formation VARCHAR(100),\n",
    "    region VARCHAR(100),           -- e.g., 'Delaware Basin', 'Midland Basin'\n",
    "    country VARCHAR(100) DEFAULT 'USA',\n",
    "    \n",
    "    -- Rock properties (ranges)\n",
    "    permeability_min FLOAT,        -- mD\n",
    "    permeability_max FLOAT,\n",
    "    permeability_typical FLOAT,\n",
    "    \n",
    "    porosity_min FLOAT,            -- fraction\n",
    "    porosity_max FLOAT,\n",
    "    porosity_typical FLOAT,\n",
    "    \n",
    "    -- Depth and pressure\n",
    "    depth_min FLOAT,               -- ft TVD\n",
    "    depth_max FLOAT,\n",
    "    depth_typical FLOAT,\n",
    "    pressure_gradient FLOAT,       -- psi/ft (normal ~0.465)\n",
    "    temperature_gradient FLOAT,    -- °F/100ft\n",
    "    \n",
    "    -- Fluid properties\n",
    "    api_gravity FLOAT,             -- °API\n",
    "    gor FLOAT,                     -- scf/stb (gas-oil ratio)\n",
    "    water_salinity FLOAT,          -- ppm TDS\n",
    "    \n",
    "    -- Recovery factors\n",
    "    primary_rf FLOAT,              -- Primary recovery factor\n",
    "    waterflood_rf FLOAT,           -- Waterflood recovery factor\n",
    "    \n",
    "    -- Additional properties as JSON\n",
    "    properties JSONB,\n",
    "    \n",
    "    -- Searchable description\n",
    "    description TEXT,\n",
    "    description_embedding vector(1536),\n",
    "    \n",
    "    -- Source/quality\n",
    "    data_source VARCHAR(255),\n",
    "    confidence VARCHAR(20),        -- 'high', 'medium', 'low'\n",
    "    \n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_analogs_basin ON reservoir_analogs (basin);\n",
    "CREATE INDEX idx_analogs_formation ON reservoir_analogs (formation);\n",
    "CREATE INDEX idx_analogs_embedding ON reservoir_analogs \n",
    "    USING ivfflat (description_embedding vector_cosine_ops) WITH (lists = 50);\n",
    "\n",
    "-- ============================================================\n",
    "-- 4. User Corrections Database\n",
    "-- Learn from user feedback\n",
    "-- ============================================================\n",
    "CREATE TABLE user_corrections (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    \n",
    "    -- Context\n",
    "    session_id VARCHAR(50),\n",
    "    user_id VARCHAR(100),\n",
    "    \n",
    "    -- The correction\n",
    "    original_response TEXT NOT NULL,\n",
    "    corrected_response TEXT NOT NULL,\n",
    "    correction_type VARCHAR(50),   -- 'factual', 'syntax', 'physics', 'preference'\n",
    "    \n",
    "    -- What triggered this\n",
    "    query_context TEXT,\n",
    "    keywords_involved TEXT[],\n",
    "    \n",
    "    -- Embedding for retrieval\n",
    "    context_embedding vector(1536),\n",
    "    \n",
    "    -- Status\n",
    "    incorporated BOOLEAN DEFAULT false,  -- Has this been learned?\n",
    "    verified BOOLEAN DEFAULT false,      -- Has an expert verified?\n",
    "    \n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_corrections_embedding ON user_corrections \n",
    "    USING ivfflat (context_embedding vector_cosine_ops) WITH (lists = 50);\n",
    "CREATE INDEX idx_corrections_type ON user_corrections (correction_type);\n",
    "\n",
    "-- ============================================================\n",
    "-- 5. Conversation Sessions (for context)\n",
    "-- ============================================================\n",
    "CREATE TABLE conversation_sessions (\n",
    "    id VARCHAR(50) PRIMARY KEY,\n",
    "    \n",
    "    -- Session state (JSON blob)\n",
    "    state JSONB NOT NULL DEFAULT '{}',\n",
    "    \n",
    "    -- Current deck being built\n",
    "    current_deck TEXT,\n",
    "    deck_valid BOOLEAN DEFAULT false,\n",
    "    \n",
    "    -- Timestamps\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    last_activity TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- ============================================================\n",
    "-- 6. Generated Decks History\n",
    "-- ============================================================\n",
    "CREATE TABLE generated_decks (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    \n",
    "    session_id VARCHAR(50) REFERENCES conversation_sessions(id),\n",
    "    \n",
    "    -- The deck\n",
    "    deck_content TEXT NOT NULL,\n",
    "    deck_hash VARCHAR(64),         -- SHA256 for deduplication\n",
    "    \n",
    "    -- Specification used\n",
    "    specification JSONB,\n",
    "    \n",
    "    -- Validation results\n",
    "    validation_result JSONB,\n",
    "    \n",
    "    -- Simulation results (if run)\n",
    "    simulation_job_id VARCHAR(50),\n",
    "    simulation_result JSONB,\n",
    "    \n",
    "    -- What assumptions were made\n",
    "    assumptions TEXT[],\n",
    "    analog_used VARCHAR(100),\n",
    "    \n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_decks_session ON generated_decks (session_id);\n",
    "CREATE INDEX idx_decks_hash ON generated_decks (deck_hash);\n",
    "''';\n",
    "\n",
    "print(\"Schema SQL defined\")\n",
    "print(f\"Total length: {len(SCHEMA_SQL)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation\n",
    "\n",
    "We need to convert text into vector embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingResult:\n",
    "    \"\"\"Result of embedding generation\"\"\"\n",
    "    text: str\n",
    "    embedding: np.ndarray\n",
    "    model: str\n",
    "    dimensions: int\n",
    "\n",
    "class EmbeddingProvider(ABC):\n",
    "    \"\"\"Abstract base for embedding providers\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def embed(self, text: str) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def embed_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def dimensions(self) -> int:\n",
    "        pass\n",
    "\n",
    "class OpenAIEmbeddings(EmbeddingProvider):\n",
    "    \"\"\"\n",
    "    OpenAI text-embedding-ada-002 (or newer models).\n",
    "    \n",
    "    Best quality, but requires API key and costs money.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"text-embedding-ada-002\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self._dimensions = 1536  # ada-002 dimensions\n",
    "    \n",
    "    @property\n",
    "    def dimensions(self) -> int:\n",
    "        return self._dimensions\n",
    "    \n",
    "    def embed(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Embed a single text\"\"\"\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI(api_key=self.api_key)\n",
    "        response = client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=text\n",
    "        )\n",
    "        return np.array(response.data[0].embedding)\n",
    "    \n",
    "    def embed_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        \"\"\"Embed multiple texts efficiently\"\"\"\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI(api_key=self.api_key)\n",
    "        response = client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=texts\n",
    "        )\n",
    "        return [np.array(item.embedding) for item in response.data]\n",
    "\n",
    "class SentenceTransformerEmbeddings(EmbeddingProvider):\n",
    "    \"\"\"\n",
    "    Local embeddings using sentence-transformers.\n",
    "    \n",
    "    Free, runs locally, good quality for technical text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self._model = None\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        if self._model is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._model = SentenceTransformer(self.model_name)\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def dimensions(self) -> int:\n",
    "        # Depends on model\n",
    "        model_dims = {\n",
    "            \"all-MiniLM-L6-v2\": 384,\n",
    "            \"all-mpnet-base-v2\": 768,\n",
    "            \"multi-qa-mpnet-base-dot-v1\": 768,\n",
    "        }\n",
    "        return model_dims.get(self.model_name, 384)\n",
    "    \n",
    "    def embed(self, text: str) -> np.ndarray:\n",
    "        return self.model.encode(text)\n",
    "    \n",
    "    def embed_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return [emb for emb in embeddings]\n",
    "\n",
    "class MockEmbeddings(EmbeddingProvider):\n",
    "    \"\"\"\n",
    "    Mock embeddings for testing.\n",
    "    \n",
    "    Generates deterministic random vectors based on text hash.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dimensions: int = 1536):\n",
    "        self._dimensions = dimensions\n",
    "    \n",
    "    @property\n",
    "    def dimensions(self) -> int:\n",
    "        return self._dimensions\n",
    "    \n",
    "    def embed(self, text: str) -> np.ndarray:\n",
    "        # Use text hash as seed for reproducibility\n",
    "        seed = hash(text) % (2**32)\n",
    "        rng = np.random.RandomState(seed)\n",
    "        vec = rng.randn(self._dimensions)\n",
    "        # Normalize to unit length\n",
    "        return vec / np.linalg.norm(vec)\n",
    "    \n",
    "    def embed_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        return [self.embed(t) for t in texts]\n",
    "\n",
    "# Example usage\n",
    "mock_embedder = MockEmbeddings(dimensions=1536)\n",
    "\n",
    "test_texts = [\n",
    "    \"WELSPECS defines well specifications including name and location\",\n",
    "    \"COMPDAT specifies well completion data and perforations\",\n",
    "    \"The Permian Basin has typical permeability of 50-200 mD\"\n",
    "]\n",
    "\n",
    "embeddings = mock_embedder.embed_batch(test_texts)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Dimensions: {embeddings[0].shape}\")\n",
    "print(f\"Sample values: {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Knowledge Service\n",
    "\n",
    "The main interface for CLARISSA to interact with the knowledge layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"A single search result from the knowledge base\"\"\"\n",
    "    content: str\n",
    "    source_type: str\n",
    "    similarity: float\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class KeywordInfo:\n",
    "    \"\"\"Structured information about an ECLIPSE keyword\"\"\"\n",
    "    keyword: str\n",
    "    section: str\n",
    "    description: str\n",
    "    syntax_template: Optional[str] = None\n",
    "    parameters: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    examples: List[str] = field(default_factory=list)\n",
    "    opm_supported: bool = True\n",
    "    opm_notes: Optional[str] = None\n",
    "    related_keywords: List[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class AnalogData:\n",
    "    \"\"\"Reservoir analog data for default values\"\"\"\n",
    "    name: str\n",
    "    basin: str\n",
    "    formation: Optional[str] = None\n",
    "    \n",
    "    # Rock properties\n",
    "    permeability_typical: Optional[float] = None\n",
    "    porosity_typical: Optional[float] = None\n",
    "    \n",
    "    # Depth/pressure\n",
    "    depth_typical: Optional[float] = None\n",
    "    pressure_gradient: Optional[float] = None\n",
    "    \n",
    "    # Fluids\n",
    "    api_gravity: Optional[float] = None\n",
    "    gor: Optional[float] = None\n",
    "    \n",
    "    confidence: str = \"medium\"\n",
    "    \n",
    "    def to_defaults(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary of default values for deck generation\"\"\"\n",
    "        defaults = {}\n",
    "        if self.permeability_typical:\n",
    "            defaults['permx'] = self.permeability_typical\n",
    "        if self.porosity_typical:\n",
    "            defaults['poro'] = self.porosity_typical\n",
    "        if self.depth_typical:\n",
    "            defaults['top_depth'] = self.depth_typical\n",
    "        if self.pressure_gradient and self.depth_typical:\n",
    "            defaults['datum_pressure'] = self.pressure_gradient * self.depth_typical\n",
    "        if self.api_gravity:\n",
    "            defaults['api_gravity'] = self.api_gravity\n",
    "        return defaults\n",
    "\n",
    "class KnowledgeService:\n",
    "    \"\"\"\n",
    "    Main interface to CLARISSA's knowledge layer.\n",
    "    \n",
    "    Provides:\n",
    "    - Semantic search over documentation\n",
    "    - Keyword lookup and assistance\n",
    "    - Analog-based default values\n",
    "    - Hybrid search (semantic + keyword)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        db_connection,  # asyncpg or psycopg2 connection\n",
    "        embedder: EmbeddingProvider\n",
    "    ):\n",
    "        self.db = db_connection\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    async def search_documentation(\n",
    "        self,\n",
    "        query: str,\n",
    "        limit: int = 5,\n",
    "        source_types: List[str] = None,\n",
    "        min_similarity: float = 0.7\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        Semantic search over simulator documentation.\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            limit: Maximum results to return\n",
    "            source_types: Filter by source ('manual', 'tutorial', etc.)\n",
    "            min_similarity: Minimum cosine similarity threshold\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        \n",
    "        # Build SQL query\n",
    "        sql = \"\"\"\n",
    "            SELECT \n",
    "                content,\n",
    "                source_type,\n",
    "                section_title,\n",
    "                keywords,\n",
    "                1 - (embedding <=> $1::vector) as similarity\n",
    "            FROM simulator_knowledge\n",
    "            WHERE 1 - (embedding <=> $1::vector) > $2\n",
    "        \"\"\"\n",
    "        \n",
    "        params = [query_embedding.tolist(), min_similarity]\n",
    "        \n",
    "        if source_types:\n",
    "            sql += \" AND source_type = ANY($3)\"\n",
    "            params.append(source_types)\n",
    "        \n",
    "        sql += \" ORDER BY similarity DESC LIMIT $\" + str(len(params) + 1)\n",
    "        params.append(limit)\n",
    "        \n",
    "        # Execute query\n",
    "        rows = await self.db.fetch(sql, *params)\n",
    "        \n",
    "        results = []\n",
    "        for row in rows:\n",
    "            results.append(SearchResult(\n",
    "                content=row['content'],\n",
    "                source_type=row['source_type'],\n",
    "                similarity=row['similarity'],\n",
    "                metadata={\n",
    "                    'section_title': row['section_title'],\n",
    "                    'keywords': row['keywords']\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def get_keyword_info(self, keyword: str) -> Optional[KeywordInfo]:\n",
    "        \"\"\"\n",
    "        Get detailed information about an ECLIPSE keyword.\n",
    "        \"\"\"\n",
    "        sql = \"\"\"\n",
    "            SELECT *\n",
    "            FROM eclipse_keywords\n",
    "            WHERE keyword = $1\n",
    "        \"\"\"\n",
    "        \n",
    "        row = await self.db.fetchrow(sql, keyword.upper())\n",
    "        \n",
    "        if not row:\n",
    "            return None\n",
    "        \n",
    "        return KeywordInfo(\n",
    "            keyword=row['keyword'],\n",
    "            section=row['section'],\n",
    "            description=row['description'],\n",
    "            syntax_template=row['syntax_template'],\n",
    "            parameters=row['parameters'] or [],\n",
    "            examples=row['examples'] or [],\n",
    "            opm_supported=row['opm_supported'],\n",
    "            opm_notes=row['opm_notes'],\n",
    "            related_keywords=row['related_keywords'] or []\n",
    "        )\n",
    "    \n",
    "    async def find_similar_keywords(\n",
    "        self,\n",
    "        description: str,\n",
    "        limit: int = 5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Find keywords by semantic similarity to a description.\n",
    "        \n",
    "        Useful when user describes what they want but doesn't know the keyword.\n",
    "        \"\"\"\n",
    "        query_embedding = self.embedder.embed(description)\n",
    "        \n",
    "        sql = \"\"\"\n",
    "            SELECT \n",
    "                keyword,\n",
    "                1 - (description_embedding <=> $1::vector) as similarity\n",
    "            FROM eclipse_keywords\n",
    "            WHERE opm_supported = true\n",
    "            ORDER BY similarity DESC\n",
    "            LIMIT $2\n",
    "        \"\"\"\n",
    "        \n",
    "        rows = await self.db.fetch(sql, query_embedding.tolist(), limit)\n",
    "        return [(row['keyword'], row['similarity']) for row in rows]\n",
    "    \n",
    "    async def find_analog(\n",
    "        self,\n",
    "        description: str,\n",
    "        limit: int = 3\n",
    "    ) -> List[AnalogData]:\n",
    "        \"\"\"\n",
    "        Find reservoir analogs by semantic matching.\n",
    "        \n",
    "        Example: \"Permian Basin tight oil formation\" → Delaware Basin analog\n",
    "        \"\"\"\n",
    "        query_embedding = self.embedder.embed(description)\n",
    "        \n",
    "        sql = \"\"\"\n",
    "            SELECT *,\n",
    "                1 - (description_embedding <=> $1::vector) as similarity\n",
    "            FROM reservoir_analogs\n",
    "            ORDER BY similarity DESC\n",
    "            LIMIT $2\n",
    "        \"\"\"\n",
    "        \n",
    "        rows = await self.db.fetch(sql, query_embedding.tolist(), limit)\n",
    "        \n",
    "        analogs = []\n",
    "        for row in rows:\n",
    "            analogs.append(AnalogData(\n",
    "                name=row['name'],\n",
    "                basin=row['basin'],\n",
    "                formation=row['formation'],\n",
    "                permeability_typical=row['permeability_typical'],\n",
    "                porosity_typical=row['porosity_typical'],\n",
    "                depth_typical=row['depth_typical'],\n",
    "                pressure_gradient=row['pressure_gradient'],\n",
    "                api_gravity=row['api_gravity'],\n",
    "                gor=row['gor'],\n",
    "                confidence=row['confidence']\n",
    "            ))\n",
    "        \n",
    "        return analogs\n",
    "    \n",
    "    async def get_analog_defaults(\n",
    "        self,\n",
    "        basin: str,\n",
    "        formation: str = None\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Get default values for a specific basin/formation.\n",
    "        \"\"\"\n",
    "        sql = \"\"\"\n",
    "            SELECT *\n",
    "            FROM reservoir_analogs\n",
    "            WHERE LOWER(basin) LIKE $1\n",
    "        \"\"\"\n",
    "        params = [f\"%{basin.lower()}%\"]\n",
    "        \n",
    "        if formation:\n",
    "            sql += \" AND LOWER(formation) LIKE $2\"\n",
    "            params.append(f\"%{formation.lower()}%\")\n",
    "        \n",
    "        sql += \" ORDER BY confidence DESC LIMIT 1\"\n",
    "        \n",
    "        row = await self.db.fetchrow(sql, *params)\n",
    "        \n",
    "        if not row:\n",
    "            return None\n",
    "        \n",
    "        analog = AnalogData(\n",
    "            name=row['name'],\n",
    "            basin=row['basin'],\n",
    "            formation=row['formation'],\n",
    "            permeability_typical=row['permeability_typical'],\n",
    "            porosity_typical=row['porosity_typical'],\n",
    "            depth_typical=row['depth_typical'],\n",
    "            pressure_gradient=row['pressure_gradient'],\n",
    "            api_gravity=row['api_gravity'],\n",
    "            gor=row['gor'],\n",
    "            confidence=row['confidence']\n",
    "        )\n",
    "        \n",
    "        return analog.to_defaults()\n",
    "    \n",
    "    async def hybrid_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        keywords: List[str] = None,\n",
    "        limit: int = 5\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        Combine semantic search with keyword filtering.\n",
    "        \n",
    "        This is often more effective than pure semantic search\n",
    "        for technical documentation.\n",
    "        \"\"\"\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        \n",
    "        # Two-stage scoring: semantic similarity + keyword match\n",
    "        sql = \"\"\"\n",
    "            WITH semantic_results AS (\n",
    "                SELECT \n",
    "                    id,\n",
    "                    content,\n",
    "                    source_type,\n",
    "                    section_title,\n",
    "                    keywords,\n",
    "                    1 - (embedding <=> $1::vector) as semantic_score\n",
    "                FROM simulator_knowledge\n",
    "                WHERE 1 - (embedding <=> $1::vector) > 0.5\n",
    "            )\n",
    "            SELECT \n",
    "                content,\n",
    "                source_type,\n",
    "                section_title,\n",
    "                keywords,\n",
    "                semantic_score,\n",
    "                CASE \n",
    "                    WHEN $2::text[] IS NULL THEN 0\n",
    "                    WHEN keywords && $2::text[] THEN 0.3\n",
    "                    ELSE 0\n",
    "                END as keyword_boost,\n",
    "                semantic_score + CASE \n",
    "                    WHEN $2::text[] IS NULL THEN 0\n",
    "                    WHEN keywords && $2::text[] THEN 0.3\n",
    "                    ELSE 0\n",
    "                END as combined_score\n",
    "            FROM semantic_results\n",
    "            ORDER BY combined_score DESC\n",
    "            LIMIT $3\n",
    "        \"\"\"\n",
    "        \n",
    "        rows = await self.db.fetch(\n",
    "            sql,\n",
    "            query_embedding.tolist(),\n",
    "            keywords,\n",
    "            limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for row in rows:\n",
    "            results.append(SearchResult(\n",
    "                content=row['content'],\n",
    "                source_type=row['source_type'],\n",
    "                similarity=row['combined_score'],\n",
    "                metadata={\n",
    "                    'section_title': row['section_title'],\n",
    "                    'keywords': row['keywords'],\n",
    "                    'semantic_score': row['semantic_score'],\n",
    "                    'keyword_boost': row['keyword_boost']\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def record_correction(\n",
    "        self,\n",
    "        session_id: str,\n",
    "        original: str,\n",
    "        corrected: str,\n",
    "        correction_type: str,\n",
    "        context: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Record a user correction for learning.\n",
    "        \"\"\"\n",
    "        context_text = context or original\n",
    "        context_embedding = self.embedder.embed(context_text)\n",
    "        \n",
    "        sql = \"\"\"\n",
    "            INSERT INTO user_corrections (\n",
    "                session_id,\n",
    "                original_response,\n",
    "                corrected_response,\n",
    "                correction_type,\n",
    "                query_context,\n",
    "                context_embedding\n",
    "            ) VALUES ($1, $2, $3, $4, $5, $6)\n",
    "        \"\"\"\n",
    "        \n",
    "        await self.db.execute(\n",
    "            sql,\n",
    "            session_id,\n",
    "            original,\n",
    "            corrected,\n",
    "            correction_type,\n",
    "            context,\n",
    "            context_embedding.tolist()\n",
    "        )\n",
    "\n",
    "print(\"KnowledgeService class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reservoir Analog Database\n",
    "\n",
    "Pre-populated analog data for common basins and formations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample reservoir analogs database\n",
    "# In production, this would be much more extensive\n",
    "\n",
    "RESERVOIR_ANALOGS = [\n",
    "    {\n",
    "        \"name\": \"Permian Basin - Delaware\",\n",
    "        \"basin\": \"Permian Basin\",\n",
    "        \"formation\": \"Wolfcamp\",\n",
    "        \"region\": \"Delaware Basin\",\n",
    "        \"country\": \"USA\",\n",
    "        \"permeability_min\": 0.001,\n",
    "        \"permeability_max\": 0.1,\n",
    "        \"permeability_typical\": 0.01,  # Very tight\n",
    "        \"porosity_min\": 0.04,\n",
    "        \"porosity_max\": 0.12,\n",
    "        \"porosity_typical\": 0.08,\n",
    "        \"depth_min\": 6000,\n",
    "        \"depth_max\": 12000,\n",
    "        \"depth_typical\": 8500,\n",
    "        \"pressure_gradient\": 0.50,  # Slightly overpressured\n",
    "        \"temperature_gradient\": 1.5,\n",
    "        \"api_gravity\": 42,\n",
    "        \"gor\": 1200,\n",
    "        \"water_salinity\": 150000,\n",
    "        \"primary_rf\": 0.08,\n",
    "        \"waterflood_rf\": 0.15,\n",
    "        \"description\": \"Permian Basin Delaware sub-basin Wolfcamp tight oil unconventional shale play\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"data_source\": \"EIA/USGS compilations\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Permian Basin - Midland\",\n",
    "        \"basin\": \"Permian Basin\",\n",
    "        \"formation\": \"Spraberry\",\n",
    "        \"region\": \"Midland Basin\",\n",
    "        \"country\": \"USA\",\n",
    "        \"permeability_min\": 0.1,\n",
    "        \"permeability_max\": 10,\n",
    "        \"permeability_typical\": 1.0,\n",
    "        \"porosity_min\": 0.06,\n",
    "        \"porosity_max\": 0.14,\n",
    "        \"porosity_typical\": 0.10,\n",
    "        \"depth_min\": 5000,\n",
    "        \"depth_max\": 9000,\n",
    "        \"depth_typical\": 7000,\n",
    "        \"pressure_gradient\": 0.46,\n",
    "        \"temperature_gradient\": 1.4,\n",
    "        \"api_gravity\": 38,\n",
    "        \"gor\": 800,\n",
    "        \"water_salinity\": 100000,\n",
    "        \"primary_rf\": 0.10,\n",
    "        \"waterflood_rf\": 0.25,\n",
    "        \"description\": \"Permian Basin Midland sub-basin Spraberry conventional tight oil\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"data_source\": \"EIA/USGS compilations\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Bakken Formation\",\n",
    "        \"basin\": \"Williston Basin\",\n",
    "        \"formation\": \"Bakken\",\n",
    "        \"region\": \"North Dakota\",\n",
    "        \"country\": \"USA\",\n",
    "        \"permeability_min\": 0.001,\n",
    "        \"permeability_max\": 0.05,\n",
    "        \"permeability_typical\": 0.005,\n",
    "        \"porosity_min\": 0.03,\n",
    "        \"porosity_max\": 0.09,\n",
    "        \"porosity_typical\": 0.06,\n",
    "        \"depth_min\": 9000,\n",
    "        \"depth_max\": 11000,\n",
    "        \"depth_typical\": 10000,\n",
    "        \"pressure_gradient\": 0.52,  # Overpressured\n",
    "        \"temperature_gradient\": 1.8,\n",
    "        \"api_gravity\": 42,\n",
    "        \"gor\": 1500,\n",
    "        \"water_salinity\": 250000,\n",
    "        \"primary_rf\": 0.06,\n",
    "        \"waterflood_rf\": 0.12,\n",
    "        \"description\": \"Bakken shale tight oil unconventional horizontal well development\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"data_source\": \"NDIC/USGS\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Eagle Ford Shale\",\n",
    "        \"basin\": \"Western Gulf Basin\",\n",
    "        \"formation\": \"Eagle Ford\",\n",
    "        \"region\": \"South Texas\",\n",
    "        \"country\": \"USA\",\n",
    "        \"permeability_min\": 0.0001,\n",
    "        \"permeability_max\": 0.01,\n",
    "        \"permeability_typical\": 0.001,\n",
    "        \"porosity_min\": 0.03,\n",
    "        \"porosity_max\": 0.10,\n",
    "        \"porosity_typical\": 0.06,\n",
    "        \"depth_min\": 4000,\n",
    "        \"depth_max\": 14000,\n",
    "        \"depth_typical\": 8000,\n",
    "        \"pressure_gradient\": 0.48,\n",
    "        \"temperature_gradient\": 1.6,\n",
    "        \"api_gravity\": 45,\n",
    "        \"gor\": 2000,\n",
    "        \"water_salinity\": 80000,\n",
    "        \"primary_rf\": 0.05,\n",
    "        \"waterflood_rf\": 0.10,\n",
    "        \"description\": \"Eagle Ford shale play South Texas oil and condensate window\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"data_source\": \"RRC Texas/EIA\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"North Sea Brent\",\n",
    "        \"basin\": \"North Sea\",\n",
    "        \"formation\": \"Brent Group\",\n",
    "        \"region\": \"UK Continental Shelf\",\n",
    "        \"country\": \"UK\",\n",
    "        \"permeability_min\": 50,\n",
    "        \"permeability_max\": 2000,\n",
    "        \"permeability_typical\": 500,\n",
    "        \"porosity_min\": 0.15,\n",
    "        \"porosity_max\": 0.28,\n",
    "        \"porosity_typical\": 0.22,\n",
    "        \"depth_min\": 7000,\n",
    "        \"depth_max\": 12000,\n",
    "        \"depth_typical\": 9000,\n",
    "        \"pressure_gradient\": 0.45,\n",
    "        \"temperature_gradient\": 1.5,\n",
    "        \"api_gravity\": 38,\n",
    "        \"gor\": 600,\n",
    "        \"water_salinity\": 35000,\n",
    "        \"primary_rf\": 0.25,\n",
    "        \"waterflood_rf\": 0.50,\n",
    "        \"description\": \"North Sea Brent province conventional sandstone reservoir waterflooded\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"data_source\": \"OGA UK\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ghawar Field Analog\",\n",
    "        \"basin\": \"Arabian Basin\",\n",
    "        \"formation\": \"Arab-D\",\n",
    "        \"region\": \"Eastern Province\",\n",
    "        \"country\": \"Saudi Arabia\",\n",
    "        \"permeability_min\": 100,\n",
    "        \"permeability_max\": 5000,\n",
    "        \"permeability_typical\": 1000,\n",
    "        \"porosity_min\": 0.15,\n",
    "        \"porosity_max\": 0.30,\n",
    "        \"porosity_typical\": 0.25,\n",
    "        \"depth_min\": 5000,\n",
    "        \"depth_max\": 8000,\n",
    "        \"depth_typical\": 6500,\n",
    "        \"pressure_gradient\": 0.46,\n",
    "        \"temperature_gradient\": 1.2,\n",
    "        \"api_gravity\": 34,\n",
    "        \"gor\": 500,\n",
    "        \"water_salinity\": 180000,\n",
    "        \"primary_rf\": 0.30,\n",
    "        \"waterflood_rf\": 0.60,\n",
    "        \"description\": \"Middle East giant carbonate reservoir Arab formation analog\",\n",
    "        \"confidence\": \"medium\",\n",
    "        \"data_source\": \"Published literature\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Generic Sandstone\",\n",
    "        \"basin\": \"Generic\",\n",
    "        \"formation\": \"Sandstone\",\n",
    "        \"region\": \"Global\",\n",
    "        \"country\": \"Global\",\n",
    "        \"permeability_min\": 10,\n",
    "        \"permeability_max\": 1000,\n",
    "        \"permeability_typical\": 100,\n",
    "        \"porosity_min\": 0.10,\n",
    "        \"porosity_max\": 0.25,\n",
    "        \"porosity_typical\": 0.18,\n",
    "        \"depth_min\": 5000,\n",
    "        \"depth_max\": 10000,\n",
    "        \"depth_typical\": 7500,\n",
    "        \"pressure_gradient\": 0.465,\n",
    "        \"temperature_gradient\": 1.5,\n",
    "        \"api_gravity\": 35,\n",
    "        \"gor\": 500,\n",
    "        \"water_salinity\": 50000,\n",
    "        \"primary_rf\": 0.15,\n",
    "        \"waterflood_rf\": 0.35,\n",
    "        \"description\": \"Generic conventional sandstone reservoir default values\",\n",
    "        \"confidence\": \"low\",\n",
    "        \"data_source\": \"Textbook values\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(RESERVOIR_ANALOGS)} reservoir analogs\")\n",
    "\n",
    "# Display summary\n",
    "for analog in RESERVOIR_ANALOGS:\n",
    "    print(f\"\\n{analog['name']}:\")\n",
    "    print(f\"  Perm: {analog['permeability_typical']} mD\")\n",
    "    print(f\"  Porosity: {analog['porosity_typical']*100:.0f}%\")\n",
    "    print(f\"  Depth: {analog['depth_typical']} ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ECLIPSE Keywords Database\n",
    "\n",
    "Structured keyword information for syntax assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample ECLIPSE keywords database\n",
    "ECLIPSE_KEYWORDS = [\n",
    "    {\n",
    "        \"keyword\": \"DIMENS\",\n",
    "        \"section\": \"RUNSPEC\",\n",
    "        \"description\": \"Specifies the dimensions of the simulation grid (NX, NY, NZ)\",\n",
    "        \"syntax_template\": \"DIMENS\\n  NX NY NZ /\",\n",
    "        \"parameters\": [\n",
    "            {\"name\": \"NX\", \"type\": \"int\", \"description\": \"Number of cells in X direction\"},\n",
    "            {\"name\": \"NY\", \"type\": \"int\", \"description\": \"Number of cells in Y direction\"},\n",
    "            {\"name\": \"NZ\", \"type\": \"int\", \"description\": \"Number of cells in Z direction\"}\n",
    "        ],\n",
    "        \"examples\": [\n",
    "            \"DIMENS\\n  10 10 3 /\",\n",
    "            \"DIMENS\\n  100 100 20 /\"\n",
    "        ],\n",
    "        \"required_keywords\": [],\n",
    "        \"incompatible_keywords\": [],\n",
    "        \"related_keywords\": [\"DX\", \"DY\", \"DZ\", \"TOPS\"],\n",
    "        \"opm_supported\": True,\n",
    "        \"opm_notes\": None\n",
    "    },\n",
    "    {\n",
    "        \"keyword\": \"WELSPECS\",\n",
    "        \"section\": \"SCHEDULE\",\n",
    "        \"description\": \"Defines well specifications including name, group, head location, and preferred phase\",\n",
    "        \"syntax_template\": \"WELSPECS\\n  'WELL' 'GROUP' I J DEPTH 'PHASE' /\\n/\",\n",
    "        \"parameters\": [\n",
    "            {\"name\": \"WELL\", \"type\": \"string\", \"description\": \"Well name (max 8 chars)\"},\n",
    "            {\"name\": \"GROUP\", \"type\": \"string\", \"description\": \"Group name\"},\n",
    "            {\"name\": \"I\", \"type\": \"int\", \"description\": \"I-index of well head\"},\n",
    "            {\"name\": \"J\", \"type\": \"int\", \"description\": \"J-index of well head\"},\n",
    "            {\"name\": \"DEPTH\", \"type\": \"float\", \"description\": \"Reference depth for BHP\"},\n",
    "            {\"name\": \"PHASE\", \"type\": \"string\", \"description\": \"Preferred phase (OIL/WATER/GAS)\"}\n",
    "        ],\n",
    "        \"examples\": [\n",
    "            \"WELSPECS\\n  'PROD1' 'G1' 5 5 8335 'OIL' /\\n/\",\n",
    "            \"WELSPECS\\n  'INJ1' 'INJ' 1 1 8335 'WATER' /\\n  'PROD1' 'PROD' 10 10 8335 'OIL' /\\n/\"\n",
    "        ],\n",
    "        \"required_keywords\": [\"COMPDAT\"],\n",
    "        \"incompatible_keywords\": [],\n",
    "        \"related_keywords\": [\"COMPDAT\", \"WCONPROD\", \"WCONINJE\"],\n",
    "        \"opm_supported\": True,\n",
    "        \"opm_notes\": None\n",
    "    },\n",
    "    {\n",
    "        \"keyword\": \"COMPDAT\",\n",
    "        \"section\": \"SCHEDULE\",\n",
    "        \"description\": \"Specifies well completion data including perforation intervals and connection properties\",\n",
    "        \"syntax_template\": \"COMPDAT\\n  'WELL' I J K1 K2 'STATUS' SAT CF DIAM KH S D /\\n/\",\n",
    "        \"parameters\": [\n",
    "            {\"name\": \"WELL\", \"type\": \"string\", \"description\": \"Well name\"},\n",
    "            {\"name\": \"I\", \"type\": \"int\", \"description\": \"I-index (or 0 for well head)\"},\n",
    "            {\"name\": \"J\", \"type\": \"int\", \"description\": \"J-index (or 0 for well head)\"},\n",
    "            {\"name\": \"K1\", \"type\": \"int\", \"description\": \"Top layer of completion\"},\n",
    "            {\"name\": \"K2\", \"type\": \"int\", \"description\": \"Bottom layer of completion\"},\n",
    "            {\"name\": \"STATUS\", \"type\": \"string\", \"description\": \"OPEN or SHUT\"},\n",
    "            {\"name\": \"DIAM\", \"type\": \"float\", \"description\": \"Wellbore diameter (ft)\"}\n",
    "        ],\n",
    "        \"examples\": [\n",
    "            \"COMPDAT\\n  'PROD1' 5 5 1 3 'OPEN' 2* 0.5 /\\n/\"\n",
    "        ],\n",
    "        \"required_keywords\": [\"WELSPECS\"],\n",
    "        \"incompatible_keywords\": [],\n",
    "        \"related_keywords\": [\"WELSPECS\", \"WCONPROD\", \"WCONINJE\"],\n",
    "        \"opm_supported\": True,\n",
    "        \"opm_notes\": None\n",
    "    },\n",
    "    {\n",
    "        \"keyword\": \"WCONPROD\",\n",
    "        \"section\": \"SCHEDULE\",\n",
    "        \"description\": \"Sets production well controls including rate targets and pressure limits\",\n",
    "        \"syntax_template\": \"WCONPROD\\n  'WELL' 'STATUS' 'MODE' ORAT WRAT GRAT LRAT RESV BHP /\\n/\",\n",
    "        \"parameters\": [\n",
    "            {\"name\": \"WELL\", \"type\": \"string\", \"description\": \"Well name\"},\n",
    "            {\"name\": \"STATUS\", \"type\": \"string\", \"description\": \"OPEN, SHUT, or AUTO\"},\n",
    "            {\"name\": \"MODE\", \"type\": \"string\", \"description\": \"Control mode (ORAT/WRAT/GRAT/LRAT/BHP)\"},\n",
    "            {\"name\": \"ORAT\", \"type\": \"float\", \"description\": \"Oil rate target (STB/day)\"},\n",
    "            {\"name\": \"BHP\", \"type\": \"float\", \"description\": \"Minimum BHP limit (psia)\"}\n",
    "        ],\n",
    "        \"examples\": [\n",
    "            \"WCONPROD\\n  'PROD1' 'OPEN' 'ORAT' 1000 4* 500 /\\n/\"\n",
    "        ],\n",
    "        \"required_keywords\": [\"WELSPECS\", \"COMPDAT\"],\n",
    "        \"incompatible_keywords\": [\"WCONINJE\"],\n",
    "        \"related_keywords\": [\"WELSPECS\", \"COMPDAT\", \"WCONINJE\"],\n",
    "        \"opm_supported\": True,\n",
    "        \"opm_notes\": None\n",
    "    },\n",
    "    {\n",
    "        \"keyword\": \"WCONINJE\",\n",
    "        \"section\": \"SCHEDULE\",\n",
    "        \"description\": \"Sets injection well controls for water or gas injection\",\n",
    "        \"syntax_template\": \"WCONINJE\\n  'WELL' 'TYPE' 'STATUS' 'MODE' RATE RESV BHP /\\n/\",\n",
    "        \"parameters\": [\n",
    "            {\"name\": \"WELL\", \"type\": \"string\", \"description\": \"Well name\"},\n",
    "            {\"name\": \"TYPE\", \"type\": \"string\", \"description\": \"Injection type (WATER/GAS)\"},\n",
    "            {\"name\": \"STATUS\", \"type\": \"string\", \"description\": \"OPEN or SHUT\"},\n",
    "            {\"name\": \"MODE\", \"type\": \"string\", \"description\": \"Control mode (RATE/BHP)\"},\n",
    "            {\"name\": \"RATE\", \"type\": \"float\", \"description\": \"Injection rate target\"},\n",
    "            {\"name\": \"BHP\", \"type\": \"float\", \"description\": \"Maximum BHP limit (psia)\"}\n",
    "        ],\n",
    "        \"examples\": [\n",
    "            \"WCONINJE\\n  'INJ1' 'WATER' 'OPEN' 'RATE' 5000 1* 6000 /\\n/\"\n",
    "        ],\n",
    "        \"required_keywords\": [\"WELSPECS\", \"COMPDAT\"],\n",
    "        \"incompatible_keywords\": [\"WCONPROD\"],\n",
    "        \"related_keywords\": [\"WELSPECS\", \"COMPDAT\", \"WCONPROD\"],\n",
    "        \"opm_supported\": True,\n",
    "        \"opm_notes\": None\n",
    "    },\n",
    "    {\n",
    "        \"keyword\": \"EQUIL\",\n",
    "        \"section\": \"SOLUTION\",\n",
    "        \"description\": \"Defines equilibration data for initialization including datum, contacts, and pressures\",\n",
    "        \"syntax_template\": \"EQUIL\\n  DATUM PDAT WOC PCOW GOC PCOG RSVD RVVD ACC /\",\n",
    "        \"parameters\": [\n",
    "            {\"name\": \"DATUM\", \"type\": \"float\", \"description\": \"Datum depth (ft)\"},\n",
    "            {\"name\": \"PDAT\", \"type\": \"float\", \"description\": \"Pressure at datum (psia)\"},\n",
    "            {\"name\": \"WOC\", \"type\": \"float\", \"description\": \"Water-oil contact depth (ft)\"},\n",
    "            {\"name\": \"GOC\", \"type\": \"float\", \"description\": \"Gas-oil contact depth (ft)\"}\n",
    "        ],\n",
    "        \"examples\": [\n",
    "            \"EQUIL\\n  8400 4000 8500 0 0 0 /\"\n",
    "        ],\n",
    "        \"required_keywords\": [\"DIMENS\", \"PORO\", \"PERMX\"],\n",
    "        \"incompatible_keywords\": [],\n",
    "        \"related_keywords\": [\"SWOF\", \"SGOF\", \"PVTO\", \"PVTW\"],\n",
    "        \"opm_supported\": True,\n",
    "        \"opm_notes\": None\n",
    "    },\n",
    "    {\n",
    "        \"keyword\": \"SWOF\",\n",
    "        \"section\": \"PROPS\",\n",
    "        \"description\": \"Water-oil relative permeability and capillary pressure table\",\n",
    "        \"syntax_template\": \"SWOF\\n  SW KRW KROW PCOW\\n  ... /\",\n",
    "        \"parameters\": [\n",
    "            {\"name\": \"SW\", \"type\": \"float\", \"description\": \"Water saturation\"},\n",
    "            {\"name\": \"KRW\", \"type\": \"float\", \"description\": \"Water relative permeability\"},\n",
    "            {\"name\": \"KROW\", \"type\": \"float\", \"description\": \"Oil relative permeability (water-oil)\"},\n",
    "            {\"name\": \"PCOW\", \"type\": \"float\", \"description\": \"Water-oil capillary pressure\"}\n",
    "        ],\n",
    "        \"examples\": [\n",
    "            \"SWOF\\n  0.2 0.0 1.0 0\\n  0.5 0.15 0.3 0\\n  0.8 0.35 0.0 0 /\"\n",
    "        ],\n",
    "        \"required_keywords\": [\"WATER\", \"OIL\"],\n",
    "        \"incompatible_keywords\": [],\n",
    "        \"related_keywords\": [\"SGOF\", \"SWFN\", \"SOF3\"],\n",
    "        \"opm_supported\": True,\n",
    "        \"opm_notes\": None\n",
    "    },\n",
    "    {\n",
    "        \"keyword\": \"TSTEP\",\n",
    "        \"section\": \"SCHEDULE\",\n",
    "        \"description\": \"Advances simulation time by specified timestep sizes\",\n",
    "        \"syntax_template\": \"TSTEP\\n  N*DT ... /\",\n",
    "        \"parameters\": [\n",
    "            {\"name\": \"DT\", \"type\": \"float\", \"description\": \"Timestep size (days)\"},\n",
    "            {\"name\": \"N\", \"type\": \"int\", \"description\": \"Number of repetitions\"}\n",
    "        ],\n",
    "        \"examples\": [\n",
    "            \"TSTEP\\n  30 30 30 30 /\",\n",
    "            \"TSTEP\\n  12*30 /\"\n",
    "        ],\n",
    "        \"required_keywords\": [],\n",
    "        \"incompatible_keywords\": [],\n",
    "        \"related_keywords\": [\"DATES\", \"TUNING\"],\n",
    "        \"opm_supported\": True,\n",
    "        \"opm_notes\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(ECLIPSE_KEYWORDS)} keyword entries\")\n",
    "\n",
    "for kw in ECLIPSE_KEYWORDS:\n",
    "    print(f\"  {kw['keyword']:12s} ({kw['section']}) - OPM: {'✓' if kw['opm_supported'] else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Knowledge Ingestion Pipeline\n",
    "\n",
    "How to populate the knowledge base from documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"A chunk of documentation ready for embedding\"\"\"\n",
    "    content: str\n",
    "    source_type: str\n",
    "    source_file: str\n",
    "    section_title: str\n",
    "    keywords: List[str]\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Process documentation files into chunks for embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ECLIPSE keywords to detect in text\n",
    "    KEYWORD_PATTERN = re.compile(r'\\b([A-Z]{2,10})\\b')\n",
    "    KNOWN_KEYWORDS = {\n",
    "        'RUNSPEC', 'GRID', 'PROPS', 'SOLUTION', 'SUMMARY', 'SCHEDULE',\n",
    "        'DIMENS', 'DX', 'DY', 'DZ', 'TOPS', 'PERMX', 'PERMY', 'PERMZ',\n",
    "        'PORO', 'NTG', 'SWOF', 'SGOF', 'PVTO', 'PVDO', 'PVTW', 'PVDG',\n",
    "        'EQUIL', 'WELSPECS', 'COMPDAT', 'WCONPROD', 'WCONINJE', 'TSTEP'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract ECLIPSE keywords mentioned in text\"\"\"\n",
    "        found = self.KEYWORD_PATTERN.findall(text)\n",
    "        return [kw for kw in found if kw in self.KNOWN_KEYWORDS]\n",
    "    \n",
    "    def chunk_markdown(self, content: str, source_file: str) -> List[DocumentChunk]:\n",
    "        \"\"\"\n",
    "        Split markdown documentation into chunks.\n",
    "        \n",
    "        Respects heading boundaries when possible.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split by headers\n",
    "        sections = re.split(r'\\n(#{1,3}\\s+[^\\n]+)\\n', content)\n",
    "        \n",
    "        current_title = \"Introduction\"\n",
    "        current_content = \"\"\n",
    "        \n",
    "        for i, section in enumerate(sections):\n",
    "            if section.startswith('#'):\n",
    "                # This is a header\n",
    "                if current_content.strip():\n",
    "                    # Save previous section\n",
    "                    chunks.extend(self._split_section(\n",
    "                        current_content, current_title, source_file, 'manual'\n",
    "                    ))\n",
    "                current_title = section.strip('# ').strip()\n",
    "                current_content = \"\"\n",
    "            else:\n",
    "                current_content += section\n",
    "        \n",
    "        # Don't forget last section\n",
    "        if current_content.strip():\n",
    "            chunks.extend(self._split_section(\n",
    "                current_content, current_title, source_file, 'manual'\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_section(\n",
    "        self,\n",
    "        content: str,\n",
    "        title: str,\n",
    "        source_file: str,\n",
    "        source_type: str\n",
    "    ) -> List[DocumentChunk]:\n",
    "        \"\"\"Split a section into chunks with overlap\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Simple character-based chunking\n",
    "        # In production, use sentence-aware chunking\n",
    "        text = content.strip()\n",
    "        \n",
    "        if len(text) <= self.chunk_size:\n",
    "            # Small enough for single chunk\n",
    "            chunks.append(DocumentChunk(\n",
    "                content=text,\n",
    "                source_type=source_type,\n",
    "                source_file=source_file,\n",
    "                section_title=title,\n",
    "                keywords=self.extract_keywords(text)\n",
    "            ))\n",
    "        else:\n",
    "            # Split with overlap\n",
    "            start = 0\n",
    "            while start < len(text):\n",
    "                end = start + self.chunk_size\n",
    "                chunk_text = text[start:end]\n",
    "                \n",
    "                chunks.append(DocumentChunk(\n",
    "                    content=chunk_text,\n",
    "                    source_type=source_type,\n",
    "                    source_file=source_file,\n",
    "                    section_title=title,\n",
    "                    keywords=self.extract_keywords(chunk_text)\n",
    "                ))\n",
    "                \n",
    "                start = end - self.chunk_overlap\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class KnowledgeIngester:\n",
    "    \"\"\"\n",
    "    Ingest documents into the knowledge base.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        db_connection,\n",
    "        embedder: EmbeddingProvider,\n",
    "        processor: DocumentProcessor = None\n",
    "    ):\n",
    "        self.db = db_connection\n",
    "        self.embedder = embedder\n",
    "        self.processor = processor or DocumentProcessor()\n",
    "    \n",
    "    async def ingest_markdown(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Ingest a markdown documentation file.\n",
    "        \"\"\"\n",
    "        from pathlib import Path\n",
    "        \n",
    "        path = Path(filepath)\n",
    "        content = path.read_text()\n",
    "        \n",
    "        # Process into chunks\n",
    "        chunks = self.processor.chunk_markdown(content, path.name)\n",
    "        \n",
    "        print(f\"Processing {len(chunks)} chunks from {path.name}\")\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        batch_size = 10\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i+batch_size]\n",
    "            texts = [c.content for c in batch]\n",
    "            embeddings = self.embedder.embed_batch(texts)\n",
    "            \n",
    "            # Insert into database\n",
    "            for chunk, embedding in zip(batch, embeddings):\n",
    "                await self._insert_chunk(chunk, embedding)\n",
    "        \n",
    "        print(f\"Ingested {len(chunks)} chunks\")\n",
    "    \n",
    "    async def _insert_chunk(self, chunk: DocumentChunk, embedding):\n",
    "        \"\"\"Insert a single chunk into the database\"\"\"\n",
    "        sql = \"\"\"\n",
    "            INSERT INTO simulator_knowledge (\n",
    "                source_type, source_file, section_title,\n",
    "                content, embedding, keywords\n",
    "            ) VALUES ($1, $2, $3, $4, $5, $6)\n",
    "        \"\"\"\n",
    "        \n",
    "        await self.db.execute(\n",
    "            sql,\n",
    "            chunk.source_type,\n",
    "            chunk.source_file,\n",
    "            chunk.section_title,\n",
    "            chunk.content,\n",
    "            embedding.tolist(),\n",
    "            chunk.keywords\n",
    "        )\n",
    "    \n",
    "    async def ingest_keywords(self, keywords: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Ingest keyword reference data.\n",
    "        \"\"\"\n",
    "        print(f\"Ingesting {len(keywords)} keywords\")\n",
    "        \n",
    "        for kw in keywords:\n",
    "            # Generate embedding for description\n",
    "            embedding = self.embedder.embed(kw['description'])\n",
    "            \n",
    "            sql = \"\"\"\n",
    "                INSERT INTO eclipse_keywords (\n",
    "                    keyword, section, description, syntax_template,\n",
    "                    parameters, examples, required_keywords,\n",
    "                    incompatible_keywords, related_keywords,\n",
    "                    opm_supported, opm_notes, description_embedding\n",
    "                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)\n",
    "                ON CONFLICT (keyword) DO UPDATE SET\n",
    "                    description = EXCLUDED.description,\n",
    "                    description_embedding = EXCLUDED.description_embedding\n",
    "            \"\"\"\n",
    "            \n",
    "            import json\n",
    "            await self.db.execute(\n",
    "                sql,\n",
    "                kw['keyword'],\n",
    "                kw['section'],\n",
    "                kw['description'],\n",
    "                kw.get('syntax_template'),\n",
    "                json.dumps(kw.get('parameters', [])),\n",
    "                json.dumps(kw.get('examples', [])),\n",
    "                kw.get('required_keywords', []),\n",
    "                kw.get('incompatible_keywords', []),\n",
    "                kw.get('related_keywords', []),\n",
    "                kw.get('opm_supported', True),\n",
    "                kw.get('opm_notes'),\n",
    "                embedding.tolist()\n",
    "            )\n",
    "        \n",
    "        print(f\"Ingested {len(keywords)} keywords\")\n",
    "    \n",
    "    async def ingest_analogs(self, analogs: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Ingest reservoir analog data.\n",
    "        \"\"\"\n",
    "        print(f\"Ingesting {len(analogs)} reservoir analogs\")\n",
    "        \n",
    "        for analog in analogs:\n",
    "            # Generate embedding for description\n",
    "            embedding = self.embedder.embed(analog['description'])\n",
    "            \n",
    "            sql = \"\"\"\n",
    "                INSERT INTO reservoir_analogs (\n",
    "                    name, basin, formation, region, country,\n",
    "                    permeability_min, permeability_max, permeability_typical,\n",
    "                    porosity_min, porosity_max, porosity_typical,\n",
    "                    depth_min, depth_max, depth_typical,\n",
    "                    pressure_gradient, temperature_gradient,\n",
    "                    api_gravity, gor, water_salinity,\n",
    "                    primary_rf, waterflood_rf,\n",
    "                    description, description_embedding,\n",
    "                    data_source, confidence\n",
    "                ) VALUES (\n",
    "                    $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,\n",
    "                    $11, $12, $13, $14, $15, $16, $17, $18, $19,\n",
    "                    $20, $21, $22, $23, $24, $25\n",
    "                )\n",
    "            \"\"\"\n",
    "            \n",
    "            await self.db.execute(\n",
    "                sql,\n",
    "                analog['name'],\n",
    "                analog['basin'],\n",
    "                analog.get('formation'),\n",
    "                analog.get('region'),\n",
    "                analog.get('country', 'USA'),\n",
    "                analog.get('permeability_min'),\n",
    "                analog.get('permeability_max'),\n",
    "                analog.get('permeability_typical'),\n",
    "                analog.get('porosity_min'),\n",
    "                analog.get('porosity_max'),\n",
    "                analog.get('porosity_typical'),\n",
    "                analog.get('depth_min'),\n",
    "                analog.get('depth_max'),\n",
    "                analog.get('depth_typical'),\n",
    "                analog.get('pressure_gradient'),\n",
    "                analog.get('temperature_gradient'),\n",
    "                analog.get('api_gravity'),\n",
    "                analog.get('gor'),\n",
    "                analog.get('water_salinity'),\n",
    "                analog.get('primary_rf'),\n",
    "                analog.get('waterflood_rf'),\n",
    "                analog['description'],\n",
    "                embedding.tolist(),\n",
    "                analog.get('data_source'),\n",
    "                analog.get('confidence', 'medium')\n",
    "            )\n",
    "        \n",
    "        print(f\"Ingested {len(analogs)} reservoir analogs\")\n",
    "\n",
    "print(\"Knowledge ingestion classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Knowledge Layer\n",
    "\n",
    "### Components Built\n",
    "\n",
    "1. **Database Schema** - PostgreSQL + pgvector for semantic search\n",
    "2. **Embedding Providers** - OpenAI, SentenceTransformers, Mock\n",
    "3. **KnowledgeService** - Main interface for CLARISSA\n",
    "4. **Analog Database** - Default values by basin/formation\n",
    "5. **Keyword Reference** - Structured ECLIPSE keyword info\n",
    "6. **Ingestion Pipeline** - Document → Chunks → Embeddings → DB\n",
    "\n",
    "### How CLARISSA Uses This\n",
    "\n",
    "```python\n",
    "# User says: \"Build me a Permian Basin waterflood model\"\n",
    "\n",
    "# 1. Find analog\n",
    "analogs = await knowledge.find_analog(\"Permian Basin\")\n",
    "defaults = analogs[0].to_defaults()\n",
    "# → {'permx': 0.01, 'poro': 0.08, 'top_depth': 8500, ...}\n",
    "\n",
    "# 2. User asks: \"How do I define producer controls?\"\n",
    "results = await knowledge.search_documentation(\"producer controls\")\n",
    "# → Returns docs about WCONPROD\n",
    "\n",
    "# 3. Get keyword syntax\n",
    "kw_info = await knowledge.get_keyword_info(\"WCONPROD\")\n",
    "# → Structured syntax, examples, parameters\n",
    "```\n",
    "\n",
    "### Next Notebook\n",
    "\n",
    "In **04_LLM_Conversation.ipynb**, we'll cover:\n",
    "- Slot extraction from natural language\n",
    "- Clarification request generation\n",
    "- Conversation state management\n",
    "- Prompt engineering for CLARISSA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": [],
   "name": "03_Knowledge_Layer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}