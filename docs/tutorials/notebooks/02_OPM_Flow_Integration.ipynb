{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - OPM Flow Integration\n",
    "\n",
    "**CLARISSA** uses OPM Flow as its open-source simulation backend. This notebook covers:\n",
    "\n",
    "1. OPM Flow architecture and capabilities\n",
    "2. Docker containerization for isolated execution\n",
    "3. Running simulations programmatically\n",
    "4. Parsing simulation results (SMSPEC, UNSMRY)\n",
    "5. Error handling and recovery strategies\n",
    "6. Integration with CLARISSA's simulation layer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is OPM Flow?\n",
    "\n",
    "**OPM** (Open Porous Media) is an open-source project providing reservoir simulation capabilities.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | OPM Flow | ECLIPSE (Commercial) |\n",
    "|---------|----------|----------------------|\n",
    "| License | GPL v3 (Free) | $$$$ |\n",
    "| Black-Oil | ✅ Full | ✅ Full |\n",
    "| Compositional | ⚠️ Limited | ✅ Full |\n",
    "| Thermal | ❌ No | ✅ Yes |\n",
    "| Input Format | ECLIPSE-compatible | Native |\n",
    "| Output Format | ECLIPSE-compatible | Native |\n",
    "\n",
    "### Why OPM for CLARISSA?\n",
    "\n",
    "1. **No licensing barriers** - Democratizes access to simulation\n",
    "2. **ECLIPSE compatibility** - Uses same input/output formats\n",
    "3. **Docker-friendly** - Easy to containerize and scale\n",
    "4. **Active development** - Backed by SINTEF, Equinor, and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPM Flow version and capabilities check\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def check_opm_installation() -> dict:\n",
    "    \"\"\"\n",
    "    Check if OPM Flow is installed and get version info.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'installed': False,\n",
    "        'version': None,\n",
    "        'path': None,\n",
    "        'capabilities': []\n",
    "    }\n",
    "    \n",
    "    # Check if flow binary exists\n",
    "    flow_path = shutil.which('flow')\n",
    "    if flow_path:\n",
    "        result['installed'] = True\n",
    "        result['path'] = flow_path\n",
    "        \n",
    "        # Get version\n",
    "        try:\n",
    "            proc = subprocess.run(\n",
    "                ['flow', '--version'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=10\n",
    "            )\n",
    "            result['version'] = proc.stdout.strip()\n",
    "        except Exception as e:\n",
    "            result['version'] = f\"Error: {e}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Check installation\n",
    "opm_info = check_opm_installation()\n",
    "print(f\"OPM Flow installed: {opm_info['installed']}\")\n",
    "if opm_info['installed']:\n",
    "    print(f\"Path: {opm_info['path']}\")\n",
    "    print(f\"Version: {opm_info['version']}\")\n",
    "else:\n",
    "    print(\"OPM Flow not found - will use Docker container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Docker Configuration for OPM Flow\n",
    "\n",
    "For production deployments, we containerize OPM Flow for:\n",
    "- Isolation from host system\n",
    "- Reproducible builds\n",
    "- Easy scaling with Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile for OPM Flow\n",
    "OPM_DOCKERFILE = '''\n",
    "# ============================================================\n",
    "# OPM Flow Docker Image for CLARISSA\n",
    "# ============================================================\n",
    "FROM ubuntu:22.04 as base\n",
    "\n",
    "# Avoid interactive prompts during build\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Install OPM from official repository\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    software-properties-common \\\\\n",
    "    curl \\\\\n",
    "    && add-apt-repository ppa:opm/ppa \\\\\n",
    "    && apt-get update \\\\\n",
    "    && apt-get install -y \\\\\n",
    "        libopm-simulators-bin \\\\\n",
    "        python3 \\\\\n",
    "        python3-pip \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python dependencies for result parsing\n",
    "RUN pip3 install --no-cache-dir \\\\\n",
    "    numpy \\\\\n",
    "    pandas \\\\\n",
    "    ecl-data-io \\\\\n",
    "    resdata\n",
    "\n",
    "# Create working directories\n",
    "RUN mkdir -p /simulation/input /simulation/output /simulation/logs\n",
    "\n",
    "WORKDIR /simulation\n",
    "\n",
    "# Copy helper scripts\n",
    "COPY scripts/run_simulation.py /usr/local/bin/\n",
    "COPY scripts/parse_results.py /usr/local/bin/\n",
    "RUN chmod +x /usr/local/bin/*.py\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD flow --version || exit 1\n",
    "\n",
    "# Default command shows version\n",
    "CMD [\"flow\", \"--version\"]\n",
    "'''\n",
    "\n",
    "print(OPM_DOCKERFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker Compose configuration for local development\n",
    "DOCKER_COMPOSE = '''\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  opm-flow:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.opm\n",
    "    image: clarissa/opm-flow:latest\n",
    "    volumes:\n",
    "      - ./data/input:/simulation/input:ro\n",
    "      - ./data/output:/simulation/output\n",
    "      - ./data/logs:/simulation/logs\n",
    "    environment:\n",
    "      - OMP_NUM_THREADS=4\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          cpus: \"4\"\n",
    "          memory: 8G\n",
    "        reservations:\n",
    "          cpus: \"2\"\n",
    "          memory: 4G\n",
    "    command: [\"flow\", \"--help\"]\n",
    "\n",
    "  # API wrapper for simulation jobs\n",
    "  opm-api:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.opm-api\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    volumes:\n",
    "      - ./data:/simulation\n",
    "    depends_on:\n",
    "      - opm-flow\n",
    "    environment:\n",
    "      - OPM_FLOW_BINARY=/usr/bin/flow\n",
    "      - MAX_CONCURRENT_JOBS=4\n",
    "'''\n",
    "\n",
    "print(DOCKER_COMPOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation Runner Service\n",
    "\n",
    "The `SimulationRunner` manages OPM Flow execution, including:\n",
    "- Job submission and queuing\n",
    "- Progress monitoring\n",
    "- Error capture and analysis\n",
    "- Result retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "class JobStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    CANCELLED = \"cancelled\"\n",
    "\n",
    "@dataclass\n",
    "class SimulationJob:\n",
    "    \"\"\"Represents a simulation job submitted to OPM Flow\"\"\"\n",
    "    job_id: str\n",
    "    deck_path: Path\n",
    "    output_dir: Path\n",
    "    status: JobStatus = JobStatus.PENDING\n",
    "    \n",
    "    # Timing\n",
    "    created_at: datetime = field(default_factory=datetime.utcnow)\n",
    "    started_at: Optional[datetime] = None\n",
    "    completed_at: Optional[datetime] = None\n",
    "    \n",
    "    # Results\n",
    "    return_code: Optional[int] = None\n",
    "    stdout: str = \"\"\n",
    "    stderr: str = \"\"\n",
    "    \n",
    "    # Parsed metrics\n",
    "    timesteps_completed: int = 0\n",
    "    solver_iterations: int = 0\n",
    "    convergence_failures: int = 0\n",
    "    \n",
    "    @property\n",
    "    def runtime_seconds(self) -> Optional[float]:\n",
    "        if self.started_at and self.completed_at:\n",
    "            return (self.completed_at - self.started_at).total_seconds()\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def succeeded(self) -> bool:\n",
    "        return self.status == JobStatus.COMPLETED and self.return_code == 0\n",
    "\n",
    "class SimulationRunner:\n",
    "    \"\"\"\n",
    "    Manages OPM Flow simulation execution.\n",
    "    \n",
    "    This is CLARISSA's interface to the simulation backend.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        flow_binary: str = \"flow\",\n",
    "        work_dir: Path = None,\n",
    "        use_docker: bool = False,\n",
    "        docker_image: str = \"clarissa/opm-flow:latest\"\n",
    "    ):\n",
    "        self.flow_binary = flow_binary\n",
    "        self.work_dir = work_dir or Path(tempfile.gettempdir()) / \"clarissa_sim\"\n",
    "        self.use_docker = use_docker\n",
    "        self.docker_image = docker_image\n",
    "        \n",
    "        # Job tracking\n",
    "        self.jobs: Dict[str, SimulationJob] = {}\n",
    "        \n",
    "        # Ensure work directory exists\n",
    "        self.work_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def create_job(\n",
    "        self,\n",
    "        deck_content: str,\n",
    "        job_name: str = None\n",
    "    ) -> SimulationJob:\n",
    "        \"\"\"\n",
    "        Create a new simulation job from deck content.\n",
    "        \"\"\"\n",
    "        job_id = job_name or str(uuid.uuid4())[:8]\n",
    "        job_dir = self.work_dir / job_id\n",
    "        job_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Write deck file\n",
    "        deck_path = job_dir / \"MODEL.DATA\"\n",
    "        deck_path.write_text(deck_content)\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = job_dir / \"output\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        job = SimulationJob(\n",
    "            job_id=job_id,\n",
    "            deck_path=deck_path,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        self.jobs[job_id] = job\n",
    "        return job\n",
    "    \n",
    "    def _build_command(self, job: SimulationJob) -> List[str]:\n",
    "        \"\"\"\n",
    "        Build the command line for OPM Flow execution.\n",
    "        \"\"\"\n",
    "        if self.use_docker:\n",
    "            return [\n",
    "                \"docker\", \"run\", \"--rm\",\n",
    "                \"-v\", f\"{job.deck_path.parent}:/simulation/input:ro\",\n",
    "                \"-v\", f\"{job.output_dir}:/simulation/output\",\n",
    "                self.docker_image,\n",
    "                \"flow\",\n",
    "                \"--output-dir=/simulation/output\",\n",
    "                \"/simulation/input/MODEL.DATA\"\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                self.flow_binary,\n",
    "                f\"--output-dir={job.output_dir}\",\n",
    "                str(job.deck_path)\n",
    "            ]\n",
    "    \n",
    "    async def run_async(self, job: SimulationJob) -> SimulationJob:\n",
    "        \"\"\"\n",
    "        Run simulation asynchronously.\n",
    "        \"\"\"\n",
    "        job.status = JobStatus.RUNNING\n",
    "        job.started_at = datetime.utcnow()\n",
    "        \n",
    "        cmd = self._build_command(job)\n",
    "        \n",
    "        try:\n",
    "            process = await asyncio.create_subprocess_exec(\n",
    "                *cmd,\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stderr=asyncio.subprocess.PIPE\n",
    "            )\n",
    "            \n",
    "            stdout, stderr = await process.communicate()\n",
    "            \n",
    "            job.return_code = process.returncode\n",
    "            job.stdout = stdout.decode('utf-8', errors='replace')\n",
    "            job.stderr = stderr.decode('utf-8', errors='replace')\n",
    "            \n",
    "            if process.returncode == 0:\n",
    "                job.status = JobStatus.COMPLETED\n",
    "            else:\n",
    "                job.status = JobStatus.FAILED\n",
    "                \n",
    "        except Exception as e:\n",
    "            job.status = JobStatus.FAILED\n",
    "            job.stderr = str(e)\n",
    "        \n",
    "        finally:\n",
    "            job.completed_at = datetime.utcnow()\n",
    "        \n",
    "        # Parse output for metrics\n",
    "        self._parse_output_metrics(job)\n",
    "        \n",
    "        return job\n",
    "    \n",
    "    def run_sync(\n",
    "        self,\n",
    "        job: SimulationJob,\n",
    "        timeout: int = 300\n",
    "    ) -> SimulationJob:\n",
    "        \"\"\"\n",
    "        Run simulation synchronously with timeout.\n",
    "        \"\"\"\n",
    "        job.status = JobStatus.RUNNING\n",
    "        job.started_at = datetime.utcnow()\n",
    "        \n",
    "        cmd = self._build_command(job)\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                cmd,\n",
    "                capture_output=True,\n",
    "                timeout=timeout,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            job.return_code = result.returncode\n",
    "            job.stdout = result.stdout\n",
    "            job.stderr = result.stderr\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                job.status = JobStatus.COMPLETED\n",
    "            else:\n",
    "                job.status = JobStatus.FAILED\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            job.status = JobStatus.FAILED\n",
    "            job.stderr = f\"Simulation timed out after {timeout} seconds\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            job.status = JobStatus.FAILED\n",
    "            job.stderr = str(e)\n",
    "        \n",
    "        finally:\n",
    "            job.completed_at = datetime.utcnow()\n",
    "        \n",
    "        self._parse_output_metrics(job)\n",
    "        return job\n",
    "    \n",
    "    def _parse_output_metrics(self, job: SimulationJob):\n",
    "        \"\"\"\n",
    "        Extract metrics from simulation output.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Parse timesteps from stdout\n",
    "        timestep_matches = re.findall(\n",
    "            r'Time step\\s+(\\d+)',\n",
    "            job.stdout\n",
    "        )\n",
    "        if timestep_matches:\n",
    "            job.timesteps_completed = max(int(t) for t in timestep_matches)\n",
    "        \n",
    "        # Parse solver iterations\n",
    "        iter_matches = re.findall(\n",
    "            r'(\\d+)\\s+linear iterations',\n",
    "            job.stdout\n",
    "        )\n",
    "        if iter_matches:\n",
    "            job.solver_iterations = sum(int(i) for i in iter_matches)\n",
    "        \n",
    "        # Count convergence failures\n",
    "        job.convergence_failures = job.stderr.count('convergence failure')\n",
    "\n",
    "# Example usage\n",
    "print(\"SimulationRunner class defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test deck\n",
    "TEST_DECK = '''\n",
    "-- Minimal test deck for OPM Flow\n",
    "RUNSPEC\n",
    "\n",
    "TITLE\n",
    "CLARISSA Test Model\n",
    "\n",
    "DIMENS\n",
    "  5 5 2 /\n",
    "\n",
    "OIL\n",
    "WATER\n",
    "\n",
    "FIELD\n",
    "\n",
    "START\n",
    "  1 'JAN' 2025 /\n",
    "\n",
    "GRID\n",
    "\n",
    "DX\n",
    "  50*100 /\n",
    "\n",
    "DY\n",
    "  50*100 /\n",
    "\n",
    "DZ\n",
    "  50*20 /\n",
    "\n",
    "TOPS\n",
    "  25*8000 /\n",
    "\n",
    "PERMX\n",
    "  50*100 /\n",
    "\n",
    "PERMY\n",
    "  50*100 /\n",
    "\n",
    "PERMZ\n",
    "  50*10 /\n",
    "\n",
    "PORO\n",
    "  50*0.2 /\n",
    "\n",
    "PROPS\n",
    "\n",
    "DENSITY\n",
    "  50.0 62.4 0.06 /\n",
    "\n",
    "PVTW\n",
    "  4000 1.01 3.0E-6 0.5 0 /\n",
    "\n",
    "PVDO\n",
    "  100  1.05 2.0\n",
    "  2000 1.02 1.5\n",
    "  4000 1.01 1.2 /\n",
    "/\n",
    "\n",
    "ROCK\n",
    "  4000 4.0E-6 /\n",
    "\n",
    "SWOF\n",
    "  0.2 0.0   1.0  0\n",
    "  0.5 0.15  0.3  0\n",
    "  0.8 0.35  0.0  0 /\n",
    "/\n",
    "\n",
    "SOLUTION\n",
    "\n",
    "EQUIL\n",
    "  8010 4000 8100 0 0 0 /\n",
    "\n",
    "SUMMARY\n",
    "\n",
    "FOPR\n",
    "FWPR\n",
    "FPR\n",
    "\n",
    "SCHEDULE\n",
    "\n",
    "WELSPECS\n",
    "  'PROD' 'G' 5 5 8010 'OIL' /\n",
    "/\n",
    "\n",
    "COMPDAT\n",
    "  'PROD' 5 5 1 2 'OPEN' 2* 0.5 /\n",
    "/\n",
    "\n",
    "WCONPROD\n",
    "  'PROD' 'OPEN' 'ORAT' 500 4* 1000 /\n",
    "/\n",
    "\n",
    "TSTEP\n",
    "  10*30 /\n",
    "\n",
    "END\n",
    "'''\n",
    "\n",
    "print(\"Test deck created\")\n",
    "print(f\"Deck size: {len(TEST_DECK)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate job creation (without actual execution)\n",
    "runner = SimulationRunner(work_dir=Path('/tmp/clarissa_demo'))\n",
    "\n",
    "job = runner.create_job(TEST_DECK, job_name=\"test_model\")\n",
    "\n",
    "print(f\"Job ID: {job.job_id}\")\n",
    "print(f\"Deck path: {job.deck_path}\")\n",
    "print(f\"Output dir: {job.output_dir}\")\n",
    "print(f\"Status: {job.status.value}\")\n",
    "\n",
    "# Show the command that would be executed\n",
    "cmd = runner._build_command(job)\n",
    "print(f\"\\nCommand: {' '.join(cmd)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parsing Simulation Results\n",
    "\n",
    "OPM Flow produces ECLIPSE-compatible output files:\n",
    "\n",
    "| File Extension | Content |\n",
    "|----------------|----------|\n",
    "| .SMSPEC | Summary specification (vector names, units) |\n",
    "| .UNSMRY | Summary data (time series) |\n",
    "| .EGRID | Grid geometry |\n",
    "| .INIT | Initial state |\n",
    "| .UNRST | Restart file (cell data over time) |\n",
    "| .PRT | Print file (text log) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from typing import Tuple, Dict, List, Any\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class SummaryVector:\n",
    "    \"\"\"A single summary output vector (time series)\"\"\"\n",
    "    keyword: str        # e.g., 'FOPR', 'WBHP'\n",
    "    wgname: str         # Well/group name (or ':+:+:+:+' for field)\n",
    "    unit: str           # e.g., 'STB/DAY', 'PSIA'\n",
    "    values: np.ndarray  # Time series values\n",
    "\n",
    "@dataclass\n",
    "class SummaryData:\n",
    "    \"\"\"Parsed summary file data\"\"\"\n",
    "    times: np.ndarray           # Time values (days)\n",
    "    vectors: Dict[str, SummaryVector]  # keyword -> vector\n",
    "    \n",
    "    @property\n",
    "    def num_timesteps(self) -> int:\n",
    "        return len(self.times)\n",
    "    \n",
    "    def get_vector(self, keyword: str, wgname: str = None) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get values for a summary vector.\n",
    "        \n",
    "        Args:\n",
    "            keyword: Vector keyword (e.g., 'FOPR', 'WOPR')\n",
    "            wgname: Well/group name for well-level vectors\n",
    "        \"\"\"\n",
    "        key = keyword if wgname is None else f\"{keyword}:{wgname}\"\n",
    "        if key in self.vectors:\n",
    "            return self.vectors[key].values\n",
    "        # Try field-level if no wgname specified\n",
    "        if wgname is None and keyword in self.vectors:\n",
    "            return self.vectors[keyword].values\n",
    "        return None\n",
    "\n",
    "class EclipseBinaryReader:\n",
    "    \"\"\"\n",
    "    Reader for ECLIPSE binary file format.\n",
    "    \n",
    "    ECLIPSE binary files use a specific format:\n",
    "    - Header: keyword (8 chars) + count (int) + type (4 chars)\n",
    "    - Data: array of values\n",
    "    - Each block wrapped with Fortran record markers\n",
    "    \"\"\"\n",
    "    \n",
    "    TYPE_MAP = {\n",
    "        'INTE': ('i', 4),   # Integer\n",
    "        'REAL': ('f', 4),   # Float (single)\n",
    "        'DOUB': ('d', 8),   # Double\n",
    "        'CHAR': ('s', 8),   # Character string\n",
    "        'LOGI': ('i', 4),   # Logical (stored as int)\n",
    "        'MESS': (None, 0),  # Message (no data)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, filepath: Path):\n",
    "        self.filepath = Path(filepath)\n",
    "        self.file = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.file = open(self.filepath, 'rb')\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        if self.file:\n",
    "            self.file.close()\n",
    "    \n",
    "    def read_record(self) -> Tuple[str, List[Any]]:\n",
    "        \"\"\"\n",
    "        Read a single record from the binary file.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (keyword, values)\n",
    "        \"\"\"\n",
    "        # Read Fortran record marker (4 bytes)\n",
    "        marker_data = self.file.read(4)\n",
    "        if not marker_data:\n",
    "            return None, None\n",
    "        \n",
    "        record_size = struct.unpack('>i', marker_data)[0]\n",
    "        \n",
    "        # Read header\n",
    "        keyword = self.file.read(8).decode('ascii').strip()\n",
    "        count = struct.unpack('>i', self.file.read(4))[0]\n",
    "        data_type = self.file.read(4).decode('ascii').strip()\n",
    "        \n",
    "        # Read trailing marker\n",
    "        self.file.read(4)\n",
    "        \n",
    "        # Read data if present\n",
    "        values = []\n",
    "        if count > 0 and data_type in self.TYPE_MAP:\n",
    "            fmt, size = self.TYPE_MAP[data_type]\n",
    "            if fmt:\n",
    "                # Read data block with markers\n",
    "                self.file.read(4)  # Leading marker\n",
    "                \n",
    "                if data_type == 'CHAR':\n",
    "                    for _ in range(count):\n",
    "                        values.append(\n",
    "                            self.file.read(8).decode('ascii').strip()\n",
    "                        )\n",
    "                else:\n",
    "                    for _ in range(count):\n",
    "                        val = struct.unpack('>' + fmt, self.file.read(size))[0]\n",
    "                        values.append(val)\n",
    "                \n",
    "                self.file.read(4)  # Trailing marker\n",
    "        \n",
    "        return keyword, values\n",
    "    \n",
    "    def read_all(self) -> List[Tuple[str, List[Any]]]:\n",
    "        \"\"\"Read all records from file\"\"\"\n",
    "        records = []\n",
    "        while True:\n",
    "            keyword, values = self.read_record()\n",
    "            if keyword is None:\n",
    "                break\n",
    "            records.append((keyword, values))\n",
    "        return records\n",
    "\n",
    "print(\"EclipseBinaryReader class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level result parser\n",
    "class ResultParser:\n",
    "    \"\"\"\n",
    "    Parse OPM Flow simulation results.\n",
    "    \n",
    "    This is CLARISSA's interface for extracting meaningful\n",
    "    information from simulation output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = Path(output_dir)\n",
    "    \n",
    "    def find_output_files(self) -> Dict[str, Path]:\n",
    "        \"\"\"Find all output files in the directory\"\"\"\n",
    "        files = {}\n",
    "        \n",
    "        extensions = [\n",
    "            '.SMSPEC', '.UNSMRY', '.EGRID', '.INIT', \n",
    "            '.UNRST', '.PRT', '.RSM'\n",
    "        ]\n",
    "        \n",
    "        for ext in extensions:\n",
    "            matches = list(self.output_dir.glob(f'*{ext}'))\n",
    "            if matches:\n",
    "                files[ext] = matches[0]\n",
    "        \n",
    "        return files\n",
    "    \n",
    "    def parse_print_file(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse the .PRT (print) file for run statistics.\n",
    "        \"\"\"\n",
    "        files = self.find_output_files()\n",
    "        prt_file = files.get('.PRT')\n",
    "        \n",
    "        if not prt_file or not prt_file.exists():\n",
    "            return {}\n",
    "        \n",
    "        content = prt_file.read_text()\n",
    "        \n",
    "        stats = {\n",
    "            'errors': [],\n",
    "            'warnings': [],\n",
    "            'timesteps': 0,\n",
    "            'solver_time': 0.0\n",
    "        }\n",
    "        \n",
    "        import re\n",
    "        \n",
    "        # Extract errors\n",
    "        stats['errors'] = re.findall(\n",
    "            r'\\*\\*\\*ERROR\\*\\*\\*(.+?)(?=\\n|$)', \n",
    "            content,\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Extract warnings\n",
    "        stats['warnings'] = re.findall(\n",
    "            r'Warning:(.+?)(?=\\n|$)',\n",
    "            content,\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Count timesteps\n",
    "        stats['timesteps'] = len(re.findall(r'Report step', content))\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def get_summary_vectors(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List available summary vectors.\n",
    "        \"\"\"\n",
    "        files = self.find_output_files()\n",
    "        smspec = files.get('.SMSPEC')\n",
    "        \n",
    "        if not smspec:\n",
    "            return []\n",
    "        \n",
    "        vectors = []\n",
    "        try:\n",
    "            with EclipseBinaryReader(smspec) as reader:\n",
    "                records = reader.read_all()\n",
    "                \n",
    "                keywords = None\n",
    "                wgnames = None\n",
    "                \n",
    "                for kw, values in records:\n",
    "                    if kw == 'KEYWORDS':\n",
    "                        keywords = values\n",
    "                    elif kw == 'WGNAMES':\n",
    "                        wgnames = values\n",
    "                \n",
    "                if keywords and wgnames:\n",
    "                    for k, w in zip(keywords, wgnames):\n",
    "                        if w and w != ':+:+:+:+':\n",
    "                            vectors.append(f\"{k}:{w}\")\n",
    "                        else:\n",
    "                            vectors.append(k)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading SMSPEC: {e}\")\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def get_production_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get high-level production summary.\n",
    "        \n",
    "        This is what CLARISSA reports back to the user.\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'status': 'unknown',\n",
    "            'timesteps': 0,\n",
    "            'final_time_days': 0,\n",
    "            'cumulative_oil': 0,\n",
    "            'cumulative_water': 0,\n",
    "            'cumulative_gas': 0,\n",
    "            'final_pressure': 0,\n",
    "            'water_breakthrough': None,\n",
    "            'errors': [],\n",
    "            'warnings': []\n",
    "        }\n",
    "        \n",
    "        # Parse PRT file for status\n",
    "        prt_stats = self.parse_print_file()\n",
    "        summary['errors'] = prt_stats.get('errors', [])\n",
    "        summary['warnings'] = prt_stats.get('warnings', [])\n",
    "        summary['timesteps'] = prt_stats.get('timesteps', 0)\n",
    "        \n",
    "        if summary['errors']:\n",
    "            summary['status'] = 'failed'\n",
    "        elif summary['timesteps'] > 0:\n",
    "            summary['status'] = 'completed'\n",
    "        \n",
    "        # TODO: Parse UNSMRY for actual values\n",
    "        # This requires full implementation of binary reader\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"ResultParser class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Handling and Recovery\n",
    "\n",
    "Simulations can fail for many reasons. CLARISSA must:\n",
    "1. Detect failures accurately\n",
    "2. Diagnose the root cause\n",
    "3. Suggest fixes to the user\n",
    "4. Automatically retry with corrections when appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "import re\n",
    "\n",
    "class ErrorCategory(Enum):\n",
    "    \"\"\"Categories of simulation errors\"\"\"\n",
    "    SYNTAX = \"syntax\"              # Deck parsing errors\n",
    "    INITIALIZATION = \"init\"        # EQUIL/initial conditions\n",
    "    CONVERGENCE = \"convergence\"    # Solver convergence\n",
    "    WELL = \"well\"                  # Well-related errors\n",
    "    GRID = \"grid\"                  # Grid/geometry issues\n",
    "    PVT = \"pvt\"                    # Fluid property issues\n",
    "    NUMERICAL = \"numerical\"        # Numerical instability\n",
    "    UNKNOWN = \"unknown\"            # Unclassified\n",
    "\n",
    "@dataclass\n",
    "class DiagnosedError:\n",
    "    \"\"\"A diagnosed simulation error with suggested fix\"\"\"\n",
    "    category: ErrorCategory\n",
    "    message: str\n",
    "    line_number: Optional[int] = None\n",
    "    keyword: Optional[str] = None\n",
    "    suggested_fix: Optional[str] = None\n",
    "    auto_fixable: bool = False\n",
    "\n",
    "class ErrorDiagnostics:\n",
    "    \"\"\"\n",
    "    Diagnose simulation errors and suggest fixes.\n",
    "    \n",
    "    This is critical for CLARISSA's error recovery capability.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Error patterns and their diagnoses\n",
    "    ERROR_PATTERNS = [\n",
    "        # Syntax errors\n",
    "        (\n",
    "            r'Unknown keyword[:\\s]+([A-Z]+)',\n",
    "            ErrorCategory.SYNTAX,\n",
    "            lambda m: f\"Unrecognized keyword: {m.group(1)}\",\n",
    "            lambda m: f\"Remove or replace keyword {m.group(1)} with a supported alternative\"\n",
    "        ),\n",
    "        (\n",
    "            r'Error in keyword ([A-Z]+).*line (\\d+)',\n",
    "            ErrorCategory.SYNTAX,\n",
    "            lambda m: f\"Syntax error in {m.group(1)} at line {m.group(2)}\",\n",
    "            lambda m: f\"Check syntax of {m.group(1)} keyword near line {m.group(2)}\"\n",
    "        ),\n",
    "        \n",
    "        # Initialization errors\n",
    "        (\n",
    "            r'Negative (pressure|saturation) in cell \\((\\d+),(\\d+),(\\d+)\\)',\n",
    "            ErrorCategory.INITIALIZATION,\n",
    "            lambda m: f\"Negative {m.group(1)} at cell ({m.group(2)},{m.group(3)},{m.group(4)})\",\n",
    "            lambda m: \"Check EQUIL datum pressure or contact depths\"\n",
    "        ),\n",
    "        (\n",
    "            r'Water saturation.*above 1|Saturation.*out of bounds',\n",
    "            ErrorCategory.INITIALIZATION,\n",
    "            lambda m: \"Invalid saturation values during initialization\",\n",
    "            lambda m: \"Verify SWOF/SGOF endpoint saturations and EQUIL contacts\"\n",
    "        ),\n",
    "        \n",
    "        # Convergence errors\n",
    "        (\n",
    "            r'Convergence failure at report step (\\d+)',\n",
    "            ErrorCategory.CONVERGENCE,\n",
    "            lambda m: f\"Convergence failure at step {m.group(1)}\",\n",
    "            lambda m: \"Reduce timestep size or add TUNING keyword\"\n",
    "        ),\n",
    "        (\n",
    "            r'Maximum number of iterations exceeded',\n",
    "            ErrorCategory.CONVERGENCE,\n",
    "            lambda m: \"Solver exceeded maximum iterations\",\n",
    "            lambda m: \"Increase MAXITER or improve initial guess via smaller timesteps\"\n",
    "        ),\n",
    "        \n",
    "        # Well errors\n",
    "        (\n",
    "            r\"Well '?([A-Za-z0-9_]+)'?.*outside.*grid\",\n",
    "            ErrorCategory.WELL,\n",
    "            lambda m: f\"Well {m.group(1)} is outside grid boundaries\",\n",
    "            lambda m: f\"Check WELSPECS I,J indices for well {m.group(1)}\"\n",
    "        ),\n",
    "        (\n",
    "            r\"Well '?([A-Za-z0-9_]+)'?.*no.*(connection|perforation)\",\n",
    "            ErrorCategory.WELL,\n",
    "            lambda m: f\"Well {m.group(1)} has no valid connections\",\n",
    "            lambda m: f\"Check COMPDAT for well {m.group(1)} - verify layer indices\"\n",
    "        ),\n",
    "        \n",
    "        # Grid errors\n",
    "        (\n",
    "            r'Invalid cell volume|Zero.*pore volume',\n",
    "            ErrorCategory.GRID,\n",
    "            lambda m: \"Invalid cell geometry (zero or negative volume)\",\n",
    "            lambda m: \"Check DX, DY, DZ values - ensure all positive\"\n",
    "        ),\n",
    "        \n",
    "        # PVT errors\n",
    "        (\n",
    "            r'(PVTO|PVDO|PVTW|PVDG).*not.*monotonic',\n",
    "            ErrorCategory.PVT,\n",
    "            lambda m: f\"Non-monotonic data in {m.group(1)} table\",\n",
    "            lambda m: f\"Ensure {m.group(1)} pressure values are strictly increasing\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    @classmethod\n",
    "    def diagnose(\n",
    "        cls,\n",
    "        stderr: str,\n",
    "        stdout: str = \"\"\n",
    "    ) -> List[DiagnosedError]:\n",
    "        \"\"\"\n",
    "        Diagnose errors from simulation output.\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        combined = stderr + \"\\n\" + stdout\n",
    "        \n",
    "        for pattern, category, msg_func, fix_func in cls.ERROR_PATTERNS:\n",
    "            matches = re.finditer(pattern, combined, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                errors.append(DiagnosedError(\n",
    "                    category=category,\n",
    "                    message=msg_func(match),\n",
    "                    suggested_fix=fix_func(match)\n",
    "                ))\n",
    "        \n",
    "        # If no specific errors found but simulation failed\n",
    "        if not errors and 'error' in combined.lower():\n",
    "            errors.append(DiagnosedError(\n",
    "                category=ErrorCategory.UNKNOWN,\n",
    "                message=\"Simulation failed with unrecognized error\",\n",
    "                suggested_fix=\"Review full error output for details\"\n",
    "            ))\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    @classmethod\n",
    "    def suggest_deck_fix(\n",
    "        cls,\n",
    "        error: DiagnosedError,\n",
    "        deck_content: str\n",
    "    ) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Suggest specific deck modifications to fix an error.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (modified_deck, description)\n",
    "        \"\"\"\n",
    "        if error.category == ErrorCategory.CONVERGENCE:\n",
    "            # Add TUNING keyword if not present\n",
    "            if 'TUNING' not in deck_content:\n",
    "                # Insert before TSTEP in SCHEDULE\n",
    "                tuning_block = '''\n",
    "-- Added by CLARISSA for convergence improvement\n",
    "TUNING\n",
    "  1.0 5.0 0.1 /    -- Initial, max, min timestep\n",
    "  /\n",
    "  12 1 20 /        -- Max nonlinear, linear iterations\n",
    "/\n",
    "\n",
    "'''\n",
    "                modified = deck_content.replace(\n",
    "                    'TSTEP',\n",
    "                    tuning_block + 'TSTEP'\n",
    "                )\n",
    "                return modified, \"Added TUNING keyword for convergence control\"\n",
    "        \n",
    "        # Default: return unchanged\n",
    "        return deck_content, \"No automatic fix available\"\n",
    "\n",
    "# Test error diagnosis\n",
    "test_stderr = '''\n",
    "***ERROR*** Convergence failure at report step 5\n",
    "***ERROR*** Well 'PROD1' is outside grid boundaries\n",
    "Warning: Non-monotonic data in PVDO table\n",
    "'''\n",
    "\n",
    "diagnosed = ErrorDiagnostics.diagnose(test_stderr)\n",
    "print(f\"Found {len(diagnosed)} errors:\\n\")\n",
    "for err in diagnosed:\n",
    "    print(f\"Category: {err.category.value}\")\n",
    "    print(f\"Message: {err.message}\")\n",
    "    print(f\"Fix: {err.suggested_fix}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Simulation Service\n",
    "\n",
    "Putting it all together into a service that CLARISSA can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimulationResult:\n",
    "    \"\"\"\n",
    "    Complete result from a simulation run.\n",
    "    \n",
    "    This is what CLARISSA returns to the user.\n",
    "    \"\"\"\n",
    "    success: bool\n",
    "    job_id: str\n",
    "    runtime_seconds: float\n",
    "    \n",
    "    # Summary metrics\n",
    "    timesteps_completed: int\n",
    "    final_time_days: float\n",
    "    \n",
    "    # Production totals (if available)\n",
    "    cumulative_oil_stb: Optional[float] = None\n",
    "    cumulative_water_stb: Optional[float] = None\n",
    "    cumulative_gas_mscf: Optional[float] = None\n",
    "    \n",
    "    # Key events\n",
    "    water_breakthrough_days: Optional[float] = None\n",
    "    peak_oil_rate_stb_day: Optional[float] = None\n",
    "    final_pressure_psi: Optional[float] = None\n",
    "    \n",
    "    # Errors and warnings\n",
    "    errors: List[DiagnosedError] = field(default_factory=list)\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "    \n",
    "    # Raw data paths\n",
    "    output_directory: Optional[Path] = None\n",
    "\n",
    "class SimulationService:\n",
    "    \"\"\"\n",
    "    High-level simulation service for CLARISSA.\n",
    "    \n",
    "    Orchestrates:\n",
    "    - Deck validation\n",
    "    - Simulation execution\n",
    "    - Result parsing\n",
    "    - Error diagnosis\n",
    "    - Automatic retry with fixes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        runner: SimulationRunner,\n",
    "        max_retries: int = 2\n",
    "    ):\n",
    "        self.runner = runner\n",
    "        self.max_retries = max_retries\n",
    "    \n",
    "    async def run_simulation(\n",
    "        self,\n",
    "        deck_content: str,\n",
    "        auto_fix: bool = True\n",
    "    ) -> SimulationResult:\n",
    "        \"\"\"\n",
    "        Run a simulation with automatic error recovery.\n",
    "        \"\"\"\n",
    "        current_deck = deck_content\n",
    "        all_errors = []\n",
    "        \n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            # Create and run job\n",
    "            job = self.runner.create_job(\n",
    "                current_deck,\n",
    "                job_name=f\"sim_attempt_{attempt}\"\n",
    "            )\n",
    "            \n",
    "            job = await self.runner.run_async(job)\n",
    "            \n",
    "            # Parse results\n",
    "            parser = ResultParser(job.output_dir)\n",
    "            \n",
    "            if job.succeeded:\n",
    "                # Success! Parse and return results\n",
    "                summary = parser.get_production_summary()\n",
    "                \n",
    "                return SimulationResult(\n",
    "                    success=True,\n",
    "                    job_id=job.job_id,\n",
    "                    runtime_seconds=job.runtime_seconds or 0,\n",
    "                    timesteps_completed=job.timesteps_completed,\n",
    "                    final_time_days=summary.get('final_time_days', 0),\n",
    "                    cumulative_oil_stb=summary.get('cumulative_oil'),\n",
    "                    cumulative_water_stb=summary.get('cumulative_water'),\n",
    "                    warnings=summary.get('warnings', []),\n",
    "                    output_directory=job.output_dir\n",
    "                )\n",
    "            \n",
    "            # Diagnose errors\n",
    "            errors = ErrorDiagnostics.diagnose(job.stderr, job.stdout)\n",
    "            all_errors.extend(errors)\n",
    "            \n",
    "            # Try auto-fix if enabled and we have retries left\n",
    "            if auto_fix and attempt < self.max_retries and errors:\n",
    "                for error in errors:\n",
    "                    current_deck, fix_desc = ErrorDiagnostics.suggest_deck_fix(\n",
    "                        error, current_deck\n",
    "                    )\n",
    "                    if fix_desc != \"No automatic fix available\":\n",
    "                        print(f\"Auto-fix applied: {fix_desc}\")\n",
    "                        break\n",
    "                else:\n",
    "                    # No fixable errors found\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Return failure result\n",
    "        return SimulationResult(\n",
    "            success=False,\n",
    "            job_id=job.job_id,\n",
    "            runtime_seconds=job.runtime_seconds or 0,\n",
    "            timesteps_completed=job.timesteps_completed,\n",
    "            final_time_days=0,\n",
    "            errors=all_errors,\n",
    "            output_directory=job.output_dir\n",
    "        )\n",
    "    \n",
    "    def format_result_summary(self, result: SimulationResult) -> str:\n",
    "        \"\"\"\n",
    "        Format result for user presentation.\n",
    "        \n",
    "        This is what CLARISSA shows to the engineer.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        if result.success:\n",
    "            lines.append(f\"✓ Simulation completed successfully\")\n",
    "            lines.append(f\"  Runtime: {result.runtime_seconds:.1f} seconds\")\n",
    "            lines.append(f\"  Timesteps: {result.timesteps_completed}\")\n",
    "            \n",
    "            if result.cumulative_oil_stb:\n",
    "                lines.append(f\"  Cumulative oil: {result.cumulative_oil_stb:,.0f} STB\")\n",
    "            if result.cumulative_water_stb:\n",
    "                lines.append(f\"  Cumulative water: {result.cumulative_water_stb:,.0f} STB\")\n",
    "            if result.water_breakthrough_days:\n",
    "                lines.append(f\"  Water breakthrough: Day {result.water_breakthrough_days:.0f}\")\n",
    "        else:\n",
    "            lines.append(f\"✗ Simulation failed\")\n",
    "            lines.append(f\"  Completed {result.timesteps_completed} timesteps before failure\")\n",
    "            \n",
    "            if result.errors:\n",
    "                lines.append(\"\")\n",
    "                lines.append(\"Errors diagnosed:\")\n",
    "                for err in result.errors:\n",
    "                    lines.append(f\"  • {err.message}\")\n",
    "                    if err.suggested_fix:\n",
    "                        lines.append(f\"    Fix: {err.suggested_fix}\")\n",
    "        \n",
    "        if result.warnings:\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"Warnings:\")\n",
    "            for warn in result.warnings[:5]:  # Limit to 5\n",
    "                lines.append(f\"  ⚠ {warn}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "print(\"SimulationService class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: OPM Flow Integration\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **SimulationRunner**: Executes OPM Flow (native or Docker)\n",
    "2. **ResultParser**: Reads ECLIPSE binary output files\n",
    "3. **ErrorDiagnostics**: Classifies errors and suggests fixes\n",
    "4. **SimulationService**: Orchestrates the complete workflow\n",
    "\n",
    "### CLARISSA Integration Points\n",
    "\n",
    "```python\n",
    "# In CLARISSA's workflow:\n",
    "\n",
    "# 1. User describes model\n",
    "# 2. CLARISSA generates deck\n",
    "deck = generate_complete_deck(...)\n",
    "\n",
    "# 3. Submit to simulation service\n",
    "service = SimulationService(runner)\n",
    "result = await service.run_simulation(deck)\n",
    "\n",
    "# 4. Report back to user\n",
    "summary = service.format_result_summary(result)\n",
    "```\n",
    "\n",
    "### Next Notebook\n",
    "\n",
    "In **03_Knowledge_Layer.ipynb**, we'll cover:\n",
    "- Vector database setup with pgvector\n",
    "- Embedding generation for simulator documentation\n",
    "- Semantic search for keyword assistance\n",
    "- Analog database for reservoir properties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
