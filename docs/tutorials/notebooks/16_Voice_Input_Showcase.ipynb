{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfa4 CLARISSA Voice Input Showcase\n",
    "\n",
    "**Talk to Your Reservoir Simulation**\n",
    "\n",
    "This notebook demonstrates CLARISSA's voice interface - control reservoir simulations through natural speech.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wolfram-laube/clarissa/blob/main/docs/tutorials/notebooks/16_Voice_Input_Showcase.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Speech-to-Text** - Convert voice to text with Whisper\n",
    "2. **Intent Recognition** - Parse commands into structured intents\n",
    "3. **Command Execution** - Trigger visualizations by voice\n",
    "4. **Full Pipeline** - End-to-end voice control demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q openai anthropic plotly numpy ipywidgets\n\n# For local Whisper (optional - skip if using API)\n# !pip install -q faster-whisper\n\nprint(\"\u2705 Packages installed (OpenAI + Anthropic)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Optional, List\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "\n",
    "print(\"\u2705 Imports ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# API Key Setup - Choose your LLM provider\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# CLARISSA supports both OpenAI and Anthropic (Claude) for intent parsing.\n# Whisper (OpenAI) is always used for speech-to-text.\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n# Option 1: Colab secrets (recommended)\ntry:\n    from google.colab import userdata\n    \n    # Try Anthropic first (CLARISSA's native LLM)\n    try:\n        os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n        print(\"\u2705 Anthropic API key loaded (Claude)\")\n    except:\n        pass\n    \n    # Also try OpenAI (for Whisper STT + optional intent parsing)\n    try:\n        os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n        print(\"\u2705 OpenAI API key loaded (Whisper STT)\")\n    except:\n        pass\n        \nexcept ImportError:\n    pass\n\n# Check what's available\nanthropic_key = os.getenv('ANTHROPIC_API_KEY')\nopenai_key = os.getenv('OPENAI_API_KEY')\n\nprint()\nprint(\"\u2550\" * 50)\nprint(\"LLM Configuration:\")\nprint(\"\u2550\" * 50)\n\nif anthropic_key:\n    print(\"\ud83d\udfe2 Claude (Anthropic): Available - will use for intent parsing\")\n    LLM_PROVIDER = \"anthropic\"\nelif openai_key:\n    print(\"\ud83d\udfe2 GPT-4 (OpenAI): Available - will use for intent parsing\")\n    LLM_PROVIDER = \"openai\"\nelse:\n    print(\"\ud83d\udfe1 No LLM API key - using rule-based parsing only\")\n    print(\"   (Works for common commands, limited for complex queries)\")\n    LLM_PROVIDER = \"rules\"\n\nif openai_key:\n    print(\"\ud83d\udfe2 Whisper STT: Available\")\nelse:\n    print(\"\ud83d\udfe1 Whisper STT: Not available (no OpenAI key)\")\n    print(\"   (Text input mode only)\")\n\nprint(\"\u2550\" * 50)\nprint()\nprint(\"\ud83d\udca1 To add API keys in Colab:\")\nprint(\"   1. Click \ud83d\udd11 Secrets (left sidebar)\")\nprint(\"   2. Add: ANTHROPIC_API_KEY and/or OPENAI_API_KEY\")\nprint(\"   3. Enable notebook access\")\nprint(\"   4. Restart runtime\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Core Voice Components\n",
    "\n",
    "These are simplified versions of the full CLARISSA voice module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent Types\n",
    "class IntentType(Enum):\n",
    "    VISUALIZE_PROPERTY = \"visualize_property\"\n",
    "    QUERY_VALUE = \"query_value\"\n",
    "    NAVIGATE = \"navigate\"\n",
    "    HELP = \"help\"\n",
    "    CANCEL = \"cancel\"\n",
    "    CONFIRM = \"confirm\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class Intent:\n",
    "    \"\"\"Parsed intent from voice command.\"\"\"\n",
    "    type: IntentType\n",
    "    confidence: float\n",
    "    slots: Dict[str, Any] = field(default_factory=dict)\n",
    "    raw_text: str = \"\"\n",
    "\n",
    "@dataclass \n",
    "class VoiceResponse:\n",
    "    \"\"\"Response to voice command.\"\"\"\n",
    "    success: bool\n",
    "    text: str\n",
    "    intent: Optional[Intent] = None\n",
    "    visualization: Optional[go.Figure] = None\n",
    "\n",
    "print(\"\u2705 Data classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain vocabulary for better recognition\n",
    "DOMAIN_VOCABULARY = \"\"\"\n",
    "Reservoir simulation terms: permeability, porosity, water saturation, \n",
    "oil saturation, pressure, BHP, bottomhole pressure, OOIP,\n",
    "waterflood, injector, producer, PROD1, INJ1, INJ2, INJ3, INJ4,\n",
    "millidarcy, mD, psi, bbl/day, STB, FOPT, FOPR, FWPT, FWPR, FWCT,\n",
    "water cut, layer, grid, cell, timestep, 3D, cross-section, animation\n",
    "\"\"\"\n",
    "\n",
    "print(\"\ud83d\udcda Domain vocabulary loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Speech-to-Text (Whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe audio file using OpenAI Whisper API.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file (WAV, MP3, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        Transcribed text\n",
    "    \"\"\"\n",
    "    import openai\n",
    "    \n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    with open(audio_path, 'rb') as audio_file:\n",
    "        response = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            prompt=DOMAIN_VOCABULARY,\n",
    "            language=\"en\"\n",
    "        )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "print(\"\ud83c\udfa4 Whisper transcription function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Intent Parser (LLM-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "INTENT_PROMPT = \"\"\"You are a reservoir simulation assistant. Parse the user's voice command into a structured intent.\n\nAvailable intents:\n- visualize_property: Show reservoir properties (permeability, porosity, saturation, pressure)\n- query_value: Ask about simulation values (rates, pressures, water cut, cumulative production)\n- navigate: Go to different sections (results, sensitivity, model)\n- help: Ask for help or guidance\n- cancel: Stop or cancel current action\n- confirm: Confirm a pending action\n\nSlots to extract:\n- property: permeability, porosity, water_saturation, oil_saturation, pressure\n- layer: integer (1-5 typically)\n- time_days: integer (simulation day)\n- view_type: 3d, cross_section_xy, cross_section_xz, animation\n- well: PROD1, INJ1, INJ2, INJ3, INJ4\n- target: results, sensitivity, model, export\n\nUser said: \"{text}\"\n\nRespond with ONLY valid JSON (no markdown, no explanation):\n{{\"intent\": \"<type>\", \"confidence\": <0.0-1.0>, \"slots\": {{...}}}}\"\"\"\n\nimport re\n\ndef parse_intent_rules(text: str) -> Intent:\n    \"\"\"Rule-based intent parsing - works WITHOUT API key.\"\"\"\n    text_lower = text.lower().strip()\n    slots = {}\n    \n    # Cancel / Confirm / Help\n    if text_lower in [\"stop\", \"cancel\", \"never mind\", \"abort\", \"quit\"]:\n        return Intent(IntentType.CANCEL, 1.0, {}, text)\n    if text_lower in [\"yes\", \"yeah\", \"yep\", \"confirm\", \"ok\", \"okay\", \"do it\", \"go ahead\"]:\n        return Intent(IntentType.CONFIRM, 1.0, {}, text)\n    if text_lower == \"help\" or \"what can\" in text_lower:\n        return Intent(IntentType.HELP, 1.0, {}, text)\n    \n    # Visualization\n    viz_keywords = [\"show\", \"display\", \"visualize\", \"plot\", \"view\", \"see\"]\n    if any(kw in text_lower for kw in viz_keywords):\n        if \"perm\" in text_lower: slots[\"property\"] = \"permeability\"\n        elif \"poro\" in text_lower: slots[\"property\"] = \"porosity\"\n        elif \"saturation\" in text_lower or \" sw\" in text_lower: slots[\"property\"] = \"water_saturation\"\n        elif \"pressure\" in text_lower: slots[\"property\"] = \"pressure\"\n        \n        layer_match = re.search(r'layer\\s*(\\d+)', text_lower)\n        if layer_match: slots[\"layer\"] = int(layer_match.group(1))\n        \n        time_match = re.search(r'(?:day|time)\\s*(\\d+)', text_lower)\n        if time_match: slots[\"time_days\"] = int(time_match.group(1))\n        \n        if not slots.get(\"property\") and not slots.get(\"layer\"):\n            slots[\"property\"] = \"permeability\"\n        \n        return Intent(IntentType.VISUALIZE_PROPERTY, 0.95, slots, text)\n    \n    # Query\n    if any(kw in text_lower for kw in [\"what\", \"how much\", \"tell me\"]):\n        if \"oil rate\" in text_lower: slots[\"property\"] = \"oil_rate\"\n        elif \"water cut\" in text_lower: slots[\"property\"] = \"water_cut\"\n        elif \"water rate\" in text_lower: slots[\"property\"] = \"water_rate\"\n        elif \"pressure\" in text_lower: slots[\"property\"] = \"pressure\"\n        if slots:\n            return Intent(IntentType.QUERY_VALUE, 0.9, slots, text)\n    \n    return None\n\n\ndef parse_with_claude(text: str) -> Intent:\n    \"\"\"Parse intent using Claude (Anthropic).\"\"\"\n    import anthropic\n    client = anthropic.Anthropic()\n    \n    response = client.messages.create(\n        model=\"claude-sonnet-4-20250514\",\n        max_tokens=200,\n        messages=[{\"role\": \"user\", \"content\": INTENT_PROMPT.format(text=text)}]\n    )\n    \n    result_text = response.content[0].text.strip()\n    if result_text.startswith(\"```\"):\n        result_text = result_text.split(\"```\")[1].replace(\"json\", \"\", 1)\n    \n    data = json.loads(result_text)\n    return Intent(\n        IntentType(data.get(\"intent\", \"unknown\")),\n        float(data.get(\"confidence\", 0.8)),\n        data.get(\"slots\", {}),\n        text\n    )\n\n\ndef parse_with_openai(text: str) -> Intent:\n    \"\"\"Parse intent using GPT-4 (OpenAI).\"\"\"\n    import openai\n    client = openai.OpenAI()\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Parse voice commands into JSON. Respond with ONLY valid JSON.\"},\n            {\"role\": \"user\", \"content\": INTENT_PROMPT.format(text=text)}\n        ],\n        temperature=0.1,\n        max_tokens=200\n    )\n    \n    result_text = response.choices[0].message.content.strip()\n    if result_text.startswith(\"```\"):\n        result_text = result_text.split(\"```\")[1].replace(\"json\", \"\", 1)\n    \n    data = json.loads(result_text)\n    return Intent(\n        IntentType(data.get(\"intent\", \"unknown\")),\n        float(data.get(\"confidence\", 0.8)),\n        data.get(\"slots\", {}),\n        text\n    )\n\n\ndef parse_intent(text: str) -> Intent:\n    \"\"\"\n    Parse text into structured intent.\n    \n    Priority:\n    1. Rule-based (instant, no API)\n    2. Claude (CLARISSA native)\n    3. OpenAI GPT-4\n    4. Fallback to unknown\n    \"\"\"\n    # Try rules first\n    rule_result = parse_intent_rules(text)\n    if rule_result is not None:\n        print(f\"   \ud83d\udccb Parsed with rules\")\n        return rule_result\n    \n    # Try Claude (CLARISSA's native LLM)\n    if os.getenv('ANTHROPIC_API_KEY'):\n        try:\n            result = parse_with_claude(text)\n            print(f\"   \ud83e\udd16 Parsed with Claude\")\n            return result\n        except Exception as e:\n            print(f\"   \u26a0\ufe0f Claude error: {e}\")\n    \n    # Try OpenAI\n    if os.getenv('OPENAI_API_KEY'):\n        try:\n            result = parse_with_openai(text)\n            print(f\"   \ud83e\udd16 Parsed with GPT-4\")\n            return result\n        except Exception as e:\n            print(f\"   \u26a0\ufe0f OpenAI error: {e}\")\n    \n    # No LLM available\n    print(f\"   \u2139\ufe0f No LLM available for complex command\")\n    return Intent(IntentType.UNKNOWN, 0.0, {}, text)\n\nprint(\"\ud83e\udde0 Intent parser ready\")\nprint(\"   Priority: Rules \u2192 Claude \u2192 GPT-4 \u2192 Fallback\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualization Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic reservoir data for demo\n",
    "NX, NY, NZ = 10, 10, 5\n",
    "\n",
    "def generate_demo_data():\n",
    "    \"\"\"Generate synthetic reservoir properties.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Permeability with channel\n",
    "    perm = np.random.lognormal(mean=4.5, sigma=0.5, size=(NX, NY, NZ))\n",
    "    perm[3:7, :, :] *= 3  # High-perm channel\n",
    "    \n",
    "    # Porosity correlated with perm\n",
    "    poro = 0.15 + 0.1 * (np.log(perm) - 4) / 2\n",
    "    poro = np.clip(poro, 0.05, 0.35)\n",
    "    \n",
    "    # Water saturation (varies with time)\n",
    "    def get_saturation(time_days):\n",
    "        progress = min(time_days / 1800, 1.0)\n",
    "        sw = np.ones((NX, NY, NZ)) * 0.2  # Connate water\n",
    "        # Water front moving from injectors\n",
    "        for i in range(NX):\n",
    "            for j in range(NY):\n",
    "                dist = np.sqrt((i - NX//2)**2 + (j - NY//2)**2)\n",
    "                if dist < progress * NX * 0.7:\n",
    "                    sw[i, j, :] = 0.2 + 0.5 * (1 - dist / (NX * 0.7))\n",
    "        return np.clip(sw, 0.2, 0.8)\n",
    "    \n",
    "    return {\n",
    "        'permeability': perm,\n",
    "        'porosity': poro,\n",
    "        'get_saturation': get_saturation\n",
    "    }\n",
    "\n",
    "DEMO_DATA = generate_demo_data()\n",
    "print(\"\ud83d\udcca Demo data generated\")\n",
    "print(f\"   Grid: {NX}\u00d7{NY}\u00d7{NZ} = {NX*NY*NZ} cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3d_visualization(prop_name: str, prop_data: np.ndarray) -> go.Figure:\n",
    "    \"\"\"Create 3D scatter plot of property.\"\"\"\n",
    "    x, y, z = [], [], []\n",
    "    values = []\n",
    "    \n",
    "    for i in range(NX):\n",
    "        for j in range(NY):\n",
    "            for k in range(NZ):\n",
    "                x.append(i)\n",
    "                y.append(j)\n",
    "                z.append(k)\n",
    "                values.append(prop_data[i, j, k])\n",
    "    \n",
    "    fig = go.Figure(data=go.Scatter3d(\n",
    "        x=x, y=y, z=z,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color=values,\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(title=prop_name.title()),\n",
    "            opacity=0.8\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"3D {prop_name.title()} Distribution\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"X\",\n",
    "            yaxis_title=\"Y\", \n",
    "            zaxis_title=\"Layer\",\n",
    "            aspectmode='cube'\n",
    "        ),\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_cross_section(prop_name: str, prop_data: np.ndarray, layer: int) -> go.Figure:\n",
    "    \"\"\"Create 2D heatmap cross-section.\"\"\"\n",
    "    layer_idx = max(0, min(layer - 1, NZ - 1))\n",
    "    data_2d = prop_data[:, :, layer_idx]\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=data_2d.T,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title=prop_name.title())\n",
    "    ))\n",
    "    \n",
    "    # Add well markers\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[NX//2], y=[NY//2],\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=15, color='blue', symbol='circle'),\n",
    "        text=['PROD1'], textposition='top center',\n",
    "        name='Producer'\n",
    "    ))\n",
    "    \n",
    "    # Injectors at corners\n",
    "    inj_x = [1, 1, NX-2, NX-2]\n",
    "    inj_y = [1, NY-2, 1, NY-2]\n",
    "    inj_names = ['INJ1', 'INJ2', 'INJ3', 'INJ4']\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=inj_x, y=inj_y,\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=12, color='red', symbol='triangle-up'),\n",
    "        text=inj_names, textposition='top center',\n",
    "        name='Injectors'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{prop_name.title()} at Layer {layer}\",\n",
    "        xaxis_title=\"X\",\n",
    "        yaxis_title=\"Y\",\n",
    "        height=450\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"\ud83c\udfa8 Visualization functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Command Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_intent(intent: Intent) -> VoiceResponse:\n",
    "    \"\"\"\n",
    "    Execute parsed intent and return response.\n",
    "    \n",
    "    Args:\n",
    "        intent: Parsed Intent object\n",
    "        \n",
    "    Returns:\n",
    "        VoiceResponse with text and optional visualization\n",
    "    \"\"\"\n",
    "    slots = intent.slots\n",
    "    \n",
    "    if intent.type == IntentType.VISUALIZE_PROPERTY:\n",
    "        prop = slots.get('property', 'permeability')\n",
    "        layer = slots.get('layer')\n",
    "        time_days = slots.get('time_days', 500)\n",
    "        view_type = slots.get('view_type', '3d')\n",
    "        \n",
    "        # Get property data\n",
    "        if 'saturation' in prop or prop == 'sw':\n",
    "            prop_data = DEMO_DATA['get_saturation'](time_days)\n",
    "            prop_name = 'water_saturation'\n",
    "        elif prop in ['permeability', 'perm']:\n",
    "            prop_data = DEMO_DATA['permeability']\n",
    "            prop_name = 'permeability'\n",
    "        elif prop in ['porosity', 'poro']:\n",
    "            prop_data = DEMO_DATA['porosity']\n",
    "            prop_name = 'porosity'\n",
    "        else:\n",
    "            prop_data = DEMO_DATA['permeability']\n",
    "            prop_name = 'permeability'\n",
    "        \n",
    "        # Create visualization\n",
    "        if layer or 'cross' in str(view_type):\n",
    "            layer = layer or 3\n",
    "            fig = create_cross_section(prop_name, prop_data, layer)\n",
    "            text = f\"Showing {prop_name.replace('_', ' ')} at layer {layer}.\"\n",
    "        else:\n",
    "            fig = create_3d_visualization(prop_name, prop_data)\n",
    "            text = f\"Showing {prop_name.replace('_', ' ')} in 3D.\"\n",
    "        \n",
    "        return VoiceResponse(True, text, intent, fig)\n",
    "    \n",
    "    elif intent.type == IntentType.QUERY_VALUE:\n",
    "        prop = slots.get('property', 'oil_rate')\n",
    "        \n",
    "        # Simulate query results\n",
    "        values = {\n",
    "            'oil_rate': ('1,250', 'bbl/day'),\n",
    "            'water_rate': ('450', 'bbl/day'),\n",
    "            'water_cut': ('26', '%'),\n",
    "            'pressure': ('3,450', 'psi'),\n",
    "            'bhp': ('3,450', 'psi'),\n",
    "            'cumulative_oil': ('2.3', 'MMSTB'),\n",
    "            'fopt': ('2.3', 'MMSTB'),\n",
    "        }\n",
    "        \n",
    "        prop_key = prop.lower().replace(' ', '_')\n",
    "        if prop_key in values:\n",
    "            val, unit = values[prop_key]\n",
    "            text = f\"The {prop.replace('_', ' ')} is {val} {unit}.\"\n",
    "        else:\n",
    "            text = f\"I don't have data for {prop}.\"\n",
    "        \n",
    "        return VoiceResponse(True, text, intent)\n",
    "    \n",
    "    elif intent.type == IntentType.HELP:\n",
    "        text = \"\"\"You can say things like:\n",
    "\u2022 \"Show me the permeability\"\n",
    "\u2022 \"Show layer 3\"\n",
    "\u2022 \"What's the oil rate?\"\n",
    "\u2022 \"Show saturation at day 500\"\n",
    "\u2022 \"Stop\" or \"Cancel\" to abort\"\"\"\n",
    "        return VoiceResponse(True, text, intent)\n",
    "    \n",
    "    elif intent.type == IntentType.CANCEL:\n",
    "        return VoiceResponse(True, \"Cancelled.\", intent)\n",
    "    \n",
    "    elif intent.type == IntentType.CONFIRM:\n",
    "        return VoiceResponse(True, \"Confirmed.\", intent)\n",
    "    \n",
    "    else:\n",
    "        return VoiceResponse(\n",
    "            False, \n",
    "            \"I didn't understand that. Try saying 'help' for available commands.\",\n",
    "            intent\n",
    "        )\n",
    "\n",
    "print(\"\u26a1 Command executor ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Voice Pipeline\n",
    "\n",
    "The complete voice processing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_voice_command(text_or_audio: str, is_audio: bool = False) -> VoiceResponse:\n",
    "    \"\"\"\n",
    "    Complete voice command processing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        text_or_audio: Either text command or path to audio file\n",
    "        is_audio: True if input is audio file path\n",
    "        \n",
    "    Returns:\n",
    "        VoiceResponse with result\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Step 1: Transcribe if audio\n",
    "    if is_audio:\n",
    "        print(\"\ud83c\udfa4 Step 1: Transcribing audio...\")\n",
    "        try:\n",
    "            text = transcribe_audio(text_or_audio)\n",
    "            print(f\"   Transcription: \\\"{text}\\\"\")\n",
    "        except Exception as e:\n",
    "            return VoiceResponse(False, f\"Transcription failed: {e}\")\n",
    "    else:\n",
    "        text = text_or_audio\n",
    "        print(f\"\ud83d\udcdd Input: \\\"{text}\\\"\")\n",
    "    \n",
    "    # Step 2: Parse intent\n",
    "    print(\"\\n\ud83e\udde0 Step 2: Parsing intent...\")\n",
    "    intent = parse_intent(text)\n",
    "    print(f\"   Intent: {intent.type.value}\")\n",
    "    print(f\"   Confidence: {intent.confidence:.0%}\")\n",
    "    if intent.slots:\n",
    "        print(f\"   Slots: {intent.slots}\")\n",
    "    \n",
    "    # Step 3: Execute\n",
    "    print(\"\\n\u26a1 Step 3: Executing command...\")\n",
    "    response = execute_intent(intent)\n",
    "    \n",
    "    # Step 4: Response\n",
    "    print(f\"\\n\ud83d\udcac Response: {response.text}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\ud83d\ude80 Voice pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Interactive Demo\n",
    "\n",
    "Try voice commands! (Text mode - type what you would say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Show permeability in 3D\n",
    "response = process_voice_command(\"show me the permeability\")\n",
    "if response.visualization:\n",
    "    response.visualization.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Cross-section at specific layer\n",
    "response = process_voice_command(\"show layer 3\")\n",
    "if response.visualization:\n",
    "    response.visualization.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Query a value\n",
    "response = process_voice_command(\"what is the water cut?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4: Show saturation at specific time\n",
    "response = process_voice_command(\"show water saturation at day 1000\")\n",
    "if response.visualization:\n",
    "    response.visualization.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 5: Help command\n",
    "response = process_voice_command(\"help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 Interactive Widget\n",
    "\n",
    "Try your own commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widget\n",
    "text_input = widgets.Text(\n",
    "    placeholder='Type a command (e.g., \"show porosity in 3D\")',\n",
    "    description='\ud83c\udfa4 Say:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_submit(change):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if text_input.value:\n",
    "            response = process_voice_command(text_input.value)\n",
    "            if response.visualization:\n",
    "                display(response.visualization)\n",
    "\n",
    "text_input.on_submit(lambda x: on_submit(x))\n",
    "\n",
    "submit_btn = widgets.Button(description=\"Process\", button_style='primary')\n",
    "submit_btn.on_click(lambda x: on_submit(x))\n",
    "\n",
    "display(widgets.HBox([text_input, submit_btn]))\n",
    "display(output)\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Type a command and press Enter or click Process\")\n",
    "print(\"\\nExample commands:\")\n",
    "print('  \u2022 \"show me porosity\"')\n",
    "print('  \u2022 \"show saturation at layer 2\"')\n",
    "print('  \u2022 \"what is the oil rate\"')\n",
    "print('  \u2022 \"help\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 Audio File Demo\n",
    "\n",
    "Upload an audio file to test real speech-to-text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File upload widget\n",
    "from google.colab import files\n",
    "\n",
    "def process_uploaded_audio():\n",
    "    \"\"\"Upload and process an audio file.\"\"\"\n",
    "    print(\"\ud83d\udcc1 Upload an audio file (WAV, MP3, M4A)...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        print(f\"\\n\ud83c\udfb5 Processing: {filename}\")\n",
    "        response = process_voice_command(filename, is_audio=True)\n",
    "        \n",
    "        if response.visualization:\n",
    "            display(response.visualization)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Uncomment to test with audio upload:\n",
    "# process_uploaded_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\ufe0f\u20e3 Test Suite\n",
    "\n",
    "Evaluate intent recognition accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "TEST_CASES = [\n",
    "    (\"show me the permeability\", \"visualize_property\", {\"property\": \"permeability\"}),\n",
    "    (\"display porosity in 3D\", \"visualize_property\", {\"property\": \"porosity\"}),\n",
    "    (\"show layer 3\", \"visualize_property\", {\"layer\": 3}),\n",
    "    (\"what is the oil rate\", \"query_value\", {\"property\": \"oil_rate\"}),\n",
    "    (\"what's the water cut\", \"query_value\", {\"property\": \"water_cut\"}),\n",
    "    (\"help\", \"help\", {}),\n",
    "    (\"stop\", \"cancel\", {}),\n",
    "    (\"yes\", \"confirm\", {}),\n",
    "]\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"Run test suite and report accuracy.\"\"\"\n",
    "    print(\"\ud83e\uddea Running intent recognition tests...\\n\")\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for text, expected_intent, expected_slots in TEST_CASES:\n",
    "        intent = parse_intent(text)\n",
    "        \n",
    "        intent_match = intent.type.value == expected_intent\n",
    "        \n",
    "        if intent_match:\n",
    "            passed += 1\n",
    "            status = \"\u2705\"\n",
    "        else:\n",
    "            failed += 1\n",
    "            status = \"\u274c\"\n",
    "        \n",
    "        print(f\"{status} \\\"{text}\\\"\")\n",
    "        print(f\"   Expected: {expected_intent}, Got: {intent.type.value}\")\n",
    "        if not intent_match:\n",
    "            print(f\"   Confidence: {intent.confidence:.0%}\")\n",
    "        print()\n",
    "    \n",
    "    total = passed + failed\n",
    "    accuracy = passed / total * 100 if total > 0 else 0\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"\ud83d\udcca Results: {passed}/{total} passed ({accuracy:.0f}% accuracy)\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Uncomment to run tests (requires API key):\n",
    "# run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Speech-to-Text** - Whisper API transcription with domain vocabulary\n",
    "2. **Intent Parsing** - LLM-based command understanding\n",
    "3. **Slot Extraction** - Property, layer, time parameters\n",
    "4. **Visualization** - 3D and cross-section views triggered by voice\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Full Integration**: Connect to OPM Flow simulations\n",
    "- **Live Microphone**: Browser-based audio capture\n",
    "- **TTS Response**: Audio feedback with OpenAI TTS\n",
    "- **Offline Mode**: Local Whisper for air-gapped deployments\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Voice Input Tutorial](../guides/voice-input-tutorial.md)\n",
    "- [ADR-028: Voice Architecture](../../architecture/adr/ADR-028-voice-input-architecture.md)\n",
    "- [CLARISSA Source Code](https://gitlab.com/wolfram_laube/blauweiss_llc/clarissa)\n",
    "\n",
    "---\n",
    "\n",
    "*Part of CLARISSA - Conversational Language Agent for Reservoir Simulation*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}